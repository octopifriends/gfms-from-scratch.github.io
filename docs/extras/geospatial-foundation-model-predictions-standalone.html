<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-10-02">

<title>Understanding Geospatial Foundation Model Predictions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">GEOG 288KC</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">üè† home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../Syllabus.html"> 
<span class="menu-text">üìã syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-weekly-sessions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üíª weekly sessions</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-weekly-sessions">    
        <li>
    <a class="dropdown-item" href="../chapters/c01-geospatial-data-foundations.html">
 <span class="dropdown-text">Week 1 - üöÄ Core Tools and Data Access</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c02-spatial-temporal-attention-mechanisms.html">
 <span class="dropdown-text">Week 2 - ‚ö° Rapid Remote Sensing Preprocessing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c03-complete-gfm-architecture.html">
 <span class="dropdown-text">Week 3 - ü§ñ Machine Learning on Remote Sensing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c04-pretraining-implementation.html">
 <span class="dropdown-text">Week 4 - üèóÔ∏è Foundation Models in Practice</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c05-training-loop-optimization.html">
 <span class="dropdown-text">Week 5 - üîß Fine-Tuning &amp; Transfer Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c06-model-evaluation-analysis.html">
 <span class="dropdown-text">Week 6 - ‚è∞ Spatiotemporal Modeling &amp; Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../cheatsheets.html"> 
<span class="menu-text">üëÄ cheatsheets</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-explainers" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üß© explainers</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-explainers">    
        <li class="dropdown-header">1Ô∏è‚É£ Week 1</li>
        <li>
    <a class="dropdown-item" href="../extras/ai-ml-dl-fm-hierarchy.html">
 <span class="dropdown-text">ü§ñ AI/ML/DL/FM Hierarchy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/geospatial-foundation-model-predictions-standalone.html">
 <span class="dropdown-text">üéØ GFM Predictions (Standalone)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/geospatial-prediction-hierarchy.html">
 <span class="dropdown-text">‚úÖ Geospatial Task/Prediction Types</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-extras" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üìñ extras</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-extras">    
        <li class="dropdown-header">üéØ Practical Examples</li>
        <li>
    <a class="dropdown-item" href="../extras/examples/normalization_comparison.html">
 <span class="dropdown-text">Normalization Comparison</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/resnet.html">
 <span class="dropdown-text">ResNet Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/text_encoder.html">
 <span class="dropdown-text">Text Encoder</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/tiling-and-patches.html">
 <span class="dropdown-text">Tiling and Patches</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/terratorch_workflows.html">
 <span class="dropdown-text">TerraTorch Workflows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/resources/course_resources.html">
 <span class="dropdown-text">üìö Reference Materials</span></a>
  </li>  
        <li class="dropdown-header">üìÅ Project Templates</li>
        <li>
    <a class="dropdown-item" href="../extras/projects/project-proposal-template.html">
 <span class="dropdown-text">Project Proposal Template</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/projects/mvp-template.html">
 <span class="dropdown-text">Project Results Template</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gfms-from-scratch/gfms-from-scratch.github.io" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Understanding Geospatial Foundation Model Predictions</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">From Self-Supervised Pre-training to Task-Specific Applications</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 2, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#the-foundation-model-architecture" id="toc-the-foundation-model-architecture" class="nav-link" data-scroll-target="#the-foundation-model-architecture">The Foundation Model Architecture</a>
  <ul class="collapse">
  <li><a href="#input-data-structure" id="toc-input-data-structure" class="nav-link" data-scroll-target="#input-data-structure">Input Data Structure</a></li>
  <li><a href="#the-encoder-decoder-framework" id="toc-the-encoder-decoder-framework" class="nav-link" data-scroll-target="#the-encoder-decoder-framework">The Encoder-Decoder Framework</a></li>
  </ul></li>
  <li><a href="#pre-training-learning-without-labels" id="toc-pre-training-learning-without-labels" class="nav-link" data-scroll-target="#pre-training-learning-without-labels">Pre-training: Learning Without Labels</a>
  <ul class="collapse">
  <li><a href="#common-pre-training-objectives" id="toc-common-pre-training-objectives" class="nav-link" data-scroll-target="#common-pre-training-objectives">Common Pre-training Objectives</a></li>
  </ul></li>
  <li><a href="#downstream-tasks-from-general-to-specific" id="toc-downstream-tasks-from-general-to-specific" class="nav-link" data-scroll-target="#downstream-tasks-from-general-to-specific">Downstream Tasks: From General to Specific</a>
  <ul class="collapse">
  <li><a href="#task-categories" id="toc-task-categories" class="nav-link" data-scroll-target="#task-categories">Task Categories</a></li>
  </ul></li>
  <li><a href="#fine-tuning-strategies" id="toc-fine-tuning-strategies" class="nav-link" data-scroll-target="#fine-tuning-strategies">Fine-tuning Strategies</a>
  <ul class="collapse">
  <li><a href="#full-fine-tuning" id="toc-full-fine-tuning" class="nav-link" data-scroll-target="#full-fine-tuning">1. Full Fine-tuning</a></li>
  <li><a href="#linear-probing" id="toc-linear-probing" class="nav-link" data-scroll-target="#linear-probing">2. Linear Probing</a></li>
  <li><a href="#adapter-layers" id="toc-adapter-layers" class="nav-link" data-scroll-target="#adapter-layers">3. Adapter Layers</a></li>
  <li><a href="#prompt-tuning" id="toc-prompt-tuning" class="nav-link" data-scroll-target="#prompt-tuning">4. Prompt Tuning</a></li>
  </ul></li>
  <li><a href="#example-from-pre-training-to-land-cover-mapping" id="toc-example-from-pre-training-to-land-cover-mapping" class="nav-link" data-scroll-target="#example-from-pre-training-to-land-cover-mapping">Example: From Pre-training to Land Cover Mapping</a></li>
  <li><a href="#why-this-approach-works" id="toc-why-this-approach-works" class="nav-link" data-scroll-target="#why-this-approach-works">Why This Approach Works</a>
  <ul class="collapse">
  <li><a href="#data-efficiency" id="toc-data-efficiency" class="nav-link" data-scroll-target="#data-efficiency">1. <strong>Data Efficiency</strong></a></li>
  <li><a href="#transfer-learning" id="toc-transfer-learning" class="nav-link" data-scroll-target="#transfer-learning">2. <strong>Transfer Learning</strong></a></li>
  <li><a href="#multi-task-capability" id="toc-multi-task-capability" class="nav-link" data-scroll-target="#multi-task-capability">3. <strong>Multi-task Capability</strong></a></li>
  <li><a href="#robustness" id="toc-robustness" class="nav-link" data-scroll-target="#robustness">4. <strong>Robustness</strong></a></li>
  <li><a href="#temporal-understanding" id="toc-temporal-understanding" class="nav-link" data-scroll-target="#temporal-understanding">5. <strong>Temporal Understanding</strong></a></li>
  </ul></li>
  <li><a href="#practical-considerations" id="toc-practical-considerations" class="nav-link" data-scroll-target="#practical-considerations">Practical Considerations</a>
  <ul class="collapse">
  <li><a href="#choosing-pre-training-objectives" id="toc-choosing-pre-training-objectives" class="nav-link" data-scroll-target="#choosing-pre-training-objectives">Choosing Pre-training Objectives</a></li>
  <li><a href="#data-requirements" id="toc-data-requirements" class="nav-link" data-scroll-target="#data-requirements">Data Requirements</a></li>
  <li><a href="#computational-resources" id="toc-computational-resources" class="nav-link" data-scroll-target="#computational-resources">Computational Resources</a></li>
  </ul></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions">Future Directions</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#available-foundation-models" id="toc-available-foundation-models" class="nav-link" data-scroll-target="#available-foundation-models">Available Foundation Models</a>
  <ul class="collapse">
  <li><a href="#open-source-models" id="toc-open-source-models" class="nav-link" data-scroll-target="#open-source-models">Open Source Models</a></li>
  <li><a href="#libraries-and-frameworks" id="toc-libraries-and-frameworks" class="nav-link" data-scroll-target="#libraries-and-frameworks">Libraries and Frameworks</a></li>
  <li><a href="#resources-and-benchmarks" id="toc-resources-and-benchmarks" class="nav-link" data-scroll-target="#resources-and-benchmarks">Resources and Benchmarks</a></li>
  </ul></li>
  <li><a href="#visualization-resources" id="toc-visualization-resources" class="nav-link" data-scroll-target="#visualization-resources">Visualization Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>Geospatial Foundation Models (GFMs) represent a paradigm shift in how we process and analyze Earth observation data. Like their counterparts in natural language processing (e.g., GPT) and computer vision (e.g., CLIP), GFMs learn powerful representations from vast amounts of unlabeled satellite imagery that can be adapted for numerous downstream tasks.</p>
<p>This document explores the journey from raw satellite data to specific predictions, explaining how pre-training and fine-tuning enable diverse applications in Earth observation.</p>
</section>
<section id="the-foundation-model-architecture" class="level2">
<h2 class="anchored" data-anchor-id="the-foundation-model-architecture">The Foundation Model Architecture</h2>
<section id="input-data-structure" class="level3">
<h3 class="anchored" data-anchor-id="input-data-structure">Input Data Structure</h3>
<p>Geospatial data presents unique challenges compared to traditional computer vision:</p>
<ul>
<li><strong>Spatial dimensions</strong>: Typically patches of 100√ó100 to 224√ó224 pixels</li>
<li><strong>Spectral dimensions</strong>: Multiple bands beyond RGB (e.g., NIR, SWIR, thermal)</li>
<li><strong>Temporal dimensions</strong>: Time series of observations (e.g., weekly, monthly)</li>
</ul>
<p>For example, a typical input might be structured as:</p>
<pre><code>3 bands √ó 100√ó100 pixels √ó 12 time steps</code></pre>
<p>This creates a high-dimensional data cube that captures how Earth‚Äôs surface changes across space, spectrum, and time.</p>
</section>
<section id="the-encoder-decoder-framework" class="level3">
<h3 class="anchored" data-anchor-id="the-encoder-decoder-framework">The Encoder-Decoder Framework</h3>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A["Satellite Data&lt;br&gt;Spatial√óSpectral√óTemporal"] --&gt; B["Encoder&lt;br&gt;Deep Learning"]
    B --&gt; C["Embedding&lt;br&gt;Learned Vector&lt;br&gt;Representation"]
    C --&gt; D["Decoder&lt;br&gt;Deep Learning"]
    D --&gt; E["Task-Specific&lt;br&gt;Output"]
    
    C --&gt; F["Alternative:&lt;br&gt;Traditional ML&lt;br&gt;e.g., Lasso Regression"]
    F --&gt; G["Simple&lt;br&gt;Predictions"]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The foundation model architecture consists of:</p>
<ol type="1">
<li><strong>Encoder</strong>: Transforms high-dimensional satellite data into compact, information-rich embeddings</li>
<li><strong>Embedding</strong>: A learned vector representation (think of it as a ‚Äúdeep learning version of PCA‚Äù)</li>
<li><strong>Decoder</strong>: Transforms embeddings back into meaningful outputs</li>
</ol>
</section>
</section>
<section id="pre-training-learning-without-labels" class="level2">
<h2 class="anchored" data-anchor-id="pre-training-learning-without-labels">Pre-training: Learning Without Labels</h2>
<p>The power of foundation models comes from self-supervised pre-training on massive unlabeled datasets. Unlike traditional supervised learning, these approaches create training signals from the data itself.</p>
<section id="common-pre-training-objectives" class="level3">
<h3 class="anchored" data-anchor-id="common-pre-training-objectives">Common Pre-training Objectives</h3>
<section id="masked-autoencoding-mae" class="level4">
<h4 class="anchored" data-anchor-id="masked-autoencoding-mae">1. Masked Autoencoding (MAE)</h4>
<ul>
<li><strong>Task</strong>: Randomly mask patches of the input and predict the missing content</li>
<li><strong>Intuition</strong>: Forces the model to understand spatial context and relationships</li>
<li><strong>Example</strong>: Hide 75% of image patches and reconstruct them</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Conceptual example</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>masked_input <span class="op">=</span> mask_random_patches(satellite_image, mask_ratio<span class="op">=</span><span class="fl">0.75</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> encoder(masked_input)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>reconstruction <span class="op">=</span> decoder(embedding)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> MSE(reconstruction, original_patches)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="temporal-prediction" class="level4">
<h4 class="anchored" data-anchor-id="temporal-prediction">2. Temporal Prediction</h4>
<ul>
<li><strong>Task</strong>: Predict the next time step or fill in missing temporal observations</li>
<li><strong>Intuition</strong>: Learns seasonal patterns and temporal dynamics</li>
<li><strong>Example</strong>: Given January-June data, predict July</li>
</ul>
</section>
<section id="multi-modal-alignment" class="level4">
<h4 class="anchored" data-anchor-id="multi-modal-alignment">3. Multi-modal Alignment</h4>
<ul>
<li><strong>Task</strong>: Align embeddings from different sensors or modalities</li>
<li><strong>Intuition</strong>: Learns invariant features across different data sources</li>
<li><strong>Example</strong>: Match Sentinel-2 optical with Sentinel-1 SAR data</li>
</ul>
</section>
<section id="contrastive-learning" class="level4">
<h4 class="anchored" data-anchor-id="contrastive-learning">4. Contrastive Learning</h4>
<ul>
<li><strong>Task</strong>: Learn similar embeddings for nearby locations/times</li>
<li><strong>Intuition</strong>: Captures spatial and temporal continuity</li>
<li><strong>Example</strong>: Patches from the same field should have similar embeddings</li>
</ul>
</section>
</section>
</section>
<section id="downstream-tasks-from-general-to-specific" class="level2">
<h2 class="anchored" data-anchor-id="downstream-tasks-from-general-to-specific">Downstream Tasks: From General to Specific</h2>
<p>Once pre-trained, GFMs can be adapted for various Earth observation tasks through fine-tuning or as feature extractors.</p>
<section id="task-categories" class="level3">
<h3 class="anchored" data-anchor-id="task-categories">Task Categories</h3>
<section id="pixel-level-predictions-semantic-segmentation" class="level4">
<h4 class="anchored" data-anchor-id="pixel-level-predictions-semantic-segmentation">1. Pixel-Level Predictions (Semantic Segmentation)</h4>
<p><strong>Land Cover Classification</strong></p>
<ul>
<li><strong>Input</strong>: Multi-spectral satellite imagery</li>
<li><strong>Output</strong>: Per-pixel class labels (forest, urban, water, etc.)</li>
<li><strong>Fine-tuning</strong>: Add segmentation head, train on labeled maps</li>
</ul>
<p><strong>Change Detection</strong></p>
<ul>
<li><strong>Input</strong>: Multi-temporal image pairs</li>
<li><strong>Output</strong>: Binary change masks or change type maps</li>
<li><strong>Fine-tuning</strong>: Modify decoder for temporal comparisons</li>
</ul>
<p><strong>Cloud/Shadow Masking</strong></p>
<ul>
<li><strong>Input</strong>: Multi-spectral imagery</li>
<li><strong>Output</strong>: Binary masks for clouds and shadows</li>
<li><strong>Fine-tuning</strong>: Lightweight decoder trained on quality masks</li>
</ul>
</section>
<section id="image-level-predictions" class="level4">
<h4 class="anchored" data-anchor-id="image-level-predictions">2. Image-Level Predictions</h4>
<p><strong>Scene Classification</strong></p>
<ul>
<li><strong>Input</strong>: Image patches</li>
<li><strong>Output</strong>: Single label per patch (agricultural, residential, etc.)</li>
<li><strong>Fine-tuning</strong>: Replace decoder with classification head</li>
</ul>
<p><strong>Regression Tasks</strong></p>
<ul>
<li><strong>Input</strong>: Image patches</li>
<li><strong>Output</strong>: Continuous values (biomass, yield, poverty indicators)</li>
<li><strong>Fine-tuning</strong>: Linear probe or shallow MLP on embeddings</li>
</ul>
</section>
<section id="time-series-analysis" class="level4">
<h4 class="anchored" data-anchor-id="time-series-analysis">3. Time Series Analysis</h4>
<p><strong>Crop Type Mapping</strong></p>
<ul>
<li><strong>Input</strong>: Temporal sequence of observations</li>
<li><strong>Output</strong>: Crop type per pixel/parcel</li>
<li><strong>Fine-tuning</strong>: Temporal attention mechanisms</li>
</ul>
<p><strong>Phenology Detection</strong></p>
<ul>
<li><strong>Input</strong>: Time series data</li>
<li><strong>Output</strong>: Key dates (green-up, peak, senescence)</li>
<li><strong>Fine-tuning</strong>: Specialized temporal decoders</li>
</ul>
</section>
<section id="multi-modal-fusion" class="level4">
<h4 class="anchored" data-anchor-id="multi-modal-fusion">4. Multi-modal Fusion</h4>
<p><strong>Data Gap Filling</strong></p>
<ul>
<li><strong>Input</strong>: Partial observations from multiple sensors</li>
<li><strong>Output</strong>: Complete, harmonized time series</li>
<li><strong>Fine-tuning</strong>: Cross-attention between modalities</li>
</ul>
<p><strong>Super-resolution</strong></p>
<ul>
<li><strong>Input</strong>: Low-resolution imagery</li>
<li><strong>Output</strong>: High-resolution reconstruction</li>
<li><strong>Fine-tuning</strong>: Specialized upsampling decoders</li>
</ul>
</section>
</section>
</section>
<section id="fine-tuning-strategies" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-strategies">Fine-tuning Strategies</h2>
<section id="full-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="full-fine-tuning">1. Full Fine-tuning</h3>
<ul>
<li>Update all model parameters</li>
<li>Best for: Large labeled datasets, significant domain shift</li>
<li>Drawback: Computationally expensive, risk of overfitting</li>
</ul>
</section>
<section id="linear-probing" class="level3">
<h3 class="anchored" data-anchor-id="linear-probing">2. Linear Probing</h3>
<ul>
<li>Freeze encoder, train only classification head</li>
<li>Best for: Limited labeled data, similar domains</li>
<li>Benefit: Fast, prevents overfitting</li>
</ul>
</section>
<section id="adapter-layers" class="level3">
<h3 class="anchored" data-anchor-id="adapter-layers">3. Adapter Layers</h3>
<ul>
<li>Insert small trainable modules between frozen layers</li>
<li>Best for: Multiple tasks, parameter efficiency</li>
<li>Benefit: Task-specific adaptation with minimal parameters</li>
</ul>
</section>
<section id="prompt-tuning" class="level3">
<h3 class="anchored" data-anchor-id="prompt-tuning">4. Prompt Tuning</h3>
<ul>
<li>Learn task-specific input modifications</li>
<li>Best for: Very limited data, zero-shot scenarios</li>
<li>Benefit: Extremely parameter efficient</li>
</ul>
</section>
</section>
<section id="example-from-pre-training-to-land-cover-mapping" class="level2">
<h2 class="anchored" data-anchor-id="example-from-pre-training-to-land-cover-mapping">Example: From Pre-training to Land Cover Mapping</h2>
<p>Let‚Äôs trace the journey for a land cover classification task:</p>
<ol type="1">
<li><p><strong>Pre-training Phase</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Masked autoencoding on unlabeled Sentinel-2 data</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> massive_unlabeled_dataset:</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    masked_input <span class="op">=</span> random_mask(batch)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    embedding <span class="op">=</span> encoder(masked_input)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    reconstruction <span class="op">=</span> decoder(embedding)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    optimize(reconstruction_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Fine-tuning Phase</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze encoder, add segmentation head</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>encoder.freeze()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>segmentation_head <span class="op">=</span> SegmentationDecoder(num_classes<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Train on labeled land cover data</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> image, label_map <span class="kw">in</span> labeled_dataset:</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    embedding <span class="op">=</span> encoder(image)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> segmentation_head(embedding)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    optimize(cross_entropy_loss(prediction, label_map))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Inference Phase</strong></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply to new imagery</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>new_image <span class="op">=</span> load_sentinel2_scene()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> encoder(new_image)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>land_cover_map <span class="op">=</span> segmentation_head(embedding)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
<section id="why-this-approach-works" class="level2">
<h2 class="anchored" data-anchor-id="why-this-approach-works">Why This Approach Works</h2>
<section id="data-efficiency" class="level3">
<h3 class="anchored" data-anchor-id="data-efficiency">1. <strong>Data Efficiency</strong></h3>
<p>Pre-training on abundant unlabeled data reduces the need for expensive labeled datasets.</p>
</section>
<section id="transfer-learning" class="level3">
<h3 class="anchored" data-anchor-id="transfer-learning">2. <strong>Transfer Learning</strong></h3>
<p>Features learned from global data transfer to local applications.</p>
</section>
<section id="multi-task-capability" class="level3">
<h3 class="anchored" data-anchor-id="multi-task-capability">3. <strong>Multi-task Capability</strong></h3>
<p>One pre-trained model can be adapted for numerous downstream tasks.</p>
</section>
<section id="robustness" class="level3">
<h3 class="anchored" data-anchor-id="robustness">4. <strong>Robustness</strong></h3>
<p>Exposure to diverse data during pre-training improves generalization.</p>
</section>
<section id="temporal-understanding" class="level3">
<h3 class="anchored" data-anchor-id="temporal-understanding">5. <strong>Temporal Understanding</strong></h3>
<p>Unlike traditional CNN approaches, GFMs can natively handle time series.</p>
</section>
</section>
<section id="practical-considerations" class="level2">
<h2 class="anchored" data-anchor-id="practical-considerations">Practical Considerations</h2>
<section id="choosing-pre-training-objectives" class="level3">
<h3 class="anchored" data-anchor-id="choosing-pre-training-objectives">Choosing Pre-training Objectives</h3>
<ul>
<li><strong>For agricultural applications</strong>: Prioritize temporal objectives</li>
<li><strong>For urban mapping</strong>: Focus on spatial detail and multi-scale features</li>
<li><strong>For climate monitoring</strong>: Emphasize long-term temporal patterns</li>
</ul>
</section>
<section id="data-requirements" class="level3">
<h3 class="anchored" data-anchor-id="data-requirements">Data Requirements</h3>
<ul>
<li><strong>Pre-training</strong>: Terabytes of unlabeled imagery</li>
<li><strong>Fine-tuning</strong>: Can work with hundreds to thousands of labeled samples</li>
<li><strong>Inference</strong>: Real-time processing possible with optimized models</li>
</ul>
</section>
<section id="computational-resources" class="level3">
<h3 class="anchored" data-anchor-id="computational-resources">Computational Resources</h3>
<ul>
<li><strong>Pre-training</strong>: Requires significant GPU resources (days to weeks)</li>
<li><strong>Fine-tuning</strong>: Feasible on single GPUs (hours to days)</li>
<li><strong>Inference</strong>: Can be optimized for edge deployment</li>
</ul>
</section>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions</h2>
<ol type="1">
<li><p><strong>Foundation Models for Specific Domains</strong></p>
<ul>
<li>Agriculture-specific models</li>
<li>Urban-focused architectures</li>
<li>Ocean and coastal specialists</li>
</ul></li>
<li><p><strong>Multi-modal Foundation Models</strong></p>
<ul>
<li>Combining optical, SAR, and hyperspectral data</li>
<li>Integration with weather and climate data</li>
<li>Fusion with ground-based sensors</li>
</ul></li>
<li><p><strong>Efficient Architectures</strong></p>
<ul>
<li>Lightweight models for edge computing</li>
<li>Quantization and pruning techniques</li>
<li>Neural architecture search for Earth observation</li>
</ul></li>
<li><p><strong>Interpretability</strong></p>
<ul>
<li>Understanding what features the model learns</li>
<li>Explainable predictions for decision support</li>
<li>Uncertainty quantification</li>
</ul></li>
</ol>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Geospatial Foundation Models represent a powerful approach to Earth observation, transforming how we extract information from satellite data. Through self-supervised pre-training on massive unlabeled datasets, these models learn rich representations that can be efficiently adapted for diverse downstream tasks. Whether predicting land cover, detecting changes, or monitoring crop growth, GFMs provide a flexible and powerful framework for understanding our changing planet.</p>
<p>The key insight is that the expensive process of learning good representations can be done once on unlabeled data, then reused many times for different applications with minimal additional training. This democratizes access to advanced Earth observation capabilities and accelerates the development of new applications.</p>
<p>As we continue to accumulate Earth observation data at unprecedented rates, foundation models will become increasingly important for transforming this data deluge into actionable insights for science, policy, and society.</p>
</section>
<section id="available-foundation-models" class="level2">
<h2 class="anchored" data-anchor-id="available-foundation-models">Available Foundation Models</h2>
<p>Several geospatial foundation models are now available for research and application:</p>
<section id="open-source-models" class="level3">
<h3 class="anchored" data-anchor-id="open-source-models">Open Source Models</h3>
<ul>
<li><strong><a href="https://github.com/NASA-IMPACT/hls-foundation-os">Prithvi</a></strong> - NASA/IBM‚Äôs 100M parameter model trained on HLS data</li>
<li><strong><a href="https://github.com/Clay-foundation/model">Clay</a></strong> - Open foundation model for environmental monitoring</li>
<li><strong><a href="https://github.com/sustainlab-group/SatMAE">SatMAE</a></strong> - Masked autoencoder for temporal-spatial satellite data</li>
<li><strong><a href="https://github.com/coolzhao/Geo-SAM">GeoSAM</a></strong> - Segment Anything adapted for Earth observation</li>
<li><strong><a href="https://github.com/danfenghong/IEEE_TPAMI_SpectralGPT">SpectralGPT</a></strong> - Foundation model for spectral remote sensing</li>
</ul>
</section>
<section id="libraries-and-frameworks" class="level3">
<h3 class="anchored" data-anchor-id="libraries-and-frameworks">Libraries and Frameworks</h3>
<ul>
<li><strong><a href="https://github.com/microsoft/torchgeo">TorchGeo</a></strong> - PyTorch library with pre-trained models</li>
<li><strong><a href="https://github.com/IBM/terratorch">TerraTorch</a></strong> - Flexible framework for Earth observation deep learning</li>
<li><strong><a href="https://github.com/bair-climate-initiative/mmearth">MMEARTH</a></strong> - Multi-modal Earth observation models</li>
</ul>
</section>
<section id="resources-and-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="resources-and-benchmarks">Resources and Benchmarks</h3>
<ul>
<li><strong><a href="https://github.com/Jack-bo1220/Awesome-Remote-Sensing-Foundation-Models">Awesome Remote Sensing Foundation Models</a></strong> - Comprehensive collection</li>
<li><strong><a href="https://github.com/ServiceNow/geo-bench">GEO-Bench</a></strong> - Benchmark for evaluating GFMs</li>
<li><strong><a href="https://github.com/ESA-PhiLab/PhilEO-Bench">PhilEO Bench</a></strong> - ESA‚Äôs Earth observation benchmark</li>
</ul>
</section>
</section>
<section id="visualization-resources" class="level2">
<h2 class="anchored" data-anchor-id="visualization-resources">Visualization Resources</h2>
<p>To generate architectural diagrams for this explainer, you can run the provided visualization script:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> book/extras/scripts</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> visualize_gfm_architecture.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This will create three diagrams in the <code>book/extras/images/</code> directory:</p>
<ul>
<li><code>gfm_architecture.png</code>: Overview of the encoder-decoder architecture</li>
<li><code>gfm_pretraining_tasks.png</code>: Examples of self-supervised pre-training objectives</li>
<li><code>gfm_task_hierarchy.png</code>: Taxonomy of downstream tasks enabled by GFMs</li>
</ul>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kcaylor\.github\.io\/GEOG-288KC-geospatial-foundation-models");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Understanding Geospatial Foundation Model Predictions"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "From Self-Supervised Pre-training to Task-Specific Applications"</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> today</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overview</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>Geospatial Foundation Models (GFMs) represent a paradigm shift in how we process and analyze Earth observation data. Like their counterparts in natural language processing (e.g., GPT) and computer vision (e.g., CLIP), GFMs learn powerful representations from vast amounts of unlabeled satellite imagery that can be adapted for numerous downstream tasks.</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>This document explores the journey from raw satellite data to specific predictions, explaining how pre-training and fine-tuning enable diverse applications in Earth observation.</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Foundation Model Architecture</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="fu">### Input Data Structure</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>Geospatial data presents unique challenges compared to traditional computer vision:</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Spatial dimensions**: Typically patches of 100√ó100 to 224√ó224 pixels</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Spectral dimensions**: Multiple bands beyond RGB (e.g., NIR, SWIR, thermal)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Temporal dimensions**: Time series of observations (e.g., weekly, monthly)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>For example, a typical input might be structured as:</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="in">3 bands √ó 100√ó100 pixels √ó 12 time steps</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>This creates a high-dimensional data cube that captures how Earth's surface changes across space, spectrum, and time.</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Encoder-Decoder Framework</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a><span class="in">flowchart LR</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="in">    A["Satellite Data&lt;br&gt;Spatial√óSpectral√óTemporal"] --&gt; B["Encoder&lt;br&gt;Deep Learning"]</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a><span class="in">    B --&gt; C["Embedding&lt;br&gt;Learned Vector&lt;br&gt;Representation"]</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a><span class="in">    C --&gt; D["Decoder&lt;br&gt;Deep Learning"]</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="in">    D --&gt; E["Task-Specific&lt;br&gt;Output"]</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="in">    C --&gt; F["Alternative:&lt;br&gt;Traditional ML&lt;br&gt;e.g., Lasso Regression"]</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="in">    F --&gt; G["Simple&lt;br&gt;Predictions"]</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>The foundation model architecture consists of:</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Encoder**: Transforms high-dimensional satellite data into compact, information-rich embeddings</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Embedding**: A learned vector representation (think of it as a "deep learning version of PCA")</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Decoder**: Transforms embeddings back into meaningful outputs</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pre-training: Learning Without Labels</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>The power of foundation models comes from self-supervised pre-training on massive unlabeled datasets. Unlike traditional supervised learning, these approaches create training signals from the data itself.</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Pre-training Objectives</span></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 1. Masked Autoencoding (MAE)</span></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Task**: Randomly mask patches of the input and predict the missing content</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Intuition**: Forces the model to understand spatial context and relationships</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Example**: Hide 75% of image patches and reconstruct them</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a><span class="co"># Conceptual example</span></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>masked_input <span class="op">=</span> mask_random_patches(satellite_image, mask_ratio<span class="op">=</span><span class="fl">0.75</span>)</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> encoder(masked_input)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>reconstruction <span class="op">=</span> decoder(embedding)</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> MSE(reconstruction, original_patches)</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 2. Temporal Prediction</span></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Task**: Predict the next time step or fill in missing temporal observations</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Intuition**: Learns seasonal patterns and temporal dynamics</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Example**: Given January-June data, predict July</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 3. Multi-modal Alignment</span></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Task**: Align embeddings from different sensors or modalities</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Intuition**: Learns invariant features across different data sources</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Example**: Match Sentinel-2 optical with Sentinel-1 SAR data</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 4. Contrastive Learning</span></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Task**: Learn similar embeddings for nearby locations/times</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Intuition**: Captures spatial and temporal continuity</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Example**: Patches from the same field should have similar embeddings</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a><span class="fu">## Downstream Tasks: From General to Specific</span></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>Once pre-trained, GFMs can be adapted for various Earth observation tasks through fine-tuning or as feature extractors.</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a><span class="fu">### Task Categories</span></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 1. Pixel-Level Predictions (Semantic Segmentation)</span></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>**Land Cover Classification**</span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input**: Multi-spectral satellite imagery</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output**: Per-pixel class labels (forest, urban, water, etc.)</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Add segmentation head, train on labeled maps</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>**Change Detection**</span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input**: Multi-temporal image pairs</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output**: Binary change masks or change type maps</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Modify decoder for temporal comparisons</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>**Cloud/Shadow Masking**</span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input**: Multi-spectral imagery</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output**: Binary masks for clouds and shadows</span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Lightweight decoder trained on quality masks</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 2. Image-Level Predictions</span></span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>**Scene Classification**</span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input**: Image patches</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output**: Single label per patch (agricultural, residential, etc.)</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Replace decoder with classification head</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>**Regression Tasks**</span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input**: Image patches</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output**: Continuous values (biomass, yield, poverty indicators)</span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Linear probe or shallow MLP on embeddings</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 3. Time Series Analysis</span></span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a>**Crop Type Mapping**</span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input**: Temporal sequence of observations</span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output**: Crop type per pixel/parcel</span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Temporal attention mechanisms</span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a>**Phenology Detection**</span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input**: Time series data</span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output**: Key dates (green-up, peak, senescence)</span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Specialized temporal decoders</span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 4. Multi-modal Fusion</span></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a>**Data Gap Filling**</span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input**: Partial observations from multiple sensors</span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output**: Complete, harmonized time series</span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Cross-attention between modalities</span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a>**Super-resolution**</span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input**: Low-resolution imagery</span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output**: High-resolution reconstruction</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Specialized upsampling decoders</span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a><span class="fu">## Fine-tuning Strategies</span></span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. Full Fine-tuning</span></span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Update all model parameters</span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Best for: Large labeled datasets, significant domain shift</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Drawback: Computationally expensive, risk of overfitting</span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. Linear Probing</span></span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Freeze encoder, train only classification head</span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Best for: Limited labeled data, similar domains</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Benefit: Fast, prevents overfitting</span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. Adapter Layers</span></span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Insert small trainable modules between frozen layers</span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Best for: Multiple tasks, parameter efficiency</span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Benefit: Task-specific adaptation with minimal parameters</span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4. Prompt Tuning</span></span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learn task-specific input modifications</span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Best for: Very limited data, zero-shot scenarios</span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Benefit: Extremely parameter efficient</span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: From Pre-training to Land Cover Mapping</span></span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>Let's trace the journey for a land cover classification task:</span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Pre-training Phase**</span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a>   <span class="in">```python</span></span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Masked autoencoding on unlabeled Sentinel-2 data</span></span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a>   <span class="cf">for</span> batch <span class="kw">in</span> massive_unlabeled_dataset:</span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a>       masked_input <span class="op">=</span> random_mask(batch)</span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a>       embedding <span class="op">=</span> encoder(masked_input)</span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a>       reconstruction <span class="op">=</span> decoder(embedding)</span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a>       optimize(reconstruction_loss)</span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a>   <span class="in">```</span></span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Fine-tuning Phase**</span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a>   <span class="in">```python</span></span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Freeze encoder, add segmentation head</span></span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a>   encoder.freeze()</span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a>   segmentation_head <span class="op">=</span> SegmentationDecoder(num_classes<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Train on labeled land cover data</span></span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a>   <span class="cf">for</span> image, label_map <span class="kw">in</span> labeled_dataset:</span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a>       embedding <span class="op">=</span> encoder(image)</span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a>       prediction <span class="op">=</span> segmentation_head(embedding)</span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a>       optimize(cross_entropy_loss(prediction, label_map))</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a>   <span class="in">```</span></span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Inference Phase**</span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a>   <span class="in">```python</span></span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Apply to new imagery</span></span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a>   new_image <span class="op">=</span> load_sentinel2_scene()</span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a>   embedding <span class="op">=</span> encoder(new_image)</span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a>   land_cover_map <span class="op">=</span> segmentation_head(embedding)</span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a>   <span class="in">```</span></span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why This Approach Works</span></span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. **Data Efficiency**</span></span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a>Pre-training on abundant unlabeled data reduces the need for expensive labeled datasets.</span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. **Transfer Learning**</span></span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a>Features learned from global data transfer to local applications.</span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. **Multi-task Capability**</span></span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a>One pre-trained model can be adapted for numerous downstream tasks.</span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4. **Robustness**</span></span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a>Exposure to diverse data during pre-training improves generalization.</span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a><span class="fu">### 5. **Temporal Understanding**</span></span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a>Unlike traditional CNN approaches, GFMs can natively handle time series.</span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a><span class="fu">## Practical Considerations</span></span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choosing Pre-training Objectives</span></span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**For agricultural applications**: Prioritize temporal objectives</span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**For urban mapping**: Focus on spatial detail and multi-scale features</span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**For climate monitoring**: Emphasize long-term temporal patterns</span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data Requirements</span></span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pre-training**: Terabytes of unlabeled imagery</span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Can work with hundreds to thousands of labeled samples</span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Inference**: Real-time processing possible with optimized models</span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computational Resources</span></span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pre-training**: Requires significant GPU resources (days to weeks)</span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Feasible on single GPUs (hours to days)</span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Inference**: Can be optimized for edge deployment</span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a><span class="fu">## Future Directions</span></span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Foundation Models for Specific Domains**</span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Agriculture-specific models</span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Urban-focused architectures</span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Ocean and coastal specialists</span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Multi-modal Foundation Models**</span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Combining optical, SAR, and hyperspectral data</span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Integration with weather and climate data</span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Fusion with ground-based sensors</span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Efficient Architectures**</span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Lightweight models for edge computing</span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Quantization and pruning techniques</span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Neural architecture search for Earth observation</span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Interpretability**</span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Understanding what features the model learns</span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Explainable predictions for decision support</span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Uncertainty quantification</span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a>Geospatial Foundation Models represent a powerful approach to Earth observation, transforming how we extract information from satellite data. Through self-supervised pre-training on massive unlabeled datasets, these models learn rich representations that can be efficiently adapted for diverse downstream tasks. Whether predicting land cover, detecting changes, or monitoring crop growth, GFMs provide a flexible and powerful framework for understanding our changing planet.</span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a>The key insight is that the expensive process of learning good representations can be done once on unlabeled data, then reused many times for different applications with minimal additional training. This democratizes access to advanced Earth observation capabilities and accelerates the development of new applications.</span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a>As we continue to accumulate Earth observation data at unprecedented rates, foundation models will become increasingly important for transforming this data deluge into actionable insights for science, policy, and society.</span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a><span class="fu">## Available Foundation Models</span></span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a>Several geospatial foundation models are now available for research and application:</span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a><span class="fu">### Open Source Models</span></span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Prithvi](https://github.com/NASA-IMPACT/hls-foundation-os)** - NASA/IBM's 100M parameter model trained on HLS data</span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Clay](https://github.com/Clay-foundation/model)** - Open foundation model for environmental monitoring</span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[SatMAE](https://github.com/sustainlab-group/SatMAE)** - Masked autoencoder for temporal-spatial satellite data</span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[GeoSAM](https://github.com/coolzhao/Geo-SAM)** - Segment Anything adapted for Earth observation</span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[SpectralGPT](https://github.com/danfenghong/IEEE_TPAMI_SpectralGPT)** - Foundation model for spectral remote sensing</span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a><span class="fu">### Libraries and Frameworks</span></span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[TorchGeo](https://github.com/microsoft/torchgeo)** - PyTorch library with pre-trained models</span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[TerraTorch](https://github.com/IBM/terratorch)** - Flexible framework for Earth observation deep learning</span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[MMEARTH](https://github.com/bair-climate-initiative/mmearth)** - Multi-modal Earth observation models</span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-313"><a href="#cb7-313" aria-hidden="true" tabindex="-1"></a><span class="fu">### Resources and Benchmarks</span></span>
<span id="cb7-314"><a href="#cb7-314" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[Awesome Remote Sensing Foundation Models](https://github.com/Jack-bo1220/Awesome-Remote-Sensing-Foundation-Models)** - Comprehensive collection</span>
<span id="cb7-315"><a href="#cb7-315" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[GEO-Bench](https://github.com/ServiceNow/geo-bench)** - Benchmark for evaluating GFMs</span>
<span id="cb7-316"><a href="#cb7-316" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**[PhilEO Bench](https://github.com/ESA-PhiLab/PhilEO-Bench)** - ESA's Earth observation benchmark</span>
<span id="cb7-317"><a href="#cb7-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-318"><a href="#cb7-318" aria-hidden="true" tabindex="-1"></a><span class="fu">## Visualization Resources</span></span>
<span id="cb7-319"><a href="#cb7-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-320"><a href="#cb7-320" aria-hidden="true" tabindex="-1"></a>To generate architectural diagrams for this explainer, you can run the provided visualization script:</span>
<span id="cb7-321"><a href="#cb7-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-322"><a href="#cb7-322" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb7-323"><a href="#cb7-323" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> book/extras/scripts</span>
<span id="cb7-324"><a href="#cb7-324" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> visualize_gfm_architecture.py</span>
<span id="cb7-325"><a href="#cb7-325" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-326"><a href="#cb7-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-327"><a href="#cb7-327" aria-hidden="true" tabindex="-1"></a>This will create three diagrams in the <span class="in">`book/extras/images/`</span> directory:</span>
<span id="cb7-328"><a href="#cb7-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-329"><a href="#cb7-329" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`gfm_architecture.png`</span>: Overview of the encoder-decoder architecture</span>
<span id="cb7-330"><a href="#cb7-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`gfm_pretraining_tasks.png`</span>: Examples of self-supervised pre-training objectives</span>
<span id="cb7-331"><a href="#cb7-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`gfm_task_hierarchy.png`</span>: Taxonomy of downstream tasks enabled by GFMs</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/geog-logo.png" class="img-fluid figure-img" width="250"></p>
<figcaption>Department of Geography logo</figcaption>
</figure>
</div>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This website is built with <a href="https://github.com/kcaylor/GEOG-288KC-geospatial-foundation-models"><i class="fa-brands fa-github" title="the github octocat logo" aria-label="github"></i></a> and <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>