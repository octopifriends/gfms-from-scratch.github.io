<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Learning Basics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">GEOG 288KC</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">🏠 home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Syllabus.html"> 
<span class="menu-text">📋 syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-weekly-sessions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">💻 weekly sessions</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-weekly-sessions">    
        <li>
    <a class="dropdown-item" href="../../chapters/c01-geospatial-data-foundations.html">
 <span class="dropdown-text">Week 1 - 🚀 Core Tools and Data Access</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c02-spatial-temporal-attention-mechanisms.html">
 <span class="dropdown-text">Week 2 - ⚡ Rapid Remote Sensing Preprocessing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c03-complete-gfm-architecture.html">
 <span class="dropdown-text">Week 3 - 🤖 Machine Learning on Remote Sensing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c04-pretraining-implementation.html">
 <span class="dropdown-text">Week 4 - 🏗️ Foundation Models in Practice</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c05-training-loop-optimization.html">
 <span class="dropdown-text">Week 5 - 🔧 Fine-Tuning &amp; Transfer Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c06-model-evaluation-analysis.html">
 <span class="dropdown-text">Week 6 - ⏰ Spatiotemporal Modeling &amp; Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-cheatsheets" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">👀 cheatsheets</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-cheatsheets">    
        <li>
    <a class="dropdown-item" href="../../cheatsheets.html">
 <span class="dropdown-text">📋 All Cheatsheets</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">⚡ Quick Starts</li>
        <li>
    <a class="dropdown-item" href="../../extras/cheatsheets/week01_imports.html">
 <span class="dropdown-text">Week 01: Import Guide</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-explainers" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">🧩 explainers</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-explainers">    
        <li class="dropdown-header">1️⃣ Week 1</li>
        <li>
    <a class="dropdown-item" href="../../extras/ai-ml-dl-fm-hierarchy.html">
 <span class="dropdown-text">🤖 AI/ML/DL/FM Hierarchy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/geospatial-foundation-model-predictions-standalone.html">
 <span class="dropdown-text">🎯 GFM Predictions (Standalone)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/geospatial-prediction-hierarchy.html">
 <span class="dropdown-text">✅ Geospatial Task/Prediction Types</span></a>
  </li>  
        <li class="dropdown-header">2️⃣ Week 2</li>
        <li>
    <a class="dropdown-item" href="../../chapters/c00a-foundation_model_architectures.html">
 <span class="dropdown-text">🏗️ Foundation Model Architectures</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c00b-introduction-to-deeplearning-architecture.html">
 <span class="dropdown-text">🎓 Introduction to Deep Learning Architecture</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-extras" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">📖 extras</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-extras">    
        <li class="dropdown-header">🎯 Practical Examples</li>
        <li>
    <a class="dropdown-item" href="../../extras/examples/normalization_comparison.html">
 <span class="dropdown-text">Normalization Comparison</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/resnet.html">
 <span class="dropdown-text">ResNet Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/text_encoder.html">
 <span class="dropdown-text">Text Encoder</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/tiling-and-patches.html">
 <span class="dropdown-text">Tiling and Patches</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/terratorch_workflows.html">
 <span class="dropdown-text">TerraTorch Workflows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/resources/course_resources.html">
 <span class="dropdown-text">📚 Reference Materials</span></a>
  </li>  
        <li class="dropdown-header">📁 Project Templates</li>
        <li>
    <a class="dropdown-item" href="../../extras/projects/project-proposal-template.html">
 <span class="dropdown-text">Project Proposal Template</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/projects/mvp-template.html">
 <span class="dropdown-text">Project Results Template</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gfms-from-scratch/gfms-from-scratch.github.io" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Deep Learning Basics</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
            <p class="subtitle lead">Building encoders and decoders</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#converting-tokens-into-ids" id="toc-converting-tokens-into-ids" class="nav-link" data-scroll-target="#converting-tokens-into-ids">Converting tokens into IDs</a></li>
  </ul></li>
  <li><a href="#handling-unknown-tokens-and-special-tokens" id="toc-handling-unknown-tokens-and-special-tokens" class="nav-link" data-scroll-target="#handling-unknown-tokens-and-special-tokens">Handling Unknown Tokens and Special Tokens</a>
  <ul class="collapse">
  <li><a href="#special-tokens-in-llms" id="toc-special-tokens-in-llms" class="nav-link" data-scroll-target="#special-tokens-in-llms">Special Tokens in LLMs</a></li>
  </ul></li>
  <li><a href="#byte-pair-encoding-bpe-with-tiktoken" id="toc-byte-pair-encoding-bpe-with-tiktoken" class="nav-link" data-scroll-target="#byte-pair-encoding-bpe-with-tiktoken">Byte Pair Encoding (BPE) with Tiktoken</a>
  <ul class="collapse">
  <li><a href="#understanding-bpe-token-breakdown" id="toc-understanding-bpe-token-breakdown" class="nav-link" data-scroll-target="#understanding-bpe-token-breakdown">Understanding BPE Token Breakdown</a></li>
  <li><a href="#working-with-different-tiktoken-encodings" id="toc-working-with-different-tiktoken-encodings" class="nav-link" data-scroll-target="#working-with-different-tiktoken-encodings">Working with Different Tiktoken Encodings</a></li>
  <li><a href="#bpe-for-geospatial-text" id="toc-bpe-for-geospatial-text" class="nav-link" data-scroll-target="#bpe-for-geospatial-text">BPE for Geospatial Text</a></li>
  <li><a href="#understanding-token-efficiency" id="toc-understanding-token-efficiency" class="nav-link" data-scroll-target="#understanding-token-efficiency">Understanding Token Efficiency</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In this interactive session, we will explore how to build deep learning encoders and decoders from scratch. Our first example will use text encoders and a small corpus of text.</p>
<p>The test we will use is “The Verdict”, which is used in Sebastian Raschka’s excellent book, “Build a Large Language Model from Scratch.”</p>
<div id="4715210b" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> (<span class="st">"https://raw.githubusercontent.com/rasbt/"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>       <span class="st">"LLMs-from-scratch/main/ch02/01_main-chapter-code/"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>       <span class="st">"the-verdict.txt"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> urllib.request.urlopen(url) <span class="im">as</span> response:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    raw_text <span class="op">=</span> response.read().decode(<span class="st">"utf-8"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total number of character:"</span>, <span class="bu">len</span>(raw_text))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(raw_text[:<span class="dv">99</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Total number of character: 20479
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no </code></pre>
</div>
</div>
<p>The first step is to tokenize this text. We can use regular expressions to split the text into words.</p>
<div id="ffd79b30" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello, world. Welcome to text encoding."</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> re.split(<span class="vs">r'(\s)'</span>, text)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>['Hello,', ' ', 'world.', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding.']</code></pre>
</div>
</div>
<p>We want to separate punctuation and preserve capitalization.</p>
<div id="ad44f8f7" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> re.split(<span class="vs">r'([,.]|\s)'</span>, text)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding', '.', '']</code></pre>
</div>
</div>
<p>The decision to remove or include whitespace characters is application-dependent. For example, in coding (especially Pyhon coding), whitespace is essential to the structure and function of text, so it should be included in training.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Whitespace in Coding
</div>
</div>
<div class="callout-body-container callout-body">
<p>Most tokenization schemes include whitespace, but for brevity, these next examples exclude it.</p>
</div>
</div>
<div id="255000c3" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello, world. Is this-- a good decoder?"</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> re.split(<span class="vs">r'([,.:;?_!"()\']|--|\s)'</span>, text)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> [item.strip() <span class="cf">for</span> item <span class="kw">in</span> result <span class="cf">if</span> item.strip()]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'good', 'decoder', '?']</code></pre>
</div>
</div>
<p>We can run this against our entire corpus of text.</p>
<div id="be18361a" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>preprocessed <span class="op">=</span> re.split(<span class="vs">r'([,.:;?_!"()\']|--|\s)'</span>, raw_text)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>preprocessed <span class="op">=</span> [item.strip() <span class="cf">for</span> item <span class="kw">in</span> preprocessed <span class="cf">if</span> item.strip()]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(preprocessed[:<span class="dv">20</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']</code></pre>
</div>
</div>
<section id="converting-tokens-into-ids" class="level3">
<h3 class="anchored" data-anchor-id="converting-tokens-into-ids">Converting tokens into IDs</h3>
<p>Models assign each token a unique ID using a dictionary. These dictionaries are built from the text corpus.</p>
<p>We create a set of unique tokens that define the vocabulary of the corpus, which determines the number of items in our dictionary.</p>
<div id="8bbb5544" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>all_words <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">set</span>(preprocessed))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(all_words)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1130</code></pre>
</div>
</div>
<p>Creating a vocabulary requires assigning each unique ID to each token. We can do this using a dictionary comprehension.</p>
<div id="82fd282d" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> {token: idx <span class="cf">for</span> idx, token <span class="kw">in</span> <span class="bu">enumerate</span>(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    all_words, start<span class="op">=</span><span class="dv">0</span>)}</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a list of the first 10 tokens and their ids:</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>(vocab.items())[:<span class="dv">10</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[('!', 0), ('"', 1), ("'", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)]</code></pre>
</div>
</div>
<p>To facilitate the conversion of tokens to ids and vice versa, we can create a tokenizer class with an encode and decode method. The encode method splits text into tokens and then maps these to ids using the vocabulary. The decode method does the reverse, converting ids back to tokens.</p>
<div id="135e2731" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Tokenizer:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab <span class="op">=</span> vocab</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_to_id <span class="op">=</span> vocab</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.id_to_token <span class="op">=</span> {idx: token <span class="cf">for</span> token, idx <span class="kw">in</span> vocab.items()}</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, text):</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Encode a string into a list of token IDs."""</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        pattern <span class="op">=</span> <span class="vs">r'([,.?_!"()\']|--|\s)'</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> (</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            t.strip() <span class="cf">for</span> t <span class="kw">in</span> re.split(pattern, text) <span class="cf">if</span> t.strip()</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.token_to_id[t] <span class="cf">for</span> t <span class="kw">in</span> tokens]</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, ids):</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Decode a list of token IDs into a string."""</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">" "</span>.join(<span class="va">self</span>.id_to_token[i] <span class="cf">for</span> i <span class="kw">in</span> ids)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> re.sub(<span class="vs">r'\s+([,.?!"()\'])'</span>, <span class="vs">r'\1'</span>, text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s test our new <code>Tokenizer</code> class</p>
<div id="3e0d00af" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">""""It's the last he painted, you know," </span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="st">       Mrs. Gisburn said with pardonable pride."""</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(vocab)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>ids <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Encoded:</span><span class="ch">\n</span><span class="sc">{</span>ids<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Decoded:</span><span class="ch">\n</span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(ids)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Encoded:
[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]
Decoded:
" It' s the last he painted, you know," Mrs. Gisburn said with pardonable pride.</code></pre>
</div>
</div>
<p>The vocabulary of our tokenizer is limited by the tokens that appear in the corpus. Any word that is not in the corpus cannot be encoded.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello, do you like tea?"</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.encode(text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Causes a <code>KeyError</code>:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">KeyError:</span> <span class="st">'Hello'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="handling-unknown-tokens-and-special-tokens" class="level2">
<h2 class="anchored" data-anchor-id="handling-unknown-tokens-and-special-tokens">Handling Unknown Tokens and Special Tokens</h2>
<p>In practical LLM applications, we need to handle words that don’t appear in our training vocabulary. This is where <strong>unknown tokens</strong> and other <strong>special tokens</strong> become essential.</p>
<section id="special-tokens-in-llms" class="level3">
<h3 class="anchored" data-anchor-id="special-tokens-in-llms">Special Tokens in LLMs</h3>
<p>Modern tokenizers use several special tokens:</p>
<ul>
<li><code>&lt;|unk|&gt;</code> or <code>[UNK]</code>: Unknown token for out-of-vocabulary words</li>
<li><code>&lt;|endoftext|&gt;</code> or <code>[EOS]</code>: End of sequence/text marker</li>
<li><code>&lt;|startoftext|&gt;</code> or <code>[BOS]</code>: Beginning of sequence marker (less common in GPT-style models)</li>
<li><code>&lt;|pad|&gt;</code>: Padding token for batch processing</li>
</ul>
<p>Let’s create an enhanced tokenizer that handles unknown tokens:</p>
<div id="3e33783c" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EnhancedTokenizer:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add special tokens to vocabulary</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.special_tokens <span class="op">=</span> {</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">"&lt;|unk|&gt;"</span>: <span class="bu">len</span>(vocab),</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">"&lt;|endoftext|&gt;"</span>: <span class="bu">len</span>(vocab) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine original vocab with special tokens</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.full_vocab <span class="op">=</span> {<span class="op">**</span>vocab, <span class="op">**</span><span class="va">self</span>.special_tokens}</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_to_id <span class="op">=</span> <span class="va">self</span>.full_vocab</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.id_to_token <span class="op">=</span> {idx: token <span class="cf">for</span> token, idx <span class="kw">in</span> <span class="va">self</span>.full_vocab.items()}</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store reverse mapping for special tokens</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.unk_token_id <span class="op">=</span> <span class="va">self</span>.special_tokens[<span class="st">"&lt;|unk|&gt;"</span>]</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.endoftext_token_id <span class="op">=</span> <span class="va">self</span>.special_tokens[<span class="st">"&lt;|endoftext|&gt;"</span>]</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, text, add_endoftext<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Encode text, handling unknown tokens."""</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        pattern <span class="op">=</span> <span class="vs">r'([,.?_!"()\']|--|\s)'</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> [t.strip() <span class="cf">for</span> t <span class="kw">in</span> re.split(pattern, text) <span class="cf">if</span> t.strip()]</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert tokens to IDs, using &lt;|unk|&gt; for unknown tokens</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        ids <span class="op">=</span> []</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> token <span class="kw">in</span> tokens:</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> token <span class="kw">in</span> <span class="va">self</span>.token_to_id:</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>                ids.append(<span class="va">self</span>.token_to_id[token])</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>                ids.append(<span class="va">self</span>.unk_token_id)  <span class="co"># Use unknown token</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optionally add end-of-text token</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> add_endoftext:</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>            ids.append(<span class="va">self</span>.endoftext_token_id)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ids</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, ids):</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Decode token IDs back to text."""</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> []</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="bu">id</span> <span class="kw">in</span> ids:</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">id</span> <span class="op">==</span> <span class="va">self</span>.endoftext_token_id:</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span>  <span class="co"># Stop at end-of-text token</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="bu">id</span> <span class="kw">in</span> <span class="va">self</span>.id_to_token:</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>                tokens.append(<span class="va">self</span>.id_to_token[<span class="bu">id</span>])</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>                tokens.append(<span class="st">"&lt;|unk|&gt;"</span>)  <span class="co"># Fallback for invalid IDs</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">" "</span>.join(tokens)</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> re.sub(<span class="vs">r'\s+([,.?!"()\'])'</span>, <span class="vs">r'\1'</span>, text)</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> vocab_size(<span class="va">self</span>):</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return the total vocabulary size including special tokens."""</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.full_vocab)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s test our enhanced tokenizer:</p>
<div id="98b729aa" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>enhanced_tokenizer <span class="op">=</span> EnhancedTokenizer(vocab)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with unknown words</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>text_with_unknown <span class="op">=</span> <span class="st">"Hello, do you like tea? This is amazing!"</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> enhanced_tokenizer.encode(text_with_unknown)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>text_with_unknown<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Encoded IDs: </span><span class="sc">{</span>encoded<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Decoded text: </span><span class="sc">{</span>enhanced_tokenizer<span class="sc">.</span>decode(encoded)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Vocabulary size: </span><span class="sc">{</span>enhanced_tokenizer<span class="sc">.</span>vocab_size()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Original text: Hello, do you like tea? This is amazing!
Encoded IDs: [1130, 5, 355, 1126, 628, 975, 10, 97, 584, 1130, 0, 1131]
Decoded text: &lt;|unk|&gt;, do you like tea? This is &lt;|unk|&gt;!
Vocabulary size: 1132</code></pre>
</div>
</div>
<p>Notice how unknown words like “Hello” and “amazing” are now handled gracefully using the <code>&lt;|unk|&gt;</code> token, and the sequence ends with an <code>&lt;|endoftext|&gt;</code> token.</p>
<div id="8cdf92cf" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's see what the special token IDs are</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unknown token '&lt;|unk|&gt;' has ID: </span><span class="sc">{</span>enhanced_tokenizer<span class="sc">.</span>unk_token_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"End-of-text token '&lt;|endoftext|&gt;' has ID: </span><span class="sc">{</span>enhanced_tokenizer<span class="sc">.</span>endoftext_token_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Test decoding with end-of-text in the middle</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>test_ids <span class="op">=</span> [<span class="dv">100</span>, <span class="dv">200</span>, enhanced_tokenizer.endoftext_token_id, <span class="dv">300</span>, <span class="dv">400</span>]</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Decoding stops at &lt;|endoftext|&gt;: </span><span class="sc">{</span>enhanced_tokenizer<span class="sc">.</span>decode(test_ids)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Unknown token '&lt;|unk|&gt;' has ID: 1130
End-of-text token '&lt;|endoftext|&gt;' has ID: 1131
Decoding stops at &lt;|endoftext|&gt;: Thwing bean-stalk</code></pre>
</div>
</div>
</section>
</section>
<section id="byte-pair-encoding-bpe-with-tiktoken" class="level2">
<h2 class="anchored" data-anchor-id="byte-pair-encoding-bpe-with-tiktoken">Byte Pair Encoding (BPE) with Tiktoken</h2>
<p>While our simple tokenizer is educational, production LLMs use more sophisticated tokenization schemes like <strong>Byte Pair Encoding (BPE)</strong>. BPE creates subword tokens that balance vocabulary size with representation efficiency.</p>
<p>The <code>tiktoken</code> library provides access to the same tokenizers used by OpenAI’s GPT models.</p>
<div id="f69b887a" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tiktoken</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the GPT-2 tokenizer (used by GPT-2, GPT-3, and early GPT-4)</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>gpt2_tokenizer <span class="op">=</span> tiktoken.get_encoding(<span class="st">"gpt2"</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GPT-2 vocabulary size: </span><span class="sc">{</span>gpt2_tokenizer<span class="sc">.</span>n_vocab<span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>GPT-2 vocabulary size: 50,257</code></pre>
</div>
</div>
<p>Let’s compare our simple tokenizer with GPT-2’s BPE tokenizer:</p>
<div id="70900ee3" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>test_text <span class="op">=</span> <span class="st">"Hello, world! This demonstrates byte pair encoding tokenization."</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Our enhanced tokenizer</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>our_tokens <span class="op">=</span> enhanced_tokenizer.encode(test_text, add_endoftext<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>our_decoded <span class="op">=</span> enhanced_tokenizer.decode(our_tokens)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># GPT-2 BPE tokenizer  </span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>gpt2_tokens <span class="op">=</span> gpt2_tokenizer.encode(test_text)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>gpt2_decoded <span class="op">=</span> gpt2_tokenizer.decode(gpt2_tokens)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== Comparison ==="</span>)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>test_text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Our tokenizer:"</span>)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Tokens: </span><span class="sc">{</span>our_tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Count: </span><span class="sc">{</span><span class="bu">len</span>(our_tokens)<span class="sc">}</span><span class="ss"> tokens"</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Decoded: </span><span class="sc">{</span>our_decoded<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GPT-2 BPE tokenizer:"</span>)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Tokens: </span><span class="sc">{</span>gpt2_tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Count: </span><span class="sc">{</span><span class="bu">len</span>(gpt2_tokens)<span class="sc">}</span><span class="ss"> tokens"</span>) </span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Decoded: </span><span class="sc">{</span>gpt2_decoded<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>=== Comparison ===
Original text: Hello, world! This demonstrates byte pair encoding tokenization.

Our tokenizer:
  Tokens: [1130, 5, 1130, 0, 97, 1130, 1130, 1130, 1130, 1130, 7]
  Count: 11 tokens
  Decoded: &lt;|unk|&gt;, &lt;|unk|&gt;! This &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt;.

GPT-2 BPE tokenizer:
  Tokens: [15496, 11, 995, 0, 770, 15687, 18022, 5166, 21004, 11241, 1634, 13]
  Count: 12 tokens
  Decoded: Hello, world! This demonstrates byte pair encoding tokenization.</code></pre>
</div>
</div>
<section id="understanding-bpe-token-breakdown" class="level3">
<h3 class="anchored" data-anchor-id="understanding-bpe-token-breakdown">Understanding BPE Token Breakdown</h3>
<p>BPE tokenizers split text into subword units. Let’s examine how GPT-2 breaks down different types of text:</p>
<div id="ef418e4d" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_tokenization(text, tokenizer_name<span class="op">=</span><span class="st">"gpt2"</span>):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Analyze how tiktoken breaks down text."""</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    tokenizer <span class="op">=</span> tiktoken.get_encoding(tokenizer_name)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Text: '</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token count: </span><span class="sc">{</span><span class="bu">len</span>(tokens)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Token breakdown:"</span>)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, token_id <span class="kw">in</span> <span class="bu">enumerate</span>(tokens):</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        token_text <span class="op">=</span> tokenizer.decode([token_id])</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>i<span class="sc">:2d}</span><span class="ss">: </span><span class="sc">{</span>token_id<span class="sc">:5d}</span><span class="ss"> -&gt; '</span><span class="sc">{</span>token_text<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Test different types of text</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>analyze_tokenization(<span class="st">"Hello world"</span>)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>analyze_tokenization(<span class="st">"Tokenization"</span>)  </span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>analyze_tokenization(<span class="st">"supercalifragilisticexpialidocious"</span>)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>analyze_tokenization(<span class="st">"AI/ML researcher"</span>)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>analyze_tokenization(<span class="st">"The year 2024"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Text: 'Hello world'
Tokens: [15496, 995]
Token count: 2
Token breakdown:
   0: 15496 -&gt; 'Hello'
   1:   995 -&gt; ' world'

Text: 'Tokenization'
Tokens: [30642, 1634]
Token count: 2
Token breakdown:
   0: 30642 -&gt; 'Token'
   1:  1634 -&gt; 'ization'

Text: 'supercalifragilisticexpialidocious'
Tokens: [16668, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]
Token count: 11
Token breakdown:
   0: 16668 -&gt; 'super'
   1:  9948 -&gt; 'cal'
   2:   361 -&gt; 'if'
   3: 22562 -&gt; 'rag'
   4:   346 -&gt; 'il'
   5:   396 -&gt; 'ist'
   6:   501 -&gt; 'ice'
   7: 42372 -&gt; 'xp'
   8:   498 -&gt; 'ial'
   9:   312 -&gt; 'id'
  10: 32346 -&gt; 'ocious'

Text: 'AI/ML researcher'
Tokens: [20185, 14, 5805, 13453]
Token count: 4
Token breakdown:
   0: 20185 -&gt; 'AI'
   1:    14 -&gt; '/'
   2:  5805 -&gt; 'ML'
   3: 13453 -&gt; ' researcher'

Text: 'The year 2024'
Tokens: [464, 614, 48609]
Token count: 3
Token breakdown:
   0:   464 -&gt; 'The'
   1:   614 -&gt; ' year'
   2: 48609 -&gt; ' 2024'
</code></pre>
</div>
</div>
</section>
<section id="working-with-different-tiktoken-encodings" class="level3">
<h3 class="anchored" data-anchor-id="working-with-different-tiktoken-encodings">Working with Different Tiktoken Encodings</h3>
<p>OpenAI uses different encodings for different models:</p>
<div id="6ceb07c3" class="cell" data-execution_count="16">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Available encodings</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>available_encodings <span class="op">=</span> [<span class="st">"gpt2"</span>, <span class="st">"p50k_base"</span>, <span class="st">"cl100k_base"</span>]</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>test_text <span class="op">=</span> <span class="st">"Geospatial foundation models revolutionize remote sensing!"</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> encoding_name <span class="kw">in</span> available_encodings:</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        tokenizer <span class="op">=</span> tiktoken.get_encoding(encoding_name)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> tokenizer.encode(test_text)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>encoding_name<span class="sc">:12s}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">len</span>(tokens)<span class="sc">:2d}</span><span class="ss"> tokens, vocab size: </span><span class="sc">{</span>tokenizer<span class="sc">.</span>n_vocab<span class="sc">:6,}</span><span class="ss">"</span>)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"              Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>encoding_name<span class="sc">}</span><span class="ss">: Error - </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>gpt2        : 10 tokens, vocab size: 50,257
              Tokens: [10082, 2117, 34961, 8489, 4981, 5854, 1096, 6569, 34244, 0]

p50k_base   : 10 tokens, vocab size: 50,281
              Tokens: [10082, 2117, 34961, 8489, 4981, 5854, 1096, 6569, 34244, 0]

cl100k_base : 10 tokens, vocab size: 100,277
              Tokens: [9688, 437, 33514, 16665, 4211, 14110, 553, 8870, 60199, 0]
</code></pre>
</div>
</div>
</section>
<section id="bpe-for-geospatial-text" class="level3">
<h3 class="anchored" data-anchor-id="bpe-for-geospatial-text">BPE for Geospatial Text</h3>
<p>Let’s see how BPE handles domain-specific geospatial terminology:</p>
<div id="7e5134b2" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>geospatial_texts <span class="op">=</span> [</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"NDVI vegetation index analysis"</span>,</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Landsat-8 multispectral imagery"</span>, </span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Convolutional neural networks for land cover classification"</span>,</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sentinel-2 satellite data preprocessing"</span>,</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Geospatial foundation models fine-tuning"</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>gpt2_enc <span class="op">=</span> tiktoken.get_encoding(<span class="st">"gpt2"</span>)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== Geospatial Text Tokenization ==="</span>)</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> geospatial_texts:</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> gpt2_enc.encode(text)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  -&gt; </span><span class="sc">{</span><span class="bu">len</span>(tokens)<span class="sc">}</span><span class="ss"> tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>=== Geospatial Text Tokenization ===
'NDVI vegetation index analysis'
  -&gt; 5 tokens: [8575, 12861, 28459, 6376, 3781]

'Landsat-8 multispectral imagery'
  -&gt; 10 tokens: [43, 1746, 265, 12, 23, 1963, 271, 806, 1373, 19506]

'Convolutional neural networks for land cover classification'
  -&gt; 10 tokens: [3103, 85, 2122, 282, 17019, 7686, 329, 1956, 3002, 17923]

'Sentinel-2 satellite data preprocessing'
  -&gt; 8 tokens: [31837, 20538, 12, 17, 11210, 1366, 662, 36948]

'Geospatial foundation models fine-tuning'
  -&gt; 9 tokens: [10082, 2117, 34961, 8489, 4981, 3734, 12, 28286, 278]
</code></pre>
</div>
</div>
</section>
<section id="understanding-token-efficiency" class="level3">
<h3 class="anchored" data-anchor-id="understanding-token-efficiency">Understanding Token Efficiency</h3>
<p>The choice of tokenizer affects model efficiency. Let’s compare token counts for our text corpus:</p>
<div id="1d1d6da5" class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use a sample from our corpus</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>sample_text <span class="op">=</span> raw_text[:<span class="dv">500</span>]  <span class="co"># First 500 characters</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== Token Efficiency Comparison ==="</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Text length: </span><span class="sc">{</span><span class="bu">len</span>(sample_text)<span class="sc">}</span><span class="ss"> characters"</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Our simple tokenizer</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>our_tokens <span class="op">=</span> enhanced_tokenizer.encode(sample_text, add_endoftext<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Simple tokenizer: </span><span class="sc">{</span><span class="bu">len</span>(our_tokens)<span class="sc">}</span><span class="ss"> tokens"</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="co"># GPT-2 BPE</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>gpt2_tokens <span class="op">=</span> gpt2_tokenizer.encode(sample_text)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GPT-2 BPE:       </span><span class="sc">{</span><span class="bu">len</span>(gpt2_tokens)<span class="sc">}</span><span class="ss"> tokens"</span>)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate efficiency</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>chars_per_token_simple <span class="op">=</span> <span class="bu">len</span>(sample_text) <span class="op">/</span> <span class="bu">len</span>(our_tokens)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>chars_per_token_bpe <span class="op">=</span> <span class="bu">len</span>(sample_text) <span class="op">/</span> <span class="bu">len</span>(gpt2_tokens)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Characters per token:"</span>)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Simple: </span><span class="sc">{</span>chars_per_token_simple<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  BPE:    </span><span class="sc">{</span>chars_per_token_bpe<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  BPE is </span><span class="sc">{</span>chars_per_token_bpe<span class="op">/</span>chars_per_token_simple<span class="sc">:.1f}</span><span class="ss">x more efficient"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>=== Token Efficiency Comparison ===
Text length: 500 characters

Simple tokenizer: 110 tokens
GPT-2 BPE:       123 tokens

Characters per token:
  Simple: 4.55
  BPE:    4.07
  BPE is 0.9x more efficient</code></pre>
</div>
</div>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ol type="1">
<li><strong>Special Tokens</strong>: Essential for handling unknown words and sequence boundaries</li>
<li><strong>Unknown Token Handling</strong>: <code>&lt;|unk|&gt;</code> tokens allow models to gracefully handle out-of-vocabulary words</li>
<li><strong>End-of-Text Tokens</strong>: <code>&lt;|endoftext|&gt;</code> tokens help models understand sequence boundaries</li>
<li><strong>BPE Efficiency</strong>: Byte Pair Encoding creates a good balance between vocabulary size and representation efficiency</li>
<li><strong>Domain Adaptation</strong>: Different tokenizers may handle domain-specific text differently</li>
</ol>
<p>In geospatial AI applications, choosing the right tokenization strategy affects both model performance and computational efficiency, especially when working with specialized terminology from remote sensing and Earth science domains.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kcaylor\.github\.io\/GEOG-288KC-geospatial-foundation-models");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb37" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Deep Learning Basics"</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Building encoders and decoders"</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="an">editor_options:</span><span class="co"> </span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co">  chunk_output_type: console</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> geoai</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="co">    html:</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="co">        toc: true</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="co">        toc-depth: 3</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="co">        code-fold: show</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>In this interactive session, we will explore how to build deep learning encoders and decoders from scratch. Our first example will use text encoders and a small corpus of text.</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>The test we will use is "The Verdict", which is used in Sebastian Raschka's excellent book, "Build a Large Language Model from Scratch."</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> (<span class="st">"https://raw.githubusercontent.com/rasbt/"</span></span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a>       <span class="st">"LLMs-from-scratch/main/ch02/01_main-chapter-code/"</span></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>       <span class="st">"the-verdict.txt"</span>)</span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> urllib.request.urlopen(url) <span class="im">as</span> response:</span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a>    raw_text <span class="op">=</span> response.read().decode(<span class="st">"utf-8"</span>)</span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total number of character:"</span>, <span class="bu">len</span>(raw_text))</span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(raw_text[:<span class="dv">99</span>])</span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a>The first step is to tokenize this text. We can use regular expressions to split the text into words.</span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello, world. Welcome to text encoding."</span></span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> re.split(<span class="vs">r'(\s)'</span>, text)</span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span>
<span id="cb37-50"><a href="#cb37-50" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-51"><a href="#cb37-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a>We want to separate punctuation and preserve capitalization.</span>
<span id="cb37-53"><a href="#cb37-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-56"><a href="#cb37-56" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-57"><a href="#cb37-57" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb37-58"><a href="#cb37-58" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> re.split(<span class="vs">r'([,.]|\s)'</span>, text)</span>
<span id="cb37-59"><a href="#cb37-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span>
<span id="cb37-60"><a href="#cb37-60" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-61"><a href="#cb37-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-62"><a href="#cb37-62" aria-hidden="true" tabindex="-1"></a>The decision to remove or include whitespace characters is application-dependent. For example, in coding (especially Pyhon coding), whitespace is essential to the structure and function of text, so it should be included in training. </span>
<span id="cb37-63"><a href="#cb37-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-64"><a href="#cb37-64" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb37-65"><a href="#cb37-65" aria-hidden="true" tabindex="-1"></a><span class="fu">## Whitespace in Coding</span></span>
<span id="cb37-66"><a href="#cb37-66" aria-hidden="true" tabindex="-1"></a>Most tokenization schemes include whitespace, but for brevity, these next examples exclude it.</span>
<span id="cb37-67"><a href="#cb37-67" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb37-68"><a href="#cb37-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-71"><a href="#cb37-71" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-72"><a href="#cb37-72" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-73"><a href="#cb37-73" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello, world. Is this-- a good decoder?"</span></span>
<span id="cb37-74"><a href="#cb37-74" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> re.split(<span class="vs">r'([,.:;?_!"()\']|--|\s)'</span>, text)</span>
<span id="cb37-75"><a href="#cb37-75" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> [item.strip() <span class="cf">for</span> item <span class="kw">in</span> result <span class="cf">if</span> item.strip()]</span>
<span id="cb37-76"><a href="#cb37-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span>
<span id="cb37-77"><a href="#cb37-77" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-78"><a href="#cb37-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-79"><a href="#cb37-79" aria-hidden="true" tabindex="-1"></a>We can run this against our entire corpus of text.</span>
<span id="cb37-80"><a href="#cb37-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-83"><a href="#cb37-83" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-84"><a href="#cb37-84" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-85"><a href="#cb37-85" aria-hidden="true" tabindex="-1"></a>preprocessed <span class="op">=</span> re.split(<span class="vs">r'([,.:;?_!"()\']|--|\s)'</span>, raw_text)</span>
<span id="cb37-86"><a href="#cb37-86" aria-hidden="true" tabindex="-1"></a>preprocessed <span class="op">=</span> [item.strip() <span class="cf">for</span> item <span class="kw">in</span> preprocessed <span class="cf">if</span> item.strip()]</span>
<span id="cb37-87"><a href="#cb37-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(preprocessed[:<span class="dv">20</span>])</span>
<span id="cb37-88"><a href="#cb37-88" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-89"><a href="#cb37-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-90"><a href="#cb37-90" aria-hidden="true" tabindex="-1"></a><span class="fu">### Converting tokens into IDs</span></span>
<span id="cb37-91"><a href="#cb37-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-92"><a href="#cb37-92" aria-hidden="true" tabindex="-1"></a>Models assign each token a unique ID using a dictionary. These dictionaries are built from the text corpus.</span>
<span id="cb37-93"><a href="#cb37-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-94"><a href="#cb37-94" aria-hidden="true" tabindex="-1"></a>We create a set of unique tokens that define the vocabulary of the corpus, which determines the number of items in our dictionary.</span>
<span id="cb37-95"><a href="#cb37-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-98"><a href="#cb37-98" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-99"><a href="#cb37-99" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-100"><a href="#cb37-100" aria-hidden="true" tabindex="-1"></a>all_words <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">set</span>(preprocessed))</span>
<span id="cb37-101"><a href="#cb37-101" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(all_words)</span>
<span id="cb37-102"><a href="#cb37-102" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vocab_size)</span>
<span id="cb37-103"><a href="#cb37-103" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-104"><a href="#cb37-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-105"><a href="#cb37-105" aria-hidden="true" tabindex="-1"></a>Creating a vocabulary requires assigning each unique ID to each token. We can do this using a dictionary comprehension.</span>
<span id="cb37-106"><a href="#cb37-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-109"><a href="#cb37-109" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-110"><a href="#cb37-110" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-111"><a href="#cb37-111" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> {token: idx <span class="cf">for</span> idx, token <span class="kw">in</span> <span class="bu">enumerate</span>(</span>
<span id="cb37-112"><a href="#cb37-112" aria-hidden="true" tabindex="-1"></a>    all_words, start<span class="op">=</span><span class="dv">0</span>)}</span>
<span id="cb37-113"><a href="#cb37-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-114"><a href="#cb37-114" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a list of the first 10 tokens and their ids:</span></span>
<span id="cb37-115"><a href="#cb37-115" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>(vocab.items())[:<span class="dv">10</span>])</span>
<span id="cb37-116"><a href="#cb37-116" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-117"><a href="#cb37-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-118"><a href="#cb37-118" aria-hidden="true" tabindex="-1"></a>To facilitate the conversion of tokens to ids and vice versa, we can create a tokenizer class with an encode and decode method. The encode method splits text into tokens and then maps these to ids using the vocabulary. The decode method does the reverse, converting ids back to tokens.</span>
<span id="cb37-119"><a href="#cb37-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-122"><a href="#cb37-122" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-123"><a href="#cb37-123" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-124"><a href="#cb37-124" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Tokenizer:</span>
<span id="cb37-125"><a href="#cb37-125" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab):</span>
<span id="cb37-126"><a href="#cb37-126" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab <span class="op">=</span> vocab</span>
<span id="cb37-127"><a href="#cb37-127" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_to_id <span class="op">=</span> vocab</span>
<span id="cb37-128"><a href="#cb37-128" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.id_to_token <span class="op">=</span> {idx: token <span class="cf">for</span> token, idx <span class="kw">in</span> vocab.items()}</span>
<span id="cb37-129"><a href="#cb37-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-130"><a href="#cb37-130" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, text):</span>
<span id="cb37-131"><a href="#cb37-131" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Encode a string into a list of token IDs."""</span></span>
<span id="cb37-132"><a href="#cb37-132" aria-hidden="true" tabindex="-1"></a>        pattern <span class="op">=</span> <span class="vs">r'([,.?_!"()\']|--|\s)'</span></span>
<span id="cb37-133"><a href="#cb37-133" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> (</span>
<span id="cb37-134"><a href="#cb37-134" aria-hidden="true" tabindex="-1"></a>            t.strip() <span class="cf">for</span> t <span class="kw">in</span> re.split(pattern, text) <span class="cf">if</span> t.strip()</span>
<span id="cb37-135"><a href="#cb37-135" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb37-136"><a href="#cb37-136" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.token_to_id[t] <span class="cf">for</span> t <span class="kw">in</span> tokens]</span>
<span id="cb37-137"><a href="#cb37-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-138"><a href="#cb37-138" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, ids):</span>
<span id="cb37-139"><a href="#cb37-139" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Decode a list of token IDs into a string."""</span></span>
<span id="cb37-140"><a href="#cb37-140" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">" "</span>.join(<span class="va">self</span>.id_to_token[i] <span class="cf">for</span> i <span class="kw">in</span> ids)</span>
<span id="cb37-141"><a href="#cb37-141" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> re.sub(<span class="vs">r'\s+([,.?!"()\'])'</span>, <span class="vs">r'\1'</span>, text)</span>
<span id="cb37-142"><a href="#cb37-142" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-143"><a href="#cb37-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-144"><a href="#cb37-144" aria-hidden="true" tabindex="-1"></a>Let's test our new <span class="in">`Tokenizer`</span> class</span>
<span id="cb37-145"><a href="#cb37-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-148"><a href="#cb37-148" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-149"><a href="#cb37-149" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-150"><a href="#cb37-150" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">""""It's the last he painted, you know," </span></span>
<span id="cb37-151"><a href="#cb37-151" aria-hidden="true" tabindex="-1"></a><span class="st">       Mrs. Gisburn said with pardonable pride."""</span></span>
<span id="cb37-152"><a href="#cb37-152" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(vocab)</span>
<span id="cb37-153"><a href="#cb37-153" aria-hidden="true" tabindex="-1"></a>ids <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb37-154"><a href="#cb37-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-155"><a href="#cb37-155" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Encoded:</span><span class="ch">\n</span><span class="sc">{</span>ids<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-156"><a href="#cb37-156" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Decoded:</span><span class="ch">\n</span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(ids)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-157"><a href="#cb37-157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-158"><a href="#cb37-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-159"><a href="#cb37-159" aria-hidden="true" tabindex="-1"></a>The vocabulary of our tokenizer is limited by the tokens that appear in the corpus. Any word that is not in the corpus cannot be encoded.</span>
<span id="cb37-160"><a href="#cb37-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-161"><a href="#cb37-161" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb37-162"><a href="#cb37-162" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Hello, do you like tea?"</span></span>
<span id="cb37-163"><a href="#cb37-163" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.encode(text))</span>
<span id="cb37-164"><a href="#cb37-164" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-165"><a href="#cb37-165" aria-hidden="true" tabindex="-1"></a>Causes a <span class="in">`KeyError`</span>:</span>
<span id="cb37-166"><a href="#cb37-166" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb37-167"><a href="#cb37-167" aria-hidden="true" tabindex="-1"></a><span class="ex">KeyError:</span> <span class="st">'Hello'</span></span>
<span id="cb37-168"><a href="#cb37-168" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-169"><a href="#cb37-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-170"><a href="#cb37-170" aria-hidden="true" tabindex="-1"></a><span class="fu">## Handling Unknown Tokens and Special Tokens</span></span>
<span id="cb37-171"><a href="#cb37-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-172"><a href="#cb37-172" aria-hidden="true" tabindex="-1"></a>In practical LLM applications, we need to handle words that don't appear in our training vocabulary. This is where **unknown tokens** and other **special tokens** become essential.</span>
<span id="cb37-173"><a href="#cb37-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-174"><a href="#cb37-174" aria-hidden="true" tabindex="-1"></a><span class="fu">### Special Tokens in LLMs</span></span>
<span id="cb37-175"><a href="#cb37-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-176"><a href="#cb37-176" aria-hidden="true" tabindex="-1"></a>Modern tokenizers use several special tokens:</span>
<span id="cb37-177"><a href="#cb37-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-178"><a href="#cb37-178" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`&lt;|unk|&gt;`</span> or <span class="in">`[UNK]`</span>: Unknown token for out-of-vocabulary words</span>
<span id="cb37-179"><a href="#cb37-179" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`&lt;|endoftext|&gt;`</span> or <span class="in">`[EOS]`</span>: End of sequence/text marker</span>
<span id="cb37-180"><a href="#cb37-180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`&lt;|startoftext|&gt;`</span> or <span class="in">`[BOS]`</span>: Beginning of sequence marker (less common in GPT-style models)</span>
<span id="cb37-181"><a href="#cb37-181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`&lt;|pad|&gt;`</span>: Padding token for batch processing</span>
<span id="cb37-182"><a href="#cb37-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-183"><a href="#cb37-183" aria-hidden="true" tabindex="-1"></a>Let's create an enhanced tokenizer that handles unknown tokens:</span>
<span id="cb37-184"><a href="#cb37-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-187"><a href="#cb37-187" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-188"><a href="#cb37-188" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-189"><a href="#cb37-189" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EnhancedTokenizer:</span>
<span id="cb37-190"><a href="#cb37-190" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab):</span>
<span id="cb37-191"><a href="#cb37-191" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add special tokens to vocabulary</span></span>
<span id="cb37-192"><a href="#cb37-192" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.special_tokens <span class="op">=</span> {</span>
<span id="cb37-193"><a href="#cb37-193" aria-hidden="true" tabindex="-1"></a>            <span class="st">"&lt;|unk|&gt;"</span>: <span class="bu">len</span>(vocab),</span>
<span id="cb37-194"><a href="#cb37-194" aria-hidden="true" tabindex="-1"></a>            <span class="st">"&lt;|endoftext|&gt;"</span>: <span class="bu">len</span>(vocab) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb37-195"><a href="#cb37-195" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb37-196"><a href="#cb37-196" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-197"><a href="#cb37-197" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine original vocab with special tokens</span></span>
<span id="cb37-198"><a href="#cb37-198" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.full_vocab <span class="op">=</span> {<span class="op">**</span>vocab, <span class="op">**</span><span class="va">self</span>.special_tokens}</span>
<span id="cb37-199"><a href="#cb37-199" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_to_id <span class="op">=</span> <span class="va">self</span>.full_vocab</span>
<span id="cb37-200"><a href="#cb37-200" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.id_to_token <span class="op">=</span> {idx: token <span class="cf">for</span> token, idx <span class="kw">in</span> <span class="va">self</span>.full_vocab.items()}</span>
<span id="cb37-201"><a href="#cb37-201" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-202"><a href="#cb37-202" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store reverse mapping for special tokens</span></span>
<span id="cb37-203"><a href="#cb37-203" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.unk_token_id <span class="op">=</span> <span class="va">self</span>.special_tokens[<span class="st">"&lt;|unk|&gt;"</span>]</span>
<span id="cb37-204"><a href="#cb37-204" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.endoftext_token_id <span class="op">=</span> <span class="va">self</span>.special_tokens[<span class="st">"&lt;|endoftext|&gt;"</span>]</span>
<span id="cb37-205"><a href="#cb37-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-206"><a href="#cb37-206" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, text, add_endoftext<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb37-207"><a href="#cb37-207" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Encode text, handling unknown tokens."""</span></span>
<span id="cb37-208"><a href="#cb37-208" aria-hidden="true" tabindex="-1"></a>        pattern <span class="op">=</span> <span class="vs">r'([,.?_!"()\']|--|\s)'</span></span>
<span id="cb37-209"><a href="#cb37-209" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> [t.strip() <span class="cf">for</span> t <span class="kw">in</span> re.split(pattern, text) <span class="cf">if</span> t.strip()]</span>
<span id="cb37-210"><a href="#cb37-210" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-211"><a href="#cb37-211" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert tokens to IDs, using &lt;|unk|&gt; for unknown tokens</span></span>
<span id="cb37-212"><a href="#cb37-212" aria-hidden="true" tabindex="-1"></a>        ids <span class="op">=</span> []</span>
<span id="cb37-213"><a href="#cb37-213" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> token <span class="kw">in</span> tokens:</span>
<span id="cb37-214"><a href="#cb37-214" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> token <span class="kw">in</span> <span class="va">self</span>.token_to_id:</span>
<span id="cb37-215"><a href="#cb37-215" aria-hidden="true" tabindex="-1"></a>                ids.append(<span class="va">self</span>.token_to_id[token])</span>
<span id="cb37-216"><a href="#cb37-216" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb37-217"><a href="#cb37-217" aria-hidden="true" tabindex="-1"></a>                ids.append(<span class="va">self</span>.unk_token_id)  <span class="co"># Use unknown token</span></span>
<span id="cb37-218"><a href="#cb37-218" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-219"><a href="#cb37-219" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optionally add end-of-text token</span></span>
<span id="cb37-220"><a href="#cb37-220" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> add_endoftext:</span>
<span id="cb37-221"><a href="#cb37-221" aria-hidden="true" tabindex="-1"></a>            ids.append(<span class="va">self</span>.endoftext_token_id)</span>
<span id="cb37-222"><a href="#cb37-222" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb37-223"><a href="#cb37-223" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ids</span>
<span id="cb37-224"><a href="#cb37-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-225"><a href="#cb37-225" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, ids):</span>
<span id="cb37-226"><a href="#cb37-226" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Decode token IDs back to text."""</span></span>
<span id="cb37-227"><a href="#cb37-227" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> []</span>
<span id="cb37-228"><a href="#cb37-228" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="bu">id</span> <span class="kw">in</span> ids:</span>
<span id="cb37-229"><a href="#cb37-229" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">id</span> <span class="op">==</span> <span class="va">self</span>.endoftext_token_id:</span>
<span id="cb37-230"><a href="#cb37-230" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span>  <span class="co"># Stop at end-of-text token</span></span>
<span id="cb37-231"><a href="#cb37-231" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="bu">id</span> <span class="kw">in</span> <span class="va">self</span>.id_to_token:</span>
<span id="cb37-232"><a href="#cb37-232" aria-hidden="true" tabindex="-1"></a>                tokens.append(<span class="va">self</span>.id_to_token[<span class="bu">id</span>])</span>
<span id="cb37-233"><a href="#cb37-233" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb37-234"><a href="#cb37-234" aria-hidden="true" tabindex="-1"></a>                tokens.append(<span class="st">"&lt;|unk|&gt;"</span>)  <span class="co"># Fallback for invalid IDs</span></span>
<span id="cb37-235"><a href="#cb37-235" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb37-236"><a href="#cb37-236" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">" "</span>.join(tokens)</span>
<span id="cb37-237"><a href="#cb37-237" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> re.sub(<span class="vs">r'\s+([,.?!"()\'])'</span>, <span class="vs">r'\1'</span>, text)</span>
<span id="cb37-238"><a href="#cb37-238" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-239"><a href="#cb37-239" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> vocab_size(<span class="va">self</span>):</span>
<span id="cb37-240"><a href="#cb37-240" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Return the total vocabulary size including special tokens."""</span></span>
<span id="cb37-241"><a href="#cb37-241" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.full_vocab)</span>
<span id="cb37-242"><a href="#cb37-242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-243"><a href="#cb37-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-244"><a href="#cb37-244" aria-hidden="true" tabindex="-1"></a>Let's test our enhanced tokenizer:</span>
<span id="cb37-245"><a href="#cb37-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-248"><a href="#cb37-248" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-249"><a href="#cb37-249" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-250"><a href="#cb37-250" aria-hidden="true" tabindex="-1"></a>enhanced_tokenizer <span class="op">=</span> EnhancedTokenizer(vocab)</span>
<span id="cb37-251"><a href="#cb37-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-252"><a href="#cb37-252" aria-hidden="true" tabindex="-1"></a><span class="co"># Test with unknown words</span></span>
<span id="cb37-253"><a href="#cb37-253" aria-hidden="true" tabindex="-1"></a>text_with_unknown <span class="op">=</span> <span class="st">"Hello, do you like tea? This is amazing!"</span></span>
<span id="cb37-254"><a href="#cb37-254" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> enhanced_tokenizer.encode(text_with_unknown)</span>
<span id="cb37-255"><a href="#cb37-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-256"><a href="#cb37-256" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>text_with_unknown<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-257"><a href="#cb37-257" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Encoded IDs: </span><span class="sc">{</span>encoded<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-258"><a href="#cb37-258" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Decoded text: </span><span class="sc">{</span>enhanced_tokenizer<span class="sc">.</span>decode(encoded)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-259"><a href="#cb37-259" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Vocabulary size: </span><span class="sc">{</span>enhanced_tokenizer<span class="sc">.</span>vocab_size()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-260"><a href="#cb37-260" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-261"><a href="#cb37-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-262"><a href="#cb37-262" aria-hidden="true" tabindex="-1"></a>Notice how unknown words like "Hello" and "amazing" are now handled gracefully using the <span class="in">`&lt;|unk|&gt;`</span> token, and the sequence ends with an <span class="in">`&lt;|endoftext|&gt;`</span> token.</span>
<span id="cb37-263"><a href="#cb37-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-266"><a href="#cb37-266" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-267"><a href="#cb37-267" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-268"><a href="#cb37-268" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's see what the special token IDs are</span></span>
<span id="cb37-269"><a href="#cb37-269" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Unknown token '&lt;|unk|&gt;' has ID: </span><span class="sc">{</span>enhanced_tokenizer<span class="sc">.</span>unk_token_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-270"><a href="#cb37-270" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"End-of-text token '&lt;|endoftext|&gt;' has ID: </span><span class="sc">{</span>enhanced_tokenizer<span class="sc">.</span>endoftext_token_id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-271"><a href="#cb37-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-272"><a href="#cb37-272" aria-hidden="true" tabindex="-1"></a><span class="co"># Test decoding with end-of-text in the middle</span></span>
<span id="cb37-273"><a href="#cb37-273" aria-hidden="true" tabindex="-1"></a>test_ids <span class="op">=</span> [<span class="dv">100</span>, <span class="dv">200</span>, enhanced_tokenizer.endoftext_token_id, <span class="dv">300</span>, <span class="dv">400</span>]</span>
<span id="cb37-274"><a href="#cb37-274" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Decoding stops at &lt;|endoftext|&gt;: </span><span class="sc">{</span>enhanced_tokenizer<span class="sc">.</span>decode(test_ids)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-275"><a href="#cb37-275" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-276"><a href="#cb37-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-277"><a href="#cb37-277" aria-hidden="true" tabindex="-1"></a><span class="fu">## Byte Pair Encoding (BPE) with Tiktoken</span></span>
<span id="cb37-278"><a href="#cb37-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-279"><a href="#cb37-279" aria-hidden="true" tabindex="-1"></a>While our simple tokenizer is educational, production LLMs use more sophisticated tokenization schemes like **Byte Pair Encoding (BPE)**. BPE creates subword tokens that balance vocabulary size with representation efficiency.</span>
<span id="cb37-280"><a href="#cb37-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-281"><a href="#cb37-281" aria-hidden="true" tabindex="-1"></a>The <span class="in">`tiktoken`</span> library provides access to the same tokenizers used by OpenAI's GPT models.</span>
<span id="cb37-282"><a href="#cb37-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-285"><a href="#cb37-285" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-286"><a href="#cb37-286" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-287"><a href="#cb37-287" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tiktoken</span>
<span id="cb37-288"><a href="#cb37-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-289"><a href="#cb37-289" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the GPT-2 tokenizer (used by GPT-2, GPT-3, and early GPT-4)</span></span>
<span id="cb37-290"><a href="#cb37-290" aria-hidden="true" tabindex="-1"></a>gpt2_tokenizer <span class="op">=</span> tiktoken.get_encoding(<span class="st">"gpt2"</span>)</span>
<span id="cb37-291"><a href="#cb37-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-292"><a href="#cb37-292" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GPT-2 vocabulary size: </span><span class="sc">{</span>gpt2_tokenizer<span class="sc">.</span>n_vocab<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb37-293"><a href="#cb37-293" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-294"><a href="#cb37-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-295"><a href="#cb37-295" aria-hidden="true" tabindex="-1"></a>Let's compare our simple tokenizer with GPT-2's BPE tokenizer:</span>
<span id="cb37-296"><a href="#cb37-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-299"><a href="#cb37-299" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-300"><a href="#cb37-300" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-301"><a href="#cb37-301" aria-hidden="true" tabindex="-1"></a>test_text <span class="op">=</span> <span class="st">"Hello, world! This demonstrates byte pair encoding tokenization."</span></span>
<span id="cb37-302"><a href="#cb37-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-303"><a href="#cb37-303" aria-hidden="true" tabindex="-1"></a><span class="co"># Our enhanced tokenizer</span></span>
<span id="cb37-304"><a href="#cb37-304" aria-hidden="true" tabindex="-1"></a>our_tokens <span class="op">=</span> enhanced_tokenizer.encode(test_text, add_endoftext<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-305"><a href="#cb37-305" aria-hidden="true" tabindex="-1"></a>our_decoded <span class="op">=</span> enhanced_tokenizer.decode(our_tokens)</span>
<span id="cb37-306"><a href="#cb37-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-307"><a href="#cb37-307" aria-hidden="true" tabindex="-1"></a><span class="co"># GPT-2 BPE tokenizer  </span></span>
<span id="cb37-308"><a href="#cb37-308" aria-hidden="true" tabindex="-1"></a>gpt2_tokens <span class="op">=</span> gpt2_tokenizer.encode(test_text)</span>
<span id="cb37-309"><a href="#cb37-309" aria-hidden="true" tabindex="-1"></a>gpt2_decoded <span class="op">=</span> gpt2_tokenizer.decode(gpt2_tokens)</span>
<span id="cb37-310"><a href="#cb37-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-311"><a href="#cb37-311" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== Comparison ==="</span>)</span>
<span id="cb37-312"><a href="#cb37-312" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>test_text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-313"><a href="#cb37-313" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb37-314"><a href="#cb37-314" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Our tokenizer:"</span>)</span>
<span id="cb37-315"><a href="#cb37-315" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Tokens: </span><span class="sc">{</span>our_tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-316"><a href="#cb37-316" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Count: </span><span class="sc">{</span><span class="bu">len</span>(our_tokens)<span class="sc">}</span><span class="ss"> tokens"</span>)</span>
<span id="cb37-317"><a href="#cb37-317" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Decoded: </span><span class="sc">{</span>our_decoded<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-318"><a href="#cb37-318" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb37-319"><a href="#cb37-319" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GPT-2 BPE tokenizer:"</span>)</span>
<span id="cb37-320"><a href="#cb37-320" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Tokens: </span><span class="sc">{</span>gpt2_tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-321"><a href="#cb37-321" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Count: </span><span class="sc">{</span><span class="bu">len</span>(gpt2_tokens)<span class="sc">}</span><span class="ss"> tokens"</span>) </span>
<span id="cb37-322"><a href="#cb37-322" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Decoded: </span><span class="sc">{</span>gpt2_decoded<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-323"><a href="#cb37-323" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-324"><a href="#cb37-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-325"><a href="#cb37-325" aria-hidden="true" tabindex="-1"></a><span class="fu">### Understanding BPE Token Breakdown</span></span>
<span id="cb37-326"><a href="#cb37-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-327"><a href="#cb37-327" aria-hidden="true" tabindex="-1"></a>BPE tokenizers split text into subword units. Let's examine how GPT-2 breaks down different types of text:</span>
<span id="cb37-328"><a href="#cb37-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-331"><a href="#cb37-331" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-332"><a href="#cb37-332" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-333"><a href="#cb37-333" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_tokenization(text, tokenizer_name<span class="op">=</span><span class="st">"gpt2"</span>):</span>
<span id="cb37-334"><a href="#cb37-334" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Analyze how tiktoken breaks down text."""</span></span>
<span id="cb37-335"><a href="#cb37-335" aria-hidden="true" tabindex="-1"></a>    tokenizer <span class="op">=</span> tiktoken.get_encoding(tokenizer_name)</span>
<span id="cb37-336"><a href="#cb37-336" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenizer.encode(text)</span>
<span id="cb37-337"><a href="#cb37-337" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-338"><a href="#cb37-338" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Text: '</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb37-339"><a href="#cb37-339" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-340"><a href="#cb37-340" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token count: </span><span class="sc">{</span><span class="bu">len</span>(tokens)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-341"><a href="#cb37-341" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Token breakdown:"</span>)</span>
<span id="cb37-342"><a href="#cb37-342" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-343"><a href="#cb37-343" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, token_id <span class="kw">in</span> <span class="bu">enumerate</span>(tokens):</span>
<span id="cb37-344"><a href="#cb37-344" aria-hidden="true" tabindex="-1"></a>        token_text <span class="op">=</span> tokenizer.decode([token_id])</span>
<span id="cb37-345"><a href="#cb37-345" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>i<span class="sc">:2d}</span><span class="ss">: </span><span class="sc">{</span>token_id<span class="sc">:5d}</span><span class="ss"> -&gt; '</span><span class="sc">{</span>token_text<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb37-346"><a href="#cb37-346" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb37-347"><a href="#cb37-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-348"><a href="#cb37-348" aria-hidden="true" tabindex="-1"></a><span class="co"># Test different types of text</span></span>
<span id="cb37-349"><a href="#cb37-349" aria-hidden="true" tabindex="-1"></a>analyze_tokenization(<span class="st">"Hello world"</span>)</span>
<span id="cb37-350"><a href="#cb37-350" aria-hidden="true" tabindex="-1"></a>analyze_tokenization(<span class="st">"Tokenization"</span>)  </span>
<span id="cb37-351"><a href="#cb37-351" aria-hidden="true" tabindex="-1"></a>analyze_tokenization(<span class="st">"supercalifragilisticexpialidocious"</span>)</span>
<span id="cb37-352"><a href="#cb37-352" aria-hidden="true" tabindex="-1"></a>analyze_tokenization(<span class="st">"AI/ML researcher"</span>)</span>
<span id="cb37-353"><a href="#cb37-353" aria-hidden="true" tabindex="-1"></a>analyze_tokenization(<span class="st">"The year 2024"</span>)</span>
<span id="cb37-354"><a href="#cb37-354" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-355"><a href="#cb37-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-356"><a href="#cb37-356" aria-hidden="true" tabindex="-1"></a><span class="fu">### Working with Different Tiktoken Encodings</span></span>
<span id="cb37-357"><a href="#cb37-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-358"><a href="#cb37-358" aria-hidden="true" tabindex="-1"></a>OpenAI uses different encodings for different models:</span>
<span id="cb37-359"><a href="#cb37-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-362"><a href="#cb37-362" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-363"><a href="#cb37-363" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-364"><a href="#cb37-364" aria-hidden="true" tabindex="-1"></a><span class="co"># Available encodings</span></span>
<span id="cb37-365"><a href="#cb37-365" aria-hidden="true" tabindex="-1"></a>available_encodings <span class="op">=</span> [<span class="st">"gpt2"</span>, <span class="st">"p50k_base"</span>, <span class="st">"cl100k_base"</span>]</span>
<span id="cb37-366"><a href="#cb37-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-367"><a href="#cb37-367" aria-hidden="true" tabindex="-1"></a>test_text <span class="op">=</span> <span class="st">"Geospatial foundation models revolutionize remote sensing!"</span></span>
<span id="cb37-368"><a href="#cb37-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-369"><a href="#cb37-369" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> encoding_name <span class="kw">in</span> available_encodings:</span>
<span id="cb37-370"><a href="#cb37-370" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb37-371"><a href="#cb37-371" aria-hidden="true" tabindex="-1"></a>        tokenizer <span class="op">=</span> tiktoken.get_encoding(encoding_name)</span>
<span id="cb37-372"><a href="#cb37-372" aria-hidden="true" tabindex="-1"></a>        tokens <span class="op">=</span> tokenizer.encode(test_text)</span>
<span id="cb37-373"><a href="#cb37-373" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-374"><a href="#cb37-374" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>encoding_name<span class="sc">:12s}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">len</span>(tokens)<span class="sc">:2d}</span><span class="ss"> tokens, vocab size: </span><span class="sc">{</span>tokenizer<span class="sc">.</span>n_vocab<span class="sc">:6,}</span><span class="ss">"</span>)</span>
<span id="cb37-375"><a href="#cb37-375" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"              Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-376"><a href="#cb37-376" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb37-377"><a href="#cb37-377" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb37-378"><a href="#cb37-378" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>encoding_name<span class="sc">}</span><span class="ss">: Error - </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-379"><a href="#cb37-379" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-380"><a href="#cb37-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-381"><a href="#cb37-381" aria-hidden="true" tabindex="-1"></a><span class="fu">### BPE for Geospatial Text</span></span>
<span id="cb37-382"><a href="#cb37-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-383"><a href="#cb37-383" aria-hidden="true" tabindex="-1"></a>Let's see how BPE handles domain-specific geospatial terminology:</span>
<span id="cb37-384"><a href="#cb37-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-387"><a href="#cb37-387" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-388"><a href="#cb37-388" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-389"><a href="#cb37-389" aria-hidden="true" tabindex="-1"></a>geospatial_texts <span class="op">=</span> [</span>
<span id="cb37-390"><a href="#cb37-390" aria-hidden="true" tabindex="-1"></a>    <span class="st">"NDVI vegetation index analysis"</span>,</span>
<span id="cb37-391"><a href="#cb37-391" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Landsat-8 multispectral imagery"</span>, </span>
<span id="cb37-392"><a href="#cb37-392" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Convolutional neural networks for land cover classification"</span>,</span>
<span id="cb37-393"><a href="#cb37-393" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sentinel-2 satellite data preprocessing"</span>,</span>
<span id="cb37-394"><a href="#cb37-394" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Geospatial foundation models fine-tuning"</span></span>
<span id="cb37-395"><a href="#cb37-395" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb37-396"><a href="#cb37-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-397"><a href="#cb37-397" aria-hidden="true" tabindex="-1"></a>gpt2_enc <span class="op">=</span> tiktoken.get_encoding(<span class="st">"gpt2"</span>)</span>
<span id="cb37-398"><a href="#cb37-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-399"><a href="#cb37-399" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== Geospatial Text Tokenization ==="</span>)</span>
<span id="cb37-400"><a href="#cb37-400" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> geospatial_texts:</span>
<span id="cb37-401"><a href="#cb37-401" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> gpt2_enc.encode(text)</span>
<span id="cb37-402"><a href="#cb37-402" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">'"</span>)</span>
<span id="cb37-403"><a href="#cb37-403" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  -&gt; </span><span class="sc">{</span><span class="bu">len</span>(tokens)<span class="sc">}</span><span class="ss"> tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-404"><a href="#cb37-404" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb37-405"><a href="#cb37-405" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-406"><a href="#cb37-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-407"><a href="#cb37-407" aria-hidden="true" tabindex="-1"></a><span class="fu">### Understanding Token Efficiency</span></span>
<span id="cb37-408"><a href="#cb37-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-409"><a href="#cb37-409" aria-hidden="true" tabindex="-1"></a>The choice of tokenizer affects model efficiency. Let's compare token counts for our text corpus:</span>
<span id="cb37-410"><a href="#cb37-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-413"><a href="#cb37-413" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-414"><a href="#cb37-414" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb37-415"><a href="#cb37-415" aria-hidden="true" tabindex="-1"></a><span class="co"># Use a sample from our corpus</span></span>
<span id="cb37-416"><a href="#cb37-416" aria-hidden="true" tabindex="-1"></a>sample_text <span class="op">=</span> raw_text[:<span class="dv">500</span>]  <span class="co"># First 500 characters</span></span>
<span id="cb37-417"><a href="#cb37-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-418"><a href="#cb37-418" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"=== Token Efficiency Comparison ==="</span>)</span>
<span id="cb37-419"><a href="#cb37-419" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Text length: </span><span class="sc">{</span><span class="bu">len</span>(sample_text)<span class="sc">}</span><span class="ss"> characters"</span>)</span>
<span id="cb37-420"><a href="#cb37-420" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb37-421"><a href="#cb37-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-422"><a href="#cb37-422" aria-hidden="true" tabindex="-1"></a><span class="co"># Our simple tokenizer</span></span>
<span id="cb37-423"><a href="#cb37-423" aria-hidden="true" tabindex="-1"></a>our_tokens <span class="op">=</span> enhanced_tokenizer.encode(sample_text, add_endoftext<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb37-424"><a href="#cb37-424" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Simple tokenizer: </span><span class="sc">{</span><span class="bu">len</span>(our_tokens)<span class="sc">}</span><span class="ss"> tokens"</span>)</span>
<span id="cb37-425"><a href="#cb37-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-426"><a href="#cb37-426" aria-hidden="true" tabindex="-1"></a><span class="co"># GPT-2 BPE</span></span>
<span id="cb37-427"><a href="#cb37-427" aria-hidden="true" tabindex="-1"></a>gpt2_tokens <span class="op">=</span> gpt2_tokenizer.encode(sample_text)</span>
<span id="cb37-428"><a href="#cb37-428" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GPT-2 BPE:       </span><span class="sc">{</span><span class="bu">len</span>(gpt2_tokens)<span class="sc">}</span><span class="ss"> tokens"</span>)</span>
<span id="cb37-429"><a href="#cb37-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-430"><a href="#cb37-430" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate efficiency</span></span>
<span id="cb37-431"><a href="#cb37-431" aria-hidden="true" tabindex="-1"></a>chars_per_token_simple <span class="op">=</span> <span class="bu">len</span>(sample_text) <span class="op">/</span> <span class="bu">len</span>(our_tokens)</span>
<span id="cb37-432"><a href="#cb37-432" aria-hidden="true" tabindex="-1"></a>chars_per_token_bpe <span class="op">=</span> <span class="bu">len</span>(sample_text) <span class="op">/</span> <span class="bu">len</span>(gpt2_tokens)</span>
<span id="cb37-433"><a href="#cb37-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-434"><a href="#cb37-434" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Characters per token:"</span>)</span>
<span id="cb37-435"><a href="#cb37-435" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Simple: </span><span class="sc">{</span>chars_per_token_simple<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb37-436"><a href="#cb37-436" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  BPE:    </span><span class="sc">{</span>chars_per_token_bpe<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb37-437"><a href="#cb37-437" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  BPE is </span><span class="sc">{</span>chars_per_token_bpe<span class="op">/</span>chars_per_token_simple<span class="sc">:.1f}</span><span class="ss">x more efficient"</span>)</span>
<span id="cb37-438"><a href="#cb37-438" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-439"><a href="#cb37-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-440"><a href="#cb37-440" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Takeaways</span></span>
<span id="cb37-441"><a href="#cb37-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-442"><a href="#cb37-442" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Special Tokens**: Essential for handling unknown words and sequence boundaries</span>
<span id="cb37-443"><a href="#cb37-443" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Unknown Token Handling**: <span class="in">`&lt;|unk|&gt;`</span> tokens allow models to gracefully handle out-of-vocabulary words</span>
<span id="cb37-444"><a href="#cb37-444" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**End-of-Text Tokens**: <span class="in">`&lt;|endoftext|&gt;`</span> tokens help models understand sequence boundaries</span>
<span id="cb37-445"><a href="#cb37-445" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**BPE Efficiency**: Byte Pair Encoding creates a good balance between vocabulary size and representation efficiency</span>
<span id="cb37-446"><a href="#cb37-446" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Domain Adaptation**: Different tokenizers may handle domain-specific text differently</span>
<span id="cb37-447"><a href="#cb37-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-448"><a href="#cb37-448" aria-hidden="true" tabindex="-1"></a>In geospatial AI applications, choosing the right tokenization strategy affects both model performance and computational efficiency, especially when working with specialized terminology from remote sensing and Earth science domains.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/geog-logo.png" class="img-fluid figure-img" width="250"></p>
<figcaption>Department of Geography logo</figcaption>
</figure>
</div>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This website is built with <a href="https://github.com/kcaylor/GEOG-288KC-geospatial-foundation-models"><i class="fa-brands fa-github" title="the github octocat logo" aria-label="github"></i></a> and <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>