<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Model Inference &amp; Feature Extraction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">GEOG 288KC</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">🏠 home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Syllabus.html"> 
<span class="menu-text">📋 syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-weekly-sessions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">💻 weekly sessions</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-weekly-sessions">    
        <li>
    <a class="dropdown-item" href="../../chapters/c01-geospatial-data-foundations.html">
 <span class="dropdown-text">Week 1 - 🚀 Core Tools and Data Access</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c02-spatial-temporal-attention-mechanisms.html">
 <span class="dropdown-text">Week 2 - ⚡ Rapid Remote Sensing Preprocessing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c03-complete-gfm-architecture.html">
 <span class="dropdown-text">Week 3 - 🤖 Machine Learning on Remote Sensing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c04-pretraining-implementation.html">
 <span class="dropdown-text">Week 4 - 🏗️ Foundation Models in Practice</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c05-training-loop-optimization.html">
 <span class="dropdown-text">Week 5 - 🔧 Fine-Tuning &amp; Transfer Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c06-model-evaluation-analysis.html">
 <span class="dropdown-text">Week 6 - ⏰ Spatiotemporal Modeling &amp; Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-cheatsheets" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">👀 cheatsheets</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-cheatsheets">    
        <li>
    <a class="dropdown-item" href="../../cheatsheets.html">
 <span class="dropdown-text">📋 All Cheatsheets</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">⚡ Quick Starts</li>
        <li>
    <a class="dropdown-item" href="../../extras/cheatsheets/week01_imports.html">
 <span class="dropdown-text">Week 01: Import Guide</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-explainers" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">🧩 explainers</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-explainers">    
        <li class="dropdown-header">1️⃣ Week 1</li>
        <li>
    <a class="dropdown-item" href="../../extras/ai-ml-dl-fm-hierarchy.html">
 <span class="dropdown-text">🤖 AI/ML/DL/FM Hierarchy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/geospatial-foundation-model-predictions-standalone.html">
 <span class="dropdown-text">🎯 GFM Predictions (Standalone)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/geospatial-prediction-hierarchy.html">
 <span class="dropdown-text">✅ Geospatial Task/Prediction Types</span></a>
  </li>  
        <li class="dropdown-header">2️⃣ Week 2</li>
        <li>
    <a class="dropdown-item" href="../../chapters/c00a-foundation_model_architectures.html">
 <span class="dropdown-text">🏗️ Foundation Model Architectures</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c00b-introduction-to-deeplearning-architecture.html">
 <span class="dropdown-text">🎓 Introduction to Deep Learning Architecture</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-extras" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">📖 extras</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-extras">    
        <li class="dropdown-header">🎯 Practical Examples</li>
        <li>
    <a class="dropdown-item" href="../../extras/examples/normalization_comparison.html">
 <span class="dropdown-text">Normalization Comparison</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/resnet.html">
 <span class="dropdown-text">ResNet Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/text_encoder.html">
 <span class="dropdown-text">Text Encoder</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/tiling-and-patches.html">
 <span class="dropdown-text">Tiling and Patches</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/terratorch_workflows.html">
 <span class="dropdown-text">TerraTorch Workflows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/resources/course_resources.html">
 <span class="dropdown-text">📚 Reference Materials</span></a>
  </li>  
        <li class="dropdown-header">📁 Project Templates</li>
        <li>
    <a class="dropdown-item" href="../../extras/projects/project-proposal-template.html">
 <span class="dropdown-text">Project Proposal Template</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/projects/mvp-template.html">
 <span class="dropdown-text">Project Results Template</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gfms-from-scratch/gfms-from-scratch.github.io" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Model Inference &amp; Feature Extraction</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Running inference and extracting features</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-model-inference" id="toc-introduction-to-model-inference" class="nav-link active" data-scroll-target="#introduction-to-model-inference">Introduction to Model Inference</a></li>
  <li><a href="#basic-inference-patterns" id="toc-basic-inference-patterns" class="nav-link" data-scroll-target="#basic-inference-patterns">Basic Inference Patterns</a>
  <ul class="collapse">
  <li><a href="#single-image-inference" id="toc-single-image-inference" class="nav-link" data-scroll-target="#single-image-inference">Single image inference</a></li>
  <li><a href="#batch-inference" id="toc-batch-inference" class="nav-link" data-scroll-target="#batch-inference">Batch inference</a></li>
  </ul></li>
  <li><a href="#feature-extraction-techniques" id="toc-feature-extraction-techniques" class="nav-link" data-scroll-target="#feature-extraction-techniques">Feature Extraction Techniques</a>
  <ul class="collapse">
  <li><a href="#layer-wise-feature-extraction" id="toc-layer-wise-feature-extraction" class="nav-link" data-scroll-target="#layer-wise-feature-extraction">Layer-wise feature extraction</a></li>
  <li><a href="#multi-scale-feature-extraction" id="toc-multi-scale-feature-extraction" class="nav-link" data-scroll-target="#multi-scale-feature-extraction">Multi-scale feature extraction</a></li>
  </ul></li>
  <li><a href="#advanced-inference-techniques" id="toc-advanced-inference-techniques" class="nav-link" data-scroll-target="#advanced-inference-techniques">Advanced Inference Techniques</a>
  <ul class="collapse">
  <li><a href="#attention-map-visualization" id="toc-attention-map-visualization" class="nav-link" data-scroll-target="#attention-map-visualization">Attention map visualization</a></li>
  <li><a href="#gradient-based-explanations" id="toc-gradient-based-explanations" class="nav-link" data-scroll-target="#gradient-based-explanations">Gradient-based explanations</a></li>
  </ul></li>
  <li><a href="#feature-analysis-and-dimensionality-reduction" id="toc-feature-analysis-and-dimensionality-reduction" class="nav-link" data-scroll-target="#feature-analysis-and-dimensionality-reduction">Feature Analysis and Dimensionality Reduction</a>
  <ul class="collapse">
  <li><a href="#pca-analysis-of-features" id="toc-pca-analysis-of-features" class="nav-link" data-scroll-target="#pca-analysis-of-features">PCA analysis of features</a></li>
  <li><a href="#t-sne-visualization" id="toc-t-sne-visualization" class="nav-link" data-scroll-target="#t-sne-visualization">t-SNE visualization</a></li>
  </ul></li>
  <li><a href="#inference-optimization" id="toc-inference-optimization" class="nav-link" data-scroll-target="#inference-optimization">Inference Optimization</a>
  <ul class="collapse">
  <li><a href="#model-quantization-for-faster-inference" id="toc-model-quantization-for-faster-inference" class="nav-link" data-scroll-target="#model-quantization-for-faster-inference">Model quantization for faster inference</a></li>
  <li><a href="#batch-size-optimization" id="toc-batch-size-optimization" class="nav-link" data-scroll-target="#batch-size-optimization">Batch size optimization</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="introduction-to-model-inference" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-model-inference">Introduction to Model Inference</h2>
<p>Model inference and feature extraction are crucial for applying trained models to satellite imagery analysis and extracting meaningful representations.</p>
<div id="25dd2933" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CUDA available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch version: 2.7.1
CUDA available: False</code></pre>
</div>
</div>
</section>
<section id="basic-inference-patterns" class="level2">
<h2 class="anchored" data-anchor-id="basic-inference-patterns">Basic Inference Patterns</h2>
<section id="single-image-inference" class="level3">
<h3 class="anchored" data-anchor-id="single-image-inference">Single image inference</h3>
<div id="7f6f4f6b" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GeospatialClassifier(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Example geospatial classifier for demonstration"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_channels<span class="op">=</span><span class="dv">6</span>, num_classes<span class="op">=</span><span class="dv">10</span>, embed_dim<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature extraction layers</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(num_channels, <span class="dv">64</span>, <span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global pooling and classification</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.global_pool <span class="op">=</span> nn.AdaptiveAvgPool2d(<span class="dv">1</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(<span class="dv">256</span>, num_classes)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature embedding layer</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_embed <span class="op">=</span> nn.Linear(<span class="dv">256</span>, embed_dim)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, return_features<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature extraction</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> F.relu(<span class="va">self</span>.conv3(x))</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global pooling</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        pooled <span class="op">=</span> <span class="va">self</span>.global_pool(features).flatten(<span class="dv">1</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classification</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.classifier(pooled)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_features:</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>            embeddings <span class="op">=</span> <span class="va">self</span>.feature_embed(pooled)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>                <span class="st">'logits'</span>: logits,</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>                <span class="st">'features'</span>: embeddings,</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>                <span class="st">'spatial_features'</span>: features,</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>                <span class="st">'pooled_features'</span>: pooled</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GeospatialClassifier(num_channels<span class="op">=</span><span class="dv">6</span>, num_classes<span class="op">=</span><span class="dv">10</span>, embed_dim<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Single image inference</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>sample_image <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)  <span class="co"># Batch of 1, 6 channels</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Basic inference</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(sample_image)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inference with features</span></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(sample_image, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>sample_image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predictions shape: </span><span class="sc">{</span>predictions<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Logits shape: </span><span class="sc">{</span>outputs[<span class="st">'logits'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Features shape: </span><span class="sc">{</span>outputs[<span class="st">'features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Spatial features shape: </span><span class="sc">{</span>outputs[<span class="st">'spatial_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([1, 6, 224, 224])
Predictions shape: torch.Size([1, 10])
Logits shape: torch.Size([1, 10])
Features shape: torch.Size([1, 256])
Spatial features shape: torch.Size([1, 256, 28, 28])</code></pre>
</div>
</div>
</section>
<section id="batch-inference" class="level3">
<h3 class="anchored" data-anchor-id="batch-inference">Batch inference</h3>
<div id="897c84b4" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_inference(model, images, batch_size<span class="op">=</span><span class="dv">32</span>, device<span class="op">=</span><span class="st">'cpu'</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Perform batch inference on multiple images"""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    all_predictions <span class="op">=</span> []</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    all_features <span class="op">=</span> []</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process in batches</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    n_images <span class="op">=</span> <span class="bu">len</span>(images)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    n_batches <span class="op">=</span> (n_images <span class="op">+</span> batch_size <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_batches):</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>            start_idx <span class="op">=</span> i <span class="op">*</span> batch_size</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>            end_idx <span class="op">=</span> <span class="bu">min</span>(start_idx <span class="op">+</span> batch_size, n_images)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>            batch <span class="op">=</span> images[start_idx:end_idx].to(device)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get predictions and features</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(batch, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            all_predictions.append(outputs[<span class="st">'logits'</span>].cpu())</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>            all_features.append(outputs[<span class="st">'features'</span>].cpu())</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Processed batch </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_batches<span class="sc">}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">'</span><span class="ch">\r</span><span class="st">'</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Concatenate results</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    final_predictions <span class="op">=</span> torch.cat(all_predictions, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    final_features <span class="op">=</span> torch.cat(all_features, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Completed inference on </span><span class="sc">{</span>n_images<span class="sc">}</span><span class="ss"> images"</span>)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_predictions, final_features</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sample batch</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>batch_images <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Run batch inference</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>predictions, features <span class="op">=</span> batch_inference(model, batch_images, batch_size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch predictions shape: </span><span class="sc">{</span>predictions<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch features shape: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Processed batch 1/7Processed batch 2/7Processed batch 3/7Processed batch 4/7Processed batch 5/7Processed batch 6/7Processed batch 7/7
Completed inference on 100 images
Batch predictions shape: torch.Size([100, 10])
Batch features shape: torch.Size([100, 256])</code></pre>
</div>
</div>
</section>
</section>
<section id="feature-extraction-techniques" class="level2">
<h2 class="anchored" data-anchor-id="feature-extraction-techniques">Feature Extraction Techniques</h2>
<section id="layer-wise-feature-extraction" class="level3">
<h3 class="anchored" data-anchor-id="layer-wise-feature-extraction">Layer-wise feature extraction</h3>
<div id="cda188ab" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeatureExtractor:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract features from specific layers of a model"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, layer_names<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> {}</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> layer_names <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract from all named modules</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>            layer_names <span class="op">=</span> [name <span class="cf">for</span> name, _ <span class="kw">in</span> model.named_modules() <span class="cf">if</span> name]</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_hooks(layer_names)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_hooks(<span class="va">self</span>, layer_names):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register forward hooks for feature extraction"""</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> make_hook(name):</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> hook(module, <span class="bu">input</span>, output):</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Store detached copy to avoid gradient tracking</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">isinstance</span>(output, torch.Tensor):</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> output.detach().cpu()</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, (<span class="bu">list</span>, <span class="bu">tuple</span>)):</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> [o.detach().cpu() <span class="cf">if</span> <span class="bu">isinstance</span>(o, torch.Tensor) <span class="cf">else</span> o <span class="cf">for</span> o <span class="kw">in</span> output]</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, <span class="bu">dict</span>):</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> {k: v.detach().cpu() <span class="cf">if</span> <span class="bu">isinstance</span>(v, torch.Tensor) <span class="cf">else</span> v </span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>                                         <span class="cf">for</span> k, v <span class="kw">in</span> output.items()}</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> hook</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register hooks</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.model.named_modules():</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> name <span class="kw">in</span> layer_names:</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                handle <span class="op">=</span> module.register_forward_hook(make_hook(name))</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(handle)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Registered hook for layer: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract(<span class="va">self</span>, images):</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract features from registered layers"""</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features.clear()</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass triggers hooks</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.features.copy()</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remove_hooks(<span class="va">self</span>):</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Remove all registered hooks"""</span></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>            hook.remove()</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks.clear()</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Create feature extractor</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> FeatureExtractor(</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    layer_names<span class="op">=</span>[<span class="st">'conv1'</span>, <span class="st">'conv2'</span>, <span class="st">'conv3'</span>, <span class="st">'global_pool'</span>]</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract features</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>extracted_features <span class="op">=</span> extractor.extract(sample_input)</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Extracted features:"</span>)</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer_name, features <span class="kw">in</span> extracted_features.items():</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(features, torch.Tensor):</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">type</span>(features)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean up</span></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>extractor.remove_hooks()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Registered hook for layer: conv1
Registered hook for layer: conv2
Registered hook for layer: conv3
Registered hook for layer: global_pool
Extracted features:
conv1: torch.Size([4, 64, 112, 112])
conv2: torch.Size([4, 128, 56, 56])
conv3: torch.Size([4, 256, 28, 28])
global_pool: torch.Size([4, 256, 1, 1])</code></pre>
</div>
</div>
</section>
<section id="multi-scale-feature-extraction" class="level3">
<h3 class="anchored" data-anchor-id="multi-scale-feature-extraction">Multi-scale feature extraction</h3>
<div id="fe08d07d" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiScaleFeatureExtractor(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract features at multiple scales"""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, backbone):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> backbone</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature pyramid levels</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scales <span class="op">=</span> [<span class="fl">1.0</span>, <span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>]</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        batch_size, channels, height, width <span class="op">=</span> x.shape</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        multiscale_features <span class="op">=</span> {}</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> scale <span class="kw">in</span> <span class="va">self</span>.scales:</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resize input</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> scale <span class="op">!=</span> <span class="fl">1.0</span>:</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>                new_size <span class="op">=</span> (<span class="bu">int</span>(height <span class="op">*</span> scale), <span class="bu">int</span>(width <span class="op">*</span> scale))</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>                scaled_input <span class="op">=</span> F.interpolate(x, size<span class="op">=</span>new_size, mode<span class="op">=</span><span class="st">'bilinear'</span>, align_corners<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>                scaled_input <span class="op">=</span> x</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract features</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> <span class="va">self</span>.backbone(scaled_input, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store features with scale info</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>            scale_key <span class="op">=</span> <span class="ss">f"scale_</span><span class="sc">{</span>scale<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>            multiscale_features[scale_key] <span class="op">=</span> {</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>                <span class="st">'features'</span>: outputs[<span class="st">'features'</span>],</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>                <span class="st">'spatial_features'</span>: outputs[<span class="st">'spatial_features'</span>],</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>                <span class="st">'input_size'</span>: scaled_input.shape[<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> multiscale_features</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Create multi-scale extractor</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>multiscale_extractor <span class="op">=</span> MultiScaleFeatureExtractor(model)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>multiscale_extractor.<span class="bu">eval</span>()</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract multi-scale features</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>multiscale_features <span class="op">=</span> multiscale_extractor(sample_input)</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Multi-scale features:"</span>)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> scale, features <span class="kw">in</span> multiscale_features.items():</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>scale<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Features: </span><span class="sc">{</span>features[<span class="st">'features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Spatial: </span><span class="sc">{</span>features[<span class="st">'spatial_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Input size: </span><span class="sc">{</span>features[<span class="st">'input_size'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Multi-scale features:
scale_1.00:
  Features: torch.Size([2, 256])
  Spatial: torch.Size([2, 256, 28, 28])
  Input size: torch.Size([224, 224])
scale_0.75:
  Features: torch.Size([2, 256])
  Spatial: torch.Size([2, 256, 21, 21])
  Input size: torch.Size([168, 168])
scale_0.50:
  Features: torch.Size([2, 256])
  Spatial: torch.Size([2, 256, 14, 14])
  Input size: torch.Size([112, 112])
scale_0.25:
  Features: torch.Size([2, 256])
  Spatial: torch.Size([2, 256, 7, 7])
  Input size: torch.Size([56, 56])</code></pre>
</div>
</div>
</section>
</section>
<section id="advanced-inference-techniques" class="level2">
<h2 class="anchored" data-anchor-id="advanced-inference-techniques">Advanced Inference Techniques</h2>
<section id="attention-map-visualization" class="level3">
<h3 class="anchored" data-anchor-id="attention-map-visualization">Attention map visualization</h3>
<div id="2bae7976" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionExtractor:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract and visualize attention maps"""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model):</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_maps <span class="op">=</span> {}</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_attention_hooks(<span class="va">self</span>):</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register hooks for attention layers"""</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> attention_hook(name):</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> hook(module, <span class="bu">input</span>, output):</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>                <span class="co"># For attention mechanisms, we typically want the attention weights</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>                <span class="co"># This is a simplified example - actual implementation depends on model architecture</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">hasattr</span>(module, <span class="st">'attention_weights'</span>):</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> module.attention_weights.detach().cpu()</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, <span class="bu">tuple</span>) <span class="kw">and</span> <span class="bu">len</span>(output) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Assume second output contains attention weights</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> output[<span class="dv">1</span>].detach().cpu()</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">hasattr</span>(output, <span class="st">'attentions'</span>):</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> output.attentions.detach().cpu()</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> hook</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Look for attention-related modules</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.model.named_modules():</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">'attention'</span> <span class="kw">in</span> name.lower() <span class="kw">or</span> <span class="st">'attn'</span> <span class="kw">in</span> name.lower():</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>                handle <span class="op">=</span> module.register_forward_hook(attention_hook(name))</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(handle)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Registered attention hook: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract_attention(<span class="va">self</span>, images):</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract attention maps"""</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_maps.clear()</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.attention_maps.copy()</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> visualize_attention(<span class="va">self</span>, image, attention_map, alpha<span class="op">=</span><span class="fl">0.6</span>):</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Visualize attention map overlaid on image"""</span></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert image to RGB if needed</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image.shape[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">3</span>:</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use first 3 channels as RGB</span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>            rgb_image <span class="op">=</span> image[:<span class="dv">3</span>]</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>            rgb_image <span class="op">=</span> image</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize image for display</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>        rgb_image <span class="op">=</span> (rgb_image <span class="op">-</span> rgb_image.<span class="bu">min</span>()) <span class="op">/</span> (rgb_image.<span class="bu">max</span>() <span class="op">-</span> rgb_image.<span class="bu">min</span>())</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>        rgb_image <span class="op">=</span> rgb_image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).numpy()</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process attention map</span></span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> attention_map.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>            attention_map <span class="op">=</span> attention_map.mean(dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># Average over heads/channels</span></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize attention map to match image size</span></span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>        attention_resized <span class="op">=</span> F.interpolate(</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>            attention_map.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>rgb_image.shape[:<span class="dv">2</span>],</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'bilinear'</span>,</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>            align_corners<span class="op">=</span><span class="va">False</span></span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>        ).squeeze().numpy()</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create visualization</span></span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Original image</span></span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].imshow(rgb_image)</span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Original Image'</span>)</span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention map</span></span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].imshow(attention_resized, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Attention Map'</span>)</span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Overlay</span></span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(rgb_image)</span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(attention_resized, alpha<span class="op">=</span>alpha, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'Attention Overlay'</span>)</span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-89"><a href="#cb11-89" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remove_hooks(<span class="va">self</span>):</span>
<span id="cb11-90"><a href="#cb11-90" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Remove attention hooks"""</span></span>
<span id="cb11-91"><a href="#cb11-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb11-92"><a href="#cb11-92" aria-hidden="true" tabindex="-1"></a>            hook.remove()</span>
<span id="cb11-93"><a href="#cb11-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks.clear()</span>
<span id="cb11-94"><a href="#cb11-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-95"><a href="#cb11-95" aria-hidden="true" tabindex="-1"></a><span class="co"># Create attention extractor (mock example)</span></span>
<span id="cb11-96"><a href="#cb11-96" aria-hidden="true" tabindex="-1"></a>attention_extractor <span class="op">=</span> AttentionExtractor(model)</span>
<span id="cb11-97"><a href="#cb11-97" aria-hidden="true" tabindex="-1"></a><span class="co"># attention_extractor.register_attention_hooks()  # Would need actual attention layers</span></span>
<span id="cb11-98"><a href="#cb11-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-99"><a href="#cb11-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention extractor ready (requires model with attention layers)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Attention extractor ready (requires model with attention layers)</code></pre>
</div>
</div>
</section>
<section id="gradient-based-explanations" class="level3">
<h3 class="anchored" data-anchor-id="gradient-based-explanations">Gradient-based explanations</h3>
<div id="2c540df5" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GradCAM:</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Gradient-weighted Class Activation Mapping"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, target_layer):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_layer <span class="op">=</span> target_layer</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gradients <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activations <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register hooks</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_hooks()</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_hooks(<span class="va">self</span>):</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register hooks for gradients and activations"""</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> backward_hook(module, grad_input, grad_output):</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.gradients <span class="op">=</span> grad_output[<span class="dv">0</span>].detach()</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> forward_hook(module, <span class="bu">input</span>, output):</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.activations <span class="op">=</span> output.detach()</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find target layer</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        target_module <span class="op">=</span> <span class="bu">dict</span>(<span class="va">self</span>.model.named_modules())[<span class="va">self</span>.target_layer]</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        target_module.register_forward_hook(forward_hook)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        target_module.register_backward_hook(backward_hook)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate_cam(<span class="va">self</span>, images, class_idx<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Generate Class Activation Map"""</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Enable gradients</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        images.requires_grad_(<span class="va">True</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If class_idx not specified, use predicted class</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> class_idx <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>            class_idx <span class="op">=</span> outputs.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass for target class</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.zero_grad()</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        class_loss <span class="op">=</span> outputs[<span class="dv">0</span>, class_idx[<span class="dv">0</span>]] <span class="cf">if</span> <span class="bu">isinstance</span>(class_idx, torch.Tensor) <span class="cf">else</span> outputs[<span class="dv">0</span>, class_idx]</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        class_loss.backward()</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute CAM</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> <span class="va">self</span>.gradients[<span class="dv">0</span>]  <span class="co"># First image in batch</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>        activations <span class="op">=</span> <span class="va">self</span>.activations[<span class="dv">0</span>]  <span class="co"># First image in batch</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global average pooling of gradients</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> torch.mean(gradients, dim<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Weighted combination of activation maps</span></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> torch.zeros(activations.shape[<span class="dv">1</span>], activations.shape[<span class="dv">2</span>])</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, w <span class="kw">in</span> <span class="bu">enumerate</span>(weights):</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>            cam <span class="op">+=</span> w <span class="op">*</span> activations[i]</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply ReLU and normalize</span></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> F.relu(cam)</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> cam <span class="op">/</span> torch.<span class="bu">max</span>(cam) <span class="cf">if</span> torch.<span class="bu">max</span>(cam) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> cam</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> cam</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> visualize_cam(<span class="va">self</span>, image, cam, alpha<span class="op">=</span><span class="fl">0.4</span>):</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Visualize CAM overlaid on original image"""</span></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert image for display</span></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image.shape[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">3</span>:</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>            display_image <span class="op">=</span> image[:<span class="dv">3</span>]  <span class="co"># Use first 3 channels</span></span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>            display_image <span class="op">=</span> image</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>        display_image <span class="op">=</span> (display_image <span class="op">-</span> display_image.<span class="bu">min</span>()) <span class="op">/</span> (display_image.<span class="bu">max</span>() <span class="op">-</span> display_image.<span class="bu">min</span>())</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>        display_image <span class="op">=</span> display_image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).detach().numpy()</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize CAM to match image size</span></span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>        cam_resized <span class="op">=</span> F.interpolate(</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>            cam.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>display_image.shape[:<span class="dv">2</span>],</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'bilinear'</span>,</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>            align_corners<span class="op">=</span><span class="va">False</span></span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        ).squeeze().numpy()</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create visualization</span></span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].imshow(display_image)</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Original Image'</span>)</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].imshow(cam_resized, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Grad-CAM'</span>)</span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(display_image)</span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(cam_resized, alpha<span class="op">=</span>alpha, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'Grad-CAM Overlay'</span>)</span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a><span class="co"># Create GradCAM for conv3 layer</span></span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>gradcam <span class="op">=</span> GradCAM(model, target_layer<span class="op">=</span><span class="st">'conv3'</span>)</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate CAM</span></span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a>cam <span class="op">=</span> gradcam.generate_cam(sample_input)</span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generated CAM shape: </span><span class="sc">{</span>cam<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CAM range: [</span><span class="sc">{</span>cam<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>cam<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.3f}</span><span class="ss">]"</span>)</span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize</span></span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>gradcam.visualize_cam(sample_input[<span class="dv">0</span>], cam)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Generated CAM shape: torch.Size([28, 28])
CAM range: [0.000, 1.000]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/torch/nn/modules/module.py:1842: FutureWarning:

Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="model_inference_files/figure-html/cell-8-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="feature-analysis-and-dimensionality-reduction" class="level2">
<h2 class="anchored" data-anchor-id="feature-analysis-and-dimensionality-reduction">Feature Analysis and Dimensionality Reduction</h2>
<section id="pca-analysis-of-features" class="level3">
<h3 class="anchored" data-anchor-id="pca-analysis-of-features">PCA analysis of features</h3>
<div id="3a42e195" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_features_pca(features, n_components<span class="op">=</span><span class="dv">50</span>, visualize<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Analyze features using PCA"""</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten features if needed</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        original_shape <span class="op">=</span> features.shape</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features.view(features.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        original_shape <span class="op">=</span> features.shape</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to numpy</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    features_np <span class="op">=</span> features_flat.numpy()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply PCA</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    pca <span class="op">=</span> PCA(n_components<span class="op">=</span>n_components)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    features_pca <span class="op">=</span> pca.fit_transform(features_np)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Analyze explained variance</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    explained_var_ratio <span class="op">=</span> pca.explained_variance_ratio_</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    cumulative_var <span class="op">=</span> np.cumsum(explained_var_ratio)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> visualize:</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">5</span>))</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Explained variance</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].bar(<span class="bu">range</span>(<span class="bu">len</span>(explained_var_ratio)), explained_var_ratio)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Explained Variance by Component'</span>)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Principal Component'</span>)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Explained Variance Ratio'</span>)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cumulative explained variance</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].plot(cumulative_var, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Cumulative Explained Variance'</span>)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Number of Components'</span>)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Cumulative Variance Ratio'</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First two components</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].scatter(features_pca[:, <span class="dv">0</span>], features_pca[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'First Two Principal Components'</span>)</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_xlabel(<span class="st">'PC1'</span>)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_ylabel(<span class="st">'PC2'</span>)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">'features_pca'</span>: features_pca,</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">'explained_variance_ratio'</span>: explained_var_ratio,</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cumulative_variance'</span>: cumulative_var,</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>        <span class="st">'pca_model'</span>: pca</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample features for analysis</span></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>sample_features <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">256</span>)  <span class="co"># 100 samples, 256 features</span></span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>pca_results <span class="op">=</span> analyze_features_pca(sample_features, n_components<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PCA features shape: </span><span class="sc">{</span>pca_results[<span class="st">'features_pca'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 5 components explain </span><span class="sc">{</span>pca_results[<span class="st">'cumulative_variance'</span>][<span class="dv">4</span>]<span class="sc">:.1%}</span><span class="ss"> of variance"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="model_inference_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>PCA features shape: (100, 20)
First 5 components explain 11.8% of variance</code></pre>
</div>
</div>
</section>
<section id="t-sne-visualization" class="level3">
<h3 class="anchored" data-anchor-id="t-sne-visualization">t-SNE visualization</h3>
<div id="1d851810" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_features_tsne(features, labels<span class="op">=</span><span class="va">None</span>, perplexity<span class="op">=</span><span class="dv">30</span>, random_state<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Visualize features using t-SNE"""</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten features if needed</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features.view(features.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    features_np <span class="op">=</span> features_flat.numpy()</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply t-SNE</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Computing t-SNE embedding..."</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, perplexity<span class="op">=</span>perplexity, random_state<span class="op">=</span>random_state)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    features_tsne <span class="op">=</span> tsne.fit_transform(features_np)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create visualization</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Color by labels</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        unique_labels <span class="op">=</span> np.unique(labels)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> plt.cm.tab10(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="bu">len</span>(unique_labels)))</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(unique_labels):</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> labels <span class="op">==</span> label</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>            plt.scatter(features_tsne[mask, <span class="dv">0</span>], features_tsne[mask, <span class="dv">1</span>], </span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>                       c<span class="op">=</span>[colors[i]], label<span class="op">=</span><span class="ss">f'Class </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        plt.legend()</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        plt.scatter(features_tsne[:, <span class="dv">0</span>], features_tsne[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'t-SNE Visualization of Features'</span>)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'t-SNE 1'</span>)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'t-SNE 2'</span>)</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> features_tsne</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample data with labels</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>sample_features <span class="op">=</span> torch.randn(<span class="dv">200</span>, <span class="dv">256</span>)</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>sample_labels <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">200</span>)  <span class="co"># 5 classes</span></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with t-SNE</span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>tsne_features <span class="op">=</span> visualize_features_tsne(sample_features, sample_labels)</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"t-SNE features shape: </span><span class="sc">{</span>tsne_features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Computing t-SNE embedding...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="model_inference_files/figure-html/cell-10-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>t-SNE features shape: (200, 2)</code></pre>
</div>
</div>
</section>
</section>
<section id="inference-optimization" class="level2">
<h2 class="anchored" data-anchor-id="inference-optimization">Inference Optimization</h2>
<section id="model-quantization-for-faster-inference" class="level3">
<h3 class="anchored" data-anchor-id="model-quantization-for-faster-inference">Model quantization for faster inference</h3>
<div id="e0e3474c" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantize_model(model, calibration_data<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Quantize model for faster inference"""</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dynamic quantization (post-training)</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    quantized_model <span class="op">=</span> torch.quantization.quantize_dynamic(</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        {nn.Linear, nn.Conv2d},  <span class="co"># Layers to quantize</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        dtype<span class="op">=</span>torch.qint8</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Applied dynamic quantization"</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_model</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_inference_speed(original_model, quantized_model, test_input, num_runs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare inference speed between models"""</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> time</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warm up</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> original_model(test_input)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> quantized_model(test_input)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Time original model</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> original_model(test_input)</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>    original_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Time quantized model</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> quantized_model(test_input)</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    quantized_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>    speedup <span class="op">=</span> original_time <span class="op">/</span> quantized_time</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Original model: </span><span class="sc">{</span>original_time<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Quantized model: </span><span class="sc">{</span>quantized_time<span class="sc">:.3f}</span><span class="ss">s"</span>) </span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Speedup: </span><span class="sc">{</span>speedup<span class="sc">:.2f}</span><span class="ss">x"</span>)</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> speedup</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Create quantized version</span></span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()  <span class="co"># Important: set to eval mode</span></span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>quantized_model <span class="op">=</span> quantize_model(model)</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare speeds</span></span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>test_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>speedup <span class="op">=</span> compare_inference_speed(model, quantized_model, test_input, num_runs<span class="op">=</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Applied dynamic quantization
Original model: 0.305s
Quantized model: 0.301s
Speedup: 1.01x</code></pre>
</div>
</div>
</section>
<section id="batch-size-optimization" class="level3">
<h3 class="anchored" data-anchor-id="batch-size-optimization">Batch size optimization</h3>
<div id="040dfac4" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_optimal_batch_size(model, input_shape, device<span class="op">=</span><span class="st">'cpu'</span>, max_batch_size<span class="op">=</span><span class="dv">128</span>):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Find optimal batch size for memory and speed"""</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    batch_sizes <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>]</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> max_batch_size <span class="op">&gt;</span> <span class="dv">64</span>:</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        batch_sizes.extend([<span class="dv">128</span>, <span class="dv">256</span>])</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    batch_sizes <span class="op">=</span> [bs <span class="cf">for</span> bs <span class="kw">in</span> batch_sizes <span class="cf">if</span> bs <span class="op">&lt;=</span> max_batch_size]</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> {}</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_size <span class="kw">in</span> batch_sizes:</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create test batch</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>            test_batch <span class="op">=</span> torch.randn(batch_size, <span class="op">*</span>input_shape[<span class="dv">1</span>:]).to(device)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Measure memory and time</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device <span class="op">!=</span> <span class="st">'cpu'</span> <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>                torch.cuda.reset_peak_memory_stats()</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>                start_memory <span class="op">=</span> torch.cuda.memory_allocated()</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>            <span class="im">import</span> time</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>            start_time <span class="op">=</span> time.time()</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):  <span class="co"># Average over multiple runs</span></span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>                    outputs <span class="op">=</span> model(test_batch)</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>            elapsed_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>            throughput <span class="op">=</span> (batch_size <span class="op">*</span> <span class="dv">10</span>) <span class="op">/</span> elapsed_time  <span class="co"># samples per second</span></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device <span class="op">!=</span> <span class="st">'cpu'</span> <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>                peak_memory <span class="op">=</span> torch.cuda.max_memory_allocated()</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>                memory_per_sample <span class="op">=</span> (peak_memory <span class="op">-</span> start_memory) <span class="op">/</span> batch_size</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>                memory_per_sample <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>            results[batch_size] <span class="op">=</span> {</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>                <span class="st">'throughput'</span>: throughput,</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>                <span class="st">'time_per_sample'</span>: elapsed_time <span class="op">/</span> (batch_size <span class="op">*</span> <span class="dv">10</span>),</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>                <span class="st">'memory_per_sample'</span>: memory_per_sample <span class="op">/</span> (<span class="dv">1024</span><span class="op">**</span><span class="dv">2</span>)  <span class="co"># MB</span></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Batch size </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>throughput<span class="sc">:.1f}</span><span class="ss"> samples/sec, "</span></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f"</span><span class="sc">{</span>memory_per_sample <span class="op">/</span> (<span class="dv">1024</span><span class="op">**</span><span class="dv">2</span>)<span class="sc">:.1f}</span><span class="ss"> MB/sample"</span>)</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">"out of memory"</span> <span class="kw">in</span> <span class="bu">str</span>(e):</span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Batch size </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">: Out of memory"</span>)</span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> e</span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find optimal batch size (highest throughput)</span></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> results:</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a>        optimal_batch_size <span class="op">=</span> <span class="bu">max</span>(results.keys(), key<span class="op">=</span><span class="kw">lambda</span> k: results[k][<span class="st">'throughput'</span>])</span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Optimal batch size: </span><span class="sc">{</span>optimal_batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> optimal_batch_size, results</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span>, results</span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Find optimal batch size</span></span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a>optimal_bs, batch_results <span class="op">=</span> find_optimal_batch_size(</span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a>    input_shape<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span><span class="st">'cpu'</span>,</span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a>    max_batch_size<span class="op">=</span><span class="dv">64</span></span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Batch size 1: 167.5 samples/sec, 0.0 MB/sample
Batch size 2: 152.9 samples/sec, 0.0 MB/sample
Batch size 4: 203.1 samples/sec, 0.0 MB/sample
Batch size 8: 200.3 samples/sec, 0.0 MB/sample
Batch size 16: 222.5 samples/sec, 0.0 MB/sample
Batch size 32: 252.3 samples/sec, 0.0 MB/sample
Batch size 64: 273.1 samples/sec, 0.0 MB/sample

Optimal batch size: 64</code></pre>
</div>
</div>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Key inference and feature extraction techniques: - <strong>Basic inference</strong>: Single image and batch processing - <strong>Feature extraction</strong>: Layer-wise and multi-scale features<br>
- <strong>Attention visualization</strong>: Understanding model focus - <strong>Gradient explanations</strong>: Grad-CAM for interpretability - <strong>Dimensionality reduction</strong>: PCA and t-SNE analysis - <strong>Optimization</strong>: Quantization and batch size tuning - <strong>Performance monitoring</strong>: Speed and memory profiling</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kcaylor\.github\.io\/GEOG-288KC-geospatial-foundation-models");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb25" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Model Inference &amp; Feature Extraction"</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Running inference and extracting features"</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> geoai</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction to Model Inference</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>Model inference and feature extraction are crucial for applying trained models to satellite imagery analysis and extracting meaningful representations.</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CUDA available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## Basic Inference Patterns</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="fu">### Single image inference</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GeospatialClassifier(nn.Module):</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Example geospatial classifier for demonstration"""</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_channels<span class="op">=</span><span class="dv">6</span>, num_classes<span class="op">=</span><span class="dv">10</span>, embed_dim<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature extraction layers</span></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(num_channels, <span class="dv">64</span>, <span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global pooling and classification</span></span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.global_pool <span class="op">=</span> nn.AdaptiveAvgPool2d(<span class="dv">1</span>)</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(<span class="dv">256</span>, num_classes)</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature embedding layer</span></span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_embed <span class="op">=</span> nn.Linear(<span class="dv">256</span>, embed_dim)</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, return_features<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature extraction</span></span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> F.relu(<span class="va">self</span>.conv3(x))</span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global pooling</span></span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a>        pooled <span class="op">=</span> <span class="va">self</span>.global_pool(features).flatten(<span class="dv">1</span>)</span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classification</span></span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.classifier(pooled)</span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_features:</span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>            embeddings <span class="op">=</span> <span class="va">self</span>.feature_embed(pooled)</span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {</span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>                <span class="st">'logits'</span>: logits,</span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>                <span class="st">'features'</span>: embeddings,</span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a>                <span class="st">'spatial_features'</span>: features,</span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a>                <span class="st">'pooled_features'</span>: pooled</span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model</span></span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GeospatialClassifier(num_channels<span class="op">=</span><span class="dv">6</span>, num_classes<span class="op">=</span><span class="dv">10</span>, embed_dim<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-81"><a href="#cb25-81" aria-hidden="true" tabindex="-1"></a><span class="co"># Single image inference</span></span>
<span id="cb25-82"><a href="#cb25-82" aria-hidden="true" tabindex="-1"></a>sample_image <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)  <span class="co"># Batch of 1, 6 channels</span></span>
<span id="cb25-83"><a href="#cb25-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-84"><a href="#cb25-84" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-85"><a href="#cb25-85" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Basic inference</span></span>
<span id="cb25-86"><a href="#cb25-86" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(sample_image)</span>
<span id="cb25-87"><a href="#cb25-87" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-88"><a href="#cb25-88" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inference with features</span></span>
<span id="cb25-89"><a href="#cb25-89" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(sample_image, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-90"><a href="#cb25-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-91"><a href="#cb25-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>sample_image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-92"><a href="#cb25-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predictions shape: </span><span class="sc">{</span>predictions<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-93"><a href="#cb25-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Logits shape: </span><span class="sc">{</span>outputs[<span class="st">'logits'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-94"><a href="#cb25-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Features shape: </span><span class="sc">{</span>outputs[<span class="st">'features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-95"><a href="#cb25-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Spatial features shape: </span><span class="sc">{</span>outputs[<span class="st">'spatial_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-96"><a href="#cb25-96" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-97"><a href="#cb25-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-98"><a href="#cb25-98" aria-hidden="true" tabindex="-1"></a><span class="fu">### Batch inference</span></span>
<span id="cb25-101"><a href="#cb25-101" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-102"><a href="#cb25-102" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_inference(model, images, batch_size<span class="op">=</span><span class="dv">32</span>, device<span class="op">=</span><span class="st">'cpu'</span>):</span>
<span id="cb25-103"><a href="#cb25-103" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Perform batch inference on multiple images"""</span></span>
<span id="cb25-104"><a href="#cb25-104" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-105"><a href="#cb25-105" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb25-106"><a href="#cb25-106" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb25-107"><a href="#cb25-107" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-108"><a href="#cb25-108" aria-hidden="true" tabindex="-1"></a>    all_predictions <span class="op">=</span> []</span>
<span id="cb25-109"><a href="#cb25-109" aria-hidden="true" tabindex="-1"></a>    all_features <span class="op">=</span> []</span>
<span id="cb25-110"><a href="#cb25-110" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-111"><a href="#cb25-111" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process in batches</span></span>
<span id="cb25-112"><a href="#cb25-112" aria-hidden="true" tabindex="-1"></a>    n_images <span class="op">=</span> <span class="bu">len</span>(images)</span>
<span id="cb25-113"><a href="#cb25-113" aria-hidden="true" tabindex="-1"></a>    n_batches <span class="op">=</span> (n_images <span class="op">+</span> batch_size <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size</span>
<span id="cb25-114"><a href="#cb25-114" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-115"><a href="#cb25-115" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-116"><a href="#cb25-116" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_batches):</span>
<span id="cb25-117"><a href="#cb25-117" aria-hidden="true" tabindex="-1"></a>            start_idx <span class="op">=</span> i <span class="op">*</span> batch_size</span>
<span id="cb25-118"><a href="#cb25-118" aria-hidden="true" tabindex="-1"></a>            end_idx <span class="op">=</span> <span class="bu">min</span>(start_idx <span class="op">+</span> batch_size, n_images)</span>
<span id="cb25-119"><a href="#cb25-119" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-120"><a href="#cb25-120" aria-hidden="true" tabindex="-1"></a>            batch <span class="op">=</span> images[start_idx:end_idx].to(device)</span>
<span id="cb25-121"><a href="#cb25-121" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-122"><a href="#cb25-122" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get predictions and features</span></span>
<span id="cb25-123"><a href="#cb25-123" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(batch, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-124"><a href="#cb25-124" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-125"><a href="#cb25-125" aria-hidden="true" tabindex="-1"></a>            all_predictions.append(outputs[<span class="st">'logits'</span>].cpu())</span>
<span id="cb25-126"><a href="#cb25-126" aria-hidden="true" tabindex="-1"></a>            all_features.append(outputs[<span class="st">'features'</span>].cpu())</span>
<span id="cb25-127"><a href="#cb25-127" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-128"><a href="#cb25-128" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Processed batch </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_batches<span class="sc">}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">'</span><span class="ch">\r</span><span class="st">'</span>)</span>
<span id="cb25-129"><a href="#cb25-129" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-130"><a href="#cb25-130" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Concatenate results</span></span>
<span id="cb25-131"><a href="#cb25-131" aria-hidden="true" tabindex="-1"></a>    final_predictions <span class="op">=</span> torch.cat(all_predictions, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-132"><a href="#cb25-132" aria-hidden="true" tabindex="-1"></a>    final_features <span class="op">=</span> torch.cat(all_features, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-133"><a href="#cb25-133" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-134"><a href="#cb25-134" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Completed inference on </span><span class="sc">{</span>n_images<span class="sc">}</span><span class="ss"> images"</span>)</span>
<span id="cb25-135"><a href="#cb25-135" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-136"><a href="#cb25-136" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_predictions, final_features</span>
<span id="cb25-137"><a href="#cb25-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-138"><a href="#cb25-138" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sample batch</span></span>
<span id="cb25-139"><a href="#cb25-139" aria-hidden="true" tabindex="-1"></a>batch_images <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb25-140"><a href="#cb25-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-141"><a href="#cb25-141" aria-hidden="true" tabindex="-1"></a><span class="co"># Run batch inference</span></span>
<span id="cb25-142"><a href="#cb25-142" aria-hidden="true" tabindex="-1"></a>predictions, features <span class="op">=</span> batch_inference(model, batch_images, batch_size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb25-143"><a href="#cb25-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-144"><a href="#cb25-144" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch predictions shape: </span><span class="sc">{</span>predictions<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-145"><a href="#cb25-145" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch features shape: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-146"><a href="#cb25-146" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-147"><a href="#cb25-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-148"><a href="#cb25-148" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Extraction Techniques</span></span>
<span id="cb25-149"><a href="#cb25-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-150"><a href="#cb25-150" aria-hidden="true" tabindex="-1"></a><span class="fu">### Layer-wise feature extraction</span></span>
<span id="cb25-153"><a href="#cb25-153" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-154"><a href="#cb25-154" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeatureExtractor:</span>
<span id="cb25-155"><a href="#cb25-155" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract features from specific layers of a model"""</span></span>
<span id="cb25-156"><a href="#cb25-156" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-157"><a href="#cb25-157" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, layer_names<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-158"><a href="#cb25-158" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb25-159"><a href="#cb25-159" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb25-160"><a href="#cb25-160" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> {}</span>
<span id="cb25-161"><a href="#cb25-161" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb25-162"><a href="#cb25-162" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-163"><a href="#cb25-163" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> layer_names <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb25-164"><a href="#cb25-164" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract from all named modules</span></span>
<span id="cb25-165"><a href="#cb25-165" aria-hidden="true" tabindex="-1"></a>            layer_names <span class="op">=</span> [name <span class="cf">for</span> name, _ <span class="kw">in</span> model.named_modules() <span class="cf">if</span> name]</span>
<span id="cb25-166"><a href="#cb25-166" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-167"><a href="#cb25-167" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_hooks(layer_names)</span>
<span id="cb25-168"><a href="#cb25-168" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-169"><a href="#cb25-169" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_hooks(<span class="va">self</span>, layer_names):</span>
<span id="cb25-170"><a href="#cb25-170" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register forward hooks for feature extraction"""</span></span>
<span id="cb25-171"><a href="#cb25-171" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-172"><a href="#cb25-172" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> make_hook(name):</span>
<span id="cb25-173"><a href="#cb25-173" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> hook(module, <span class="bu">input</span>, output):</span>
<span id="cb25-174"><a href="#cb25-174" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Store detached copy to avoid gradient tracking</span></span>
<span id="cb25-175"><a href="#cb25-175" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">isinstance</span>(output, torch.Tensor):</span>
<span id="cb25-176"><a href="#cb25-176" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> output.detach().cpu()</span>
<span id="cb25-177"><a href="#cb25-177" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, (<span class="bu">list</span>, <span class="bu">tuple</span>)):</span>
<span id="cb25-178"><a href="#cb25-178" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> [o.detach().cpu() <span class="cf">if</span> <span class="bu">isinstance</span>(o, torch.Tensor) <span class="cf">else</span> o <span class="cf">for</span> o <span class="kw">in</span> output]</span>
<span id="cb25-179"><a href="#cb25-179" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, <span class="bu">dict</span>):</span>
<span id="cb25-180"><a href="#cb25-180" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> {k: v.detach().cpu() <span class="cf">if</span> <span class="bu">isinstance</span>(v, torch.Tensor) <span class="cf">else</span> v </span>
<span id="cb25-181"><a href="#cb25-181" aria-hidden="true" tabindex="-1"></a>                                         <span class="cf">for</span> k, v <span class="kw">in</span> output.items()}</span>
<span id="cb25-182"><a href="#cb25-182" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> hook</span>
<span id="cb25-183"><a href="#cb25-183" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-184"><a href="#cb25-184" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register hooks</span></span>
<span id="cb25-185"><a href="#cb25-185" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.model.named_modules():</span>
<span id="cb25-186"><a href="#cb25-186" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> name <span class="kw">in</span> layer_names:</span>
<span id="cb25-187"><a href="#cb25-187" aria-hidden="true" tabindex="-1"></a>                handle <span class="op">=</span> module.register_forward_hook(make_hook(name))</span>
<span id="cb25-188"><a href="#cb25-188" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(handle)</span>
<span id="cb25-189"><a href="#cb25-189" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Registered hook for layer: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-190"><a href="#cb25-190" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-191"><a href="#cb25-191" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract(<span class="va">self</span>, images):</span>
<span id="cb25-192"><a href="#cb25-192" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract features from registered layers"""</span></span>
<span id="cb25-193"><a href="#cb25-193" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-194"><a href="#cb25-194" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features.clear()</span>
<span id="cb25-195"><a href="#cb25-195" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-196"><a href="#cb25-196" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-197"><a href="#cb25-197" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass triggers hooks</span></span>
<span id="cb25-198"><a href="#cb25-198" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb25-199"><a href="#cb25-199" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-200"><a href="#cb25-200" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.features.copy()</span>
<span id="cb25-201"><a href="#cb25-201" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-202"><a href="#cb25-202" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remove_hooks(<span class="va">self</span>):</span>
<span id="cb25-203"><a href="#cb25-203" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Remove all registered hooks"""</span></span>
<span id="cb25-204"><a href="#cb25-204" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb25-205"><a href="#cb25-205" aria-hidden="true" tabindex="-1"></a>            hook.remove()</span>
<span id="cb25-206"><a href="#cb25-206" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks.clear()</span>
<span id="cb25-207"><a href="#cb25-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-208"><a href="#cb25-208" aria-hidden="true" tabindex="-1"></a><span class="co"># Create feature extractor</span></span>
<span id="cb25-209"><a href="#cb25-209" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> FeatureExtractor(</span>
<span id="cb25-210"><a href="#cb25-210" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb25-211"><a href="#cb25-211" aria-hidden="true" tabindex="-1"></a>    layer_names<span class="op">=</span>[<span class="st">'conv1'</span>, <span class="st">'conv2'</span>, <span class="st">'conv3'</span>, <span class="st">'global_pool'</span>]</span>
<span id="cb25-212"><a href="#cb25-212" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-213"><a href="#cb25-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-214"><a href="#cb25-214" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract features</span></span>
<span id="cb25-215"><a href="#cb25-215" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb25-216"><a href="#cb25-216" aria-hidden="true" tabindex="-1"></a>extracted_features <span class="op">=</span> extractor.extract(sample_input)</span>
<span id="cb25-217"><a href="#cb25-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-218"><a href="#cb25-218" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Extracted features:"</span>)</span>
<span id="cb25-219"><a href="#cb25-219" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer_name, features <span class="kw">in</span> extracted_features.items():</span>
<span id="cb25-220"><a href="#cb25-220" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(features, torch.Tensor):</span>
<span id="cb25-221"><a href="#cb25-221" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-222"><a href="#cb25-222" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb25-223"><a href="#cb25-223" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">type</span>(features)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-224"><a href="#cb25-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-225"><a href="#cb25-225" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean up</span></span>
<span id="cb25-226"><a href="#cb25-226" aria-hidden="true" tabindex="-1"></a>extractor.remove_hooks()</span>
<span id="cb25-227"><a href="#cb25-227" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-228"><a href="#cb25-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-229"><a href="#cb25-229" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-scale feature extraction</span></span>
<span id="cb25-232"><a href="#cb25-232" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-233"><a href="#cb25-233" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiScaleFeatureExtractor(nn.Module):</span>
<span id="cb25-234"><a href="#cb25-234" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract features at multiple scales"""</span></span>
<span id="cb25-235"><a href="#cb25-235" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-236"><a href="#cb25-236" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, backbone):</span>
<span id="cb25-237"><a href="#cb25-237" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-238"><a href="#cb25-238" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> backbone</span>
<span id="cb25-239"><a href="#cb25-239" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-240"><a href="#cb25-240" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature pyramid levels</span></span>
<span id="cb25-241"><a href="#cb25-241" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scales <span class="op">=</span> [<span class="fl">1.0</span>, <span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>]</span>
<span id="cb25-242"><a href="#cb25-242" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-243"><a href="#cb25-243" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb25-244"><a href="#cb25-244" aria-hidden="true" tabindex="-1"></a>        batch_size, channels, height, width <span class="op">=</span> x.shape</span>
<span id="cb25-245"><a href="#cb25-245" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-246"><a href="#cb25-246" aria-hidden="true" tabindex="-1"></a>        multiscale_features <span class="op">=</span> {}</span>
<span id="cb25-247"><a href="#cb25-247" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-248"><a href="#cb25-248" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> scale <span class="kw">in</span> <span class="va">self</span>.scales:</span>
<span id="cb25-249"><a href="#cb25-249" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resize input</span></span>
<span id="cb25-250"><a href="#cb25-250" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> scale <span class="op">!=</span> <span class="fl">1.0</span>:</span>
<span id="cb25-251"><a href="#cb25-251" aria-hidden="true" tabindex="-1"></a>                new_size <span class="op">=</span> (<span class="bu">int</span>(height <span class="op">*</span> scale), <span class="bu">int</span>(width <span class="op">*</span> scale))</span>
<span id="cb25-252"><a href="#cb25-252" aria-hidden="true" tabindex="-1"></a>                scaled_input <span class="op">=</span> F.interpolate(x, size<span class="op">=</span>new_size, mode<span class="op">=</span><span class="st">'bilinear'</span>, align_corners<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb25-253"><a href="#cb25-253" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb25-254"><a href="#cb25-254" aria-hidden="true" tabindex="-1"></a>                scaled_input <span class="op">=</span> x</span>
<span id="cb25-255"><a href="#cb25-255" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-256"><a href="#cb25-256" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract features</span></span>
<span id="cb25-257"><a href="#cb25-257" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-258"><a href="#cb25-258" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> <span class="va">self</span>.backbone(scaled_input, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-259"><a href="#cb25-259" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb25-260"><a href="#cb25-260" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store features with scale info</span></span>
<span id="cb25-261"><a href="#cb25-261" aria-hidden="true" tabindex="-1"></a>            scale_key <span class="op">=</span> <span class="ss">f"scale_</span><span class="sc">{</span>scale<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb25-262"><a href="#cb25-262" aria-hidden="true" tabindex="-1"></a>            multiscale_features[scale_key] <span class="op">=</span> {</span>
<span id="cb25-263"><a href="#cb25-263" aria-hidden="true" tabindex="-1"></a>                <span class="st">'features'</span>: outputs[<span class="st">'features'</span>],</span>
<span id="cb25-264"><a href="#cb25-264" aria-hidden="true" tabindex="-1"></a>                <span class="st">'spatial_features'</span>: outputs[<span class="st">'spatial_features'</span>],</span>
<span id="cb25-265"><a href="#cb25-265" aria-hidden="true" tabindex="-1"></a>                <span class="st">'input_size'</span>: scaled_input.shape[<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb25-266"><a href="#cb25-266" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb25-267"><a href="#cb25-267" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-268"><a href="#cb25-268" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> multiscale_features</span>
<span id="cb25-269"><a href="#cb25-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-270"><a href="#cb25-270" aria-hidden="true" tabindex="-1"></a><span class="co"># Create multi-scale extractor</span></span>
<span id="cb25-271"><a href="#cb25-271" aria-hidden="true" tabindex="-1"></a>multiscale_extractor <span class="op">=</span> MultiScaleFeatureExtractor(model)</span>
<span id="cb25-272"><a href="#cb25-272" aria-hidden="true" tabindex="-1"></a>multiscale_extractor.<span class="bu">eval</span>()</span>
<span id="cb25-273"><a href="#cb25-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-274"><a href="#cb25-274" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract multi-scale features</span></span>
<span id="cb25-275"><a href="#cb25-275" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb25-276"><a href="#cb25-276" aria-hidden="true" tabindex="-1"></a>multiscale_features <span class="op">=</span> multiscale_extractor(sample_input)</span>
<span id="cb25-277"><a href="#cb25-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-278"><a href="#cb25-278" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Multi-scale features:"</span>)</span>
<span id="cb25-279"><a href="#cb25-279" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> scale, features <span class="kw">in</span> multiscale_features.items():</span>
<span id="cb25-280"><a href="#cb25-280" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>scale<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb25-281"><a href="#cb25-281" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Features: </span><span class="sc">{</span>features[<span class="st">'features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-282"><a href="#cb25-282" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Spatial: </span><span class="sc">{</span>features[<span class="st">'spatial_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-283"><a href="#cb25-283" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Input size: </span><span class="sc">{</span>features[<span class="st">'input_size'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-284"><a href="#cb25-284" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-285"><a href="#cb25-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-286"><a href="#cb25-286" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced Inference Techniques</span></span>
<span id="cb25-287"><a href="#cb25-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-288"><a href="#cb25-288" aria-hidden="true" tabindex="-1"></a><span class="fu">### Attention map visualization</span></span>
<span id="cb25-291"><a href="#cb25-291" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-292"><a href="#cb25-292" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionExtractor:</span>
<span id="cb25-293"><a href="#cb25-293" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract and visualize attention maps"""</span></span>
<span id="cb25-294"><a href="#cb25-294" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-295"><a href="#cb25-295" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model):</span>
<span id="cb25-296"><a href="#cb25-296" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb25-297"><a href="#cb25-297" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_maps <span class="op">=</span> {}</span>
<span id="cb25-298"><a href="#cb25-298" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb25-299"><a href="#cb25-299" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-300"><a href="#cb25-300" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_attention_hooks(<span class="va">self</span>):</span>
<span id="cb25-301"><a href="#cb25-301" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register hooks for attention layers"""</span></span>
<span id="cb25-302"><a href="#cb25-302" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-303"><a href="#cb25-303" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> attention_hook(name):</span>
<span id="cb25-304"><a href="#cb25-304" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> hook(module, <span class="bu">input</span>, output):</span>
<span id="cb25-305"><a href="#cb25-305" aria-hidden="true" tabindex="-1"></a>                <span class="co"># For attention mechanisms, we typically want the attention weights</span></span>
<span id="cb25-306"><a href="#cb25-306" aria-hidden="true" tabindex="-1"></a>                <span class="co"># This is a simplified example - actual implementation depends on model architecture</span></span>
<span id="cb25-307"><a href="#cb25-307" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">hasattr</span>(module, <span class="st">'attention_weights'</span>):</span>
<span id="cb25-308"><a href="#cb25-308" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> module.attention_weights.detach().cpu()</span>
<span id="cb25-309"><a href="#cb25-309" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, <span class="bu">tuple</span>) <span class="kw">and</span> <span class="bu">len</span>(output) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb25-310"><a href="#cb25-310" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Assume second output contains attention weights</span></span>
<span id="cb25-311"><a href="#cb25-311" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> output[<span class="dv">1</span>].detach().cpu()</span>
<span id="cb25-312"><a href="#cb25-312" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">hasattr</span>(output, <span class="st">'attentions'</span>):</span>
<span id="cb25-313"><a href="#cb25-313" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> output.attentions.detach().cpu()</span>
<span id="cb25-314"><a href="#cb25-314" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> hook</span>
<span id="cb25-315"><a href="#cb25-315" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-316"><a href="#cb25-316" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Look for attention-related modules</span></span>
<span id="cb25-317"><a href="#cb25-317" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.model.named_modules():</span>
<span id="cb25-318"><a href="#cb25-318" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">'attention'</span> <span class="kw">in</span> name.lower() <span class="kw">or</span> <span class="st">'attn'</span> <span class="kw">in</span> name.lower():</span>
<span id="cb25-319"><a href="#cb25-319" aria-hidden="true" tabindex="-1"></a>                handle <span class="op">=</span> module.register_forward_hook(attention_hook(name))</span>
<span id="cb25-320"><a href="#cb25-320" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(handle)</span>
<span id="cb25-321"><a href="#cb25-321" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Registered attention hook: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-322"><a href="#cb25-322" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-323"><a href="#cb25-323" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract_attention(<span class="va">self</span>, images):</span>
<span id="cb25-324"><a href="#cb25-324" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract attention maps"""</span></span>
<span id="cb25-325"><a href="#cb25-325" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_maps.clear()</span>
<span id="cb25-326"><a href="#cb25-326" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-327"><a href="#cb25-327" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-328"><a href="#cb25-328" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb25-329"><a href="#cb25-329" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-330"><a href="#cb25-330" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.attention_maps.copy()</span>
<span id="cb25-331"><a href="#cb25-331" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-332"><a href="#cb25-332" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> visualize_attention(<span class="va">self</span>, image, attention_map, alpha<span class="op">=</span><span class="fl">0.6</span>):</span>
<span id="cb25-333"><a href="#cb25-333" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Visualize attention map overlaid on image"""</span></span>
<span id="cb25-334"><a href="#cb25-334" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-335"><a href="#cb25-335" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert image to RGB if needed</span></span>
<span id="cb25-336"><a href="#cb25-336" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image.shape[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">3</span>:</span>
<span id="cb25-337"><a href="#cb25-337" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use first 3 channels as RGB</span></span>
<span id="cb25-338"><a href="#cb25-338" aria-hidden="true" tabindex="-1"></a>            rgb_image <span class="op">=</span> image[:<span class="dv">3</span>]</span>
<span id="cb25-339"><a href="#cb25-339" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb25-340"><a href="#cb25-340" aria-hidden="true" tabindex="-1"></a>            rgb_image <span class="op">=</span> image</span>
<span id="cb25-341"><a href="#cb25-341" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-342"><a href="#cb25-342" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize image for display</span></span>
<span id="cb25-343"><a href="#cb25-343" aria-hidden="true" tabindex="-1"></a>        rgb_image <span class="op">=</span> (rgb_image <span class="op">-</span> rgb_image.<span class="bu">min</span>()) <span class="op">/</span> (rgb_image.<span class="bu">max</span>() <span class="op">-</span> rgb_image.<span class="bu">min</span>())</span>
<span id="cb25-344"><a href="#cb25-344" aria-hidden="true" tabindex="-1"></a>        rgb_image <span class="op">=</span> rgb_image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).numpy()</span>
<span id="cb25-345"><a href="#cb25-345" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-346"><a href="#cb25-346" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process attention map</span></span>
<span id="cb25-347"><a href="#cb25-347" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> attention_map.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb25-348"><a href="#cb25-348" aria-hidden="true" tabindex="-1"></a>            attention_map <span class="op">=</span> attention_map.mean(dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># Average over heads/channels</span></span>
<span id="cb25-349"><a href="#cb25-349" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-350"><a href="#cb25-350" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize attention map to match image size</span></span>
<span id="cb25-351"><a href="#cb25-351" aria-hidden="true" tabindex="-1"></a>        attention_resized <span class="op">=</span> F.interpolate(</span>
<span id="cb25-352"><a href="#cb25-352" aria-hidden="true" tabindex="-1"></a>            attention_map.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb25-353"><a href="#cb25-353" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>rgb_image.shape[:<span class="dv">2</span>],</span>
<span id="cb25-354"><a href="#cb25-354" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'bilinear'</span>,</span>
<span id="cb25-355"><a href="#cb25-355" aria-hidden="true" tabindex="-1"></a>            align_corners<span class="op">=</span><span class="va">False</span></span>
<span id="cb25-356"><a href="#cb25-356" aria-hidden="true" tabindex="-1"></a>        ).squeeze().numpy()</span>
<span id="cb25-357"><a href="#cb25-357" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-358"><a href="#cb25-358" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create visualization</span></span>
<span id="cb25-359"><a href="#cb25-359" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb25-360"><a href="#cb25-360" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-361"><a href="#cb25-361" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Original image</span></span>
<span id="cb25-362"><a href="#cb25-362" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].imshow(rgb_image)</span>
<span id="cb25-363"><a href="#cb25-363" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Original Image'</span>)</span>
<span id="cb25-364"><a href="#cb25-364" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb25-365"><a href="#cb25-365" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-366"><a href="#cb25-366" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention map</span></span>
<span id="cb25-367"><a href="#cb25-367" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].imshow(attention_resized, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb25-368"><a href="#cb25-368" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Attention Map'</span>)</span>
<span id="cb25-369"><a href="#cb25-369" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb25-370"><a href="#cb25-370" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-371"><a href="#cb25-371" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Overlay</span></span>
<span id="cb25-372"><a href="#cb25-372" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(rgb_image)</span>
<span id="cb25-373"><a href="#cb25-373" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(attention_resized, alpha<span class="op">=</span>alpha, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb25-374"><a href="#cb25-374" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'Attention Overlay'</span>)</span>
<span id="cb25-375"><a href="#cb25-375" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb25-376"><a href="#cb25-376" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-377"><a href="#cb25-377" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb25-378"><a href="#cb25-378" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb25-379"><a href="#cb25-379" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-380"><a href="#cb25-380" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remove_hooks(<span class="va">self</span>):</span>
<span id="cb25-381"><a href="#cb25-381" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Remove attention hooks"""</span></span>
<span id="cb25-382"><a href="#cb25-382" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb25-383"><a href="#cb25-383" aria-hidden="true" tabindex="-1"></a>            hook.remove()</span>
<span id="cb25-384"><a href="#cb25-384" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks.clear()</span>
<span id="cb25-385"><a href="#cb25-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-386"><a href="#cb25-386" aria-hidden="true" tabindex="-1"></a><span class="co"># Create attention extractor (mock example)</span></span>
<span id="cb25-387"><a href="#cb25-387" aria-hidden="true" tabindex="-1"></a>attention_extractor <span class="op">=</span> AttentionExtractor(model)</span>
<span id="cb25-388"><a href="#cb25-388" aria-hidden="true" tabindex="-1"></a><span class="co"># attention_extractor.register_attention_hooks()  # Would need actual attention layers</span></span>
<span id="cb25-389"><a href="#cb25-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-390"><a href="#cb25-390" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention extractor ready (requires model with attention layers)"</span>)</span>
<span id="cb25-391"><a href="#cb25-391" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-392"><a href="#cb25-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-393"><a href="#cb25-393" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient-based explanations</span></span>
<span id="cb25-396"><a href="#cb25-396" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-397"><a href="#cb25-397" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GradCAM:</span>
<span id="cb25-398"><a href="#cb25-398" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Gradient-weighted Class Activation Mapping"""</span></span>
<span id="cb25-399"><a href="#cb25-399" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-400"><a href="#cb25-400" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, target_layer):</span>
<span id="cb25-401"><a href="#cb25-401" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb25-402"><a href="#cb25-402" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_layer <span class="op">=</span> target_layer</span>
<span id="cb25-403"><a href="#cb25-403" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gradients <span class="op">=</span> <span class="va">None</span></span>
<span id="cb25-404"><a href="#cb25-404" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activations <span class="op">=</span> <span class="va">None</span></span>
<span id="cb25-405"><a href="#cb25-405" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-406"><a href="#cb25-406" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register hooks</span></span>
<span id="cb25-407"><a href="#cb25-407" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_hooks()</span>
<span id="cb25-408"><a href="#cb25-408" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-409"><a href="#cb25-409" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_hooks(<span class="va">self</span>):</span>
<span id="cb25-410"><a href="#cb25-410" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register hooks for gradients and activations"""</span></span>
<span id="cb25-411"><a href="#cb25-411" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-412"><a href="#cb25-412" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> backward_hook(module, grad_input, grad_output):</span>
<span id="cb25-413"><a href="#cb25-413" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.gradients <span class="op">=</span> grad_output[<span class="dv">0</span>].detach()</span>
<span id="cb25-414"><a href="#cb25-414" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-415"><a href="#cb25-415" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> forward_hook(module, <span class="bu">input</span>, output):</span>
<span id="cb25-416"><a href="#cb25-416" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.activations <span class="op">=</span> output.detach()</span>
<span id="cb25-417"><a href="#cb25-417" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-418"><a href="#cb25-418" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find target layer</span></span>
<span id="cb25-419"><a href="#cb25-419" aria-hidden="true" tabindex="-1"></a>        target_module <span class="op">=</span> <span class="bu">dict</span>(<span class="va">self</span>.model.named_modules())[<span class="va">self</span>.target_layer]</span>
<span id="cb25-420"><a href="#cb25-420" aria-hidden="true" tabindex="-1"></a>        target_module.register_forward_hook(forward_hook)</span>
<span id="cb25-421"><a href="#cb25-421" aria-hidden="true" tabindex="-1"></a>        target_module.register_backward_hook(backward_hook)</span>
<span id="cb25-422"><a href="#cb25-422" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-423"><a href="#cb25-423" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate_cam(<span class="va">self</span>, images, class_idx<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-424"><a href="#cb25-424" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Generate Class Activation Map"""</span></span>
<span id="cb25-425"><a href="#cb25-425" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-426"><a href="#cb25-426" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Enable gradients</span></span>
<span id="cb25-427"><a href="#cb25-427" aria-hidden="true" tabindex="-1"></a>        images.requires_grad_(<span class="va">True</span>)</span>
<span id="cb25-428"><a href="#cb25-428" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-429"><a href="#cb25-429" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb25-430"><a href="#cb25-430" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb25-431"><a href="#cb25-431" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-432"><a href="#cb25-432" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If class_idx not specified, use predicted class</span></span>
<span id="cb25-433"><a href="#cb25-433" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> class_idx <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb25-434"><a href="#cb25-434" aria-hidden="true" tabindex="-1"></a>            class_idx <span class="op">=</span> outputs.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-435"><a href="#cb25-435" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-436"><a href="#cb25-436" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass for target class</span></span>
<span id="cb25-437"><a href="#cb25-437" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.zero_grad()</span>
<span id="cb25-438"><a href="#cb25-438" aria-hidden="true" tabindex="-1"></a>        class_loss <span class="op">=</span> outputs[<span class="dv">0</span>, class_idx[<span class="dv">0</span>]] <span class="cf">if</span> <span class="bu">isinstance</span>(class_idx, torch.Tensor) <span class="cf">else</span> outputs[<span class="dv">0</span>, class_idx]</span>
<span id="cb25-439"><a href="#cb25-439" aria-hidden="true" tabindex="-1"></a>        class_loss.backward()</span>
<span id="cb25-440"><a href="#cb25-440" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-441"><a href="#cb25-441" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute CAM</span></span>
<span id="cb25-442"><a href="#cb25-442" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> <span class="va">self</span>.gradients[<span class="dv">0</span>]  <span class="co"># First image in batch</span></span>
<span id="cb25-443"><a href="#cb25-443" aria-hidden="true" tabindex="-1"></a>        activations <span class="op">=</span> <span class="va">self</span>.activations[<span class="dv">0</span>]  <span class="co"># First image in batch</span></span>
<span id="cb25-444"><a href="#cb25-444" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-445"><a href="#cb25-445" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global average pooling of gradients</span></span>
<span id="cb25-446"><a href="#cb25-446" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> torch.mean(gradients, dim<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb25-447"><a href="#cb25-447" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-448"><a href="#cb25-448" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Weighted combination of activation maps</span></span>
<span id="cb25-449"><a href="#cb25-449" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> torch.zeros(activations.shape[<span class="dv">1</span>], activations.shape[<span class="dv">2</span>])</span>
<span id="cb25-450"><a href="#cb25-450" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, w <span class="kw">in</span> <span class="bu">enumerate</span>(weights):</span>
<span id="cb25-451"><a href="#cb25-451" aria-hidden="true" tabindex="-1"></a>            cam <span class="op">+=</span> w <span class="op">*</span> activations[i]</span>
<span id="cb25-452"><a href="#cb25-452" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-453"><a href="#cb25-453" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply ReLU and normalize</span></span>
<span id="cb25-454"><a href="#cb25-454" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> F.relu(cam)</span>
<span id="cb25-455"><a href="#cb25-455" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> cam <span class="op">/</span> torch.<span class="bu">max</span>(cam) <span class="cf">if</span> torch.<span class="bu">max</span>(cam) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> cam</span>
<span id="cb25-456"><a href="#cb25-456" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-457"><a href="#cb25-457" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> cam</span>
<span id="cb25-458"><a href="#cb25-458" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-459"><a href="#cb25-459" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> visualize_cam(<span class="va">self</span>, image, cam, alpha<span class="op">=</span><span class="fl">0.4</span>):</span>
<span id="cb25-460"><a href="#cb25-460" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Visualize CAM overlaid on original image"""</span></span>
<span id="cb25-461"><a href="#cb25-461" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-462"><a href="#cb25-462" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert image for display</span></span>
<span id="cb25-463"><a href="#cb25-463" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image.shape[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">3</span>:</span>
<span id="cb25-464"><a href="#cb25-464" aria-hidden="true" tabindex="-1"></a>            display_image <span class="op">=</span> image[:<span class="dv">3</span>]  <span class="co"># Use first 3 channels</span></span>
<span id="cb25-465"><a href="#cb25-465" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb25-466"><a href="#cb25-466" aria-hidden="true" tabindex="-1"></a>            display_image <span class="op">=</span> image</span>
<span id="cb25-467"><a href="#cb25-467" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-468"><a href="#cb25-468" aria-hidden="true" tabindex="-1"></a>        display_image <span class="op">=</span> (display_image <span class="op">-</span> display_image.<span class="bu">min</span>()) <span class="op">/</span> (display_image.<span class="bu">max</span>() <span class="op">-</span> display_image.<span class="bu">min</span>())</span>
<span id="cb25-469"><a href="#cb25-469" aria-hidden="true" tabindex="-1"></a>        display_image <span class="op">=</span> display_image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).detach().numpy()</span>
<span id="cb25-470"><a href="#cb25-470" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-471"><a href="#cb25-471" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize CAM to match image size</span></span>
<span id="cb25-472"><a href="#cb25-472" aria-hidden="true" tabindex="-1"></a>        cam_resized <span class="op">=</span> F.interpolate(</span>
<span id="cb25-473"><a href="#cb25-473" aria-hidden="true" tabindex="-1"></a>            cam.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb25-474"><a href="#cb25-474" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>display_image.shape[:<span class="dv">2</span>],</span>
<span id="cb25-475"><a href="#cb25-475" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'bilinear'</span>,</span>
<span id="cb25-476"><a href="#cb25-476" aria-hidden="true" tabindex="-1"></a>            align_corners<span class="op">=</span><span class="va">False</span></span>
<span id="cb25-477"><a href="#cb25-477" aria-hidden="true" tabindex="-1"></a>        ).squeeze().numpy()</span>
<span id="cb25-478"><a href="#cb25-478" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-479"><a href="#cb25-479" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create visualization</span></span>
<span id="cb25-480"><a href="#cb25-480" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb25-481"><a href="#cb25-481" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-482"><a href="#cb25-482" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].imshow(display_image)</span>
<span id="cb25-483"><a href="#cb25-483" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Original Image'</span>)</span>
<span id="cb25-484"><a href="#cb25-484" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb25-485"><a href="#cb25-485" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-486"><a href="#cb25-486" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].imshow(cam_resized, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb25-487"><a href="#cb25-487" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Grad-CAM'</span>)</span>
<span id="cb25-488"><a href="#cb25-488" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb25-489"><a href="#cb25-489" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-490"><a href="#cb25-490" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(display_image)</span>
<span id="cb25-491"><a href="#cb25-491" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(cam_resized, alpha<span class="op">=</span>alpha, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb25-492"><a href="#cb25-492" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'Grad-CAM Overlay'</span>)</span>
<span id="cb25-493"><a href="#cb25-493" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb25-494"><a href="#cb25-494" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-495"><a href="#cb25-495" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb25-496"><a href="#cb25-496" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb25-497"><a href="#cb25-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-498"><a href="#cb25-498" aria-hidden="true" tabindex="-1"></a><span class="co"># Create GradCAM for conv3 layer</span></span>
<span id="cb25-499"><a href="#cb25-499" aria-hidden="true" tabindex="-1"></a>gradcam <span class="op">=</span> GradCAM(model, target_layer<span class="op">=</span><span class="st">'conv3'</span>)</span>
<span id="cb25-500"><a href="#cb25-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-501"><a href="#cb25-501" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate CAM</span></span>
<span id="cb25-502"><a href="#cb25-502" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb25-503"><a href="#cb25-503" aria-hidden="true" tabindex="-1"></a>cam <span class="op">=</span> gradcam.generate_cam(sample_input)</span>
<span id="cb25-504"><a href="#cb25-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-505"><a href="#cb25-505" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generated CAM shape: </span><span class="sc">{</span>cam<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-506"><a href="#cb25-506" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CAM range: [</span><span class="sc">{</span>cam<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>cam<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.3f}</span><span class="ss">]"</span>)</span>
<span id="cb25-507"><a href="#cb25-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-508"><a href="#cb25-508" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize</span></span>
<span id="cb25-509"><a href="#cb25-509" aria-hidden="true" tabindex="-1"></a>gradcam.visualize_cam(sample_input[<span class="dv">0</span>], cam)</span>
<span id="cb25-510"><a href="#cb25-510" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-511"><a href="#cb25-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-512"><a href="#cb25-512" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Analysis and Dimensionality Reduction</span></span>
<span id="cb25-513"><a href="#cb25-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-514"><a href="#cb25-514" aria-hidden="true" tabindex="-1"></a><span class="fu">### PCA analysis of features</span></span>
<span id="cb25-517"><a href="#cb25-517" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-518"><a href="#cb25-518" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_features_pca(features, n_components<span class="op">=</span><span class="dv">50</span>, visualize<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb25-519"><a href="#cb25-519" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Analyze features using PCA"""</span></span>
<span id="cb25-520"><a href="#cb25-520" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-521"><a href="#cb25-521" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten features if needed</span></span>
<span id="cb25-522"><a href="#cb25-522" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb25-523"><a href="#cb25-523" aria-hidden="true" tabindex="-1"></a>        original_shape <span class="op">=</span> features.shape</span>
<span id="cb25-524"><a href="#cb25-524" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features.view(features.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb25-525"><a href="#cb25-525" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb25-526"><a href="#cb25-526" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features</span>
<span id="cb25-527"><a href="#cb25-527" aria-hidden="true" tabindex="-1"></a>        original_shape <span class="op">=</span> features.shape</span>
<span id="cb25-528"><a href="#cb25-528" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-529"><a href="#cb25-529" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to numpy</span></span>
<span id="cb25-530"><a href="#cb25-530" aria-hidden="true" tabindex="-1"></a>    features_np <span class="op">=</span> features_flat.numpy()</span>
<span id="cb25-531"><a href="#cb25-531" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-532"><a href="#cb25-532" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply PCA</span></span>
<span id="cb25-533"><a href="#cb25-533" aria-hidden="true" tabindex="-1"></a>    pca <span class="op">=</span> PCA(n_components<span class="op">=</span>n_components)</span>
<span id="cb25-534"><a href="#cb25-534" aria-hidden="true" tabindex="-1"></a>    features_pca <span class="op">=</span> pca.fit_transform(features_np)</span>
<span id="cb25-535"><a href="#cb25-535" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-536"><a href="#cb25-536" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Analyze explained variance</span></span>
<span id="cb25-537"><a href="#cb25-537" aria-hidden="true" tabindex="-1"></a>    explained_var_ratio <span class="op">=</span> pca.explained_variance_ratio_</span>
<span id="cb25-538"><a href="#cb25-538" aria-hidden="true" tabindex="-1"></a>    cumulative_var <span class="op">=</span> np.cumsum(explained_var_ratio)</span>
<span id="cb25-539"><a href="#cb25-539" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-540"><a href="#cb25-540" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> visualize:</span>
<span id="cb25-541"><a href="#cb25-541" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">5</span>))</span>
<span id="cb25-542"><a href="#cb25-542" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-543"><a href="#cb25-543" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Explained variance</span></span>
<span id="cb25-544"><a href="#cb25-544" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].bar(<span class="bu">range</span>(<span class="bu">len</span>(explained_var_ratio)), explained_var_ratio)</span>
<span id="cb25-545"><a href="#cb25-545" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Explained Variance by Component'</span>)</span>
<span id="cb25-546"><a href="#cb25-546" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Principal Component'</span>)</span>
<span id="cb25-547"><a href="#cb25-547" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Explained Variance Ratio'</span>)</span>
<span id="cb25-548"><a href="#cb25-548" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-549"><a href="#cb25-549" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cumulative explained variance</span></span>
<span id="cb25-550"><a href="#cb25-550" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].plot(cumulative_var, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb25-551"><a href="#cb25-551" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Cumulative Explained Variance'</span>)</span>
<span id="cb25-552"><a href="#cb25-552" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Number of Components'</span>)</span>
<span id="cb25-553"><a href="#cb25-553" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Cumulative Variance Ratio'</span>)</span>
<span id="cb25-554"><a href="#cb25-554" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb25-555"><a href="#cb25-555" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-556"><a href="#cb25-556" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First two components</span></span>
<span id="cb25-557"><a href="#cb25-557" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].scatter(features_pca[:, <span class="dv">0</span>], features_pca[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb25-558"><a href="#cb25-558" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'First Two Principal Components'</span>)</span>
<span id="cb25-559"><a href="#cb25-559" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_xlabel(<span class="st">'PC1'</span>)</span>
<span id="cb25-560"><a href="#cb25-560" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_ylabel(<span class="st">'PC2'</span>)</span>
<span id="cb25-561"><a href="#cb25-561" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-562"><a href="#cb25-562" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb25-563"><a href="#cb25-563" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb25-564"><a href="#cb25-564" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-565"><a href="#cb25-565" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb25-566"><a href="#cb25-566" aria-hidden="true" tabindex="-1"></a>        <span class="st">'features_pca'</span>: features_pca,</span>
<span id="cb25-567"><a href="#cb25-567" aria-hidden="true" tabindex="-1"></a>        <span class="st">'explained_variance_ratio'</span>: explained_var_ratio,</span>
<span id="cb25-568"><a href="#cb25-568" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cumulative_variance'</span>: cumulative_var,</span>
<span id="cb25-569"><a href="#cb25-569" aria-hidden="true" tabindex="-1"></a>        <span class="st">'pca_model'</span>: pca</span>
<span id="cb25-570"><a href="#cb25-570" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb25-571"><a href="#cb25-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-572"><a href="#cb25-572" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample features for analysis</span></span>
<span id="cb25-573"><a href="#cb25-573" aria-hidden="true" tabindex="-1"></a>sample_features <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">256</span>)  <span class="co"># 100 samples, 256 features</span></span>
<span id="cb25-574"><a href="#cb25-574" aria-hidden="true" tabindex="-1"></a>pca_results <span class="op">=</span> analyze_features_pca(sample_features, n_components<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb25-575"><a href="#cb25-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-576"><a href="#cb25-576" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PCA features shape: </span><span class="sc">{</span>pca_results[<span class="st">'features_pca'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-577"><a href="#cb25-577" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 5 components explain </span><span class="sc">{</span>pca_results[<span class="st">'cumulative_variance'</span>][<span class="dv">4</span>]<span class="sc">:.1%}</span><span class="ss"> of variance"</span>)</span>
<span id="cb25-578"><a href="#cb25-578" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-579"><a href="#cb25-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-580"><a href="#cb25-580" aria-hidden="true" tabindex="-1"></a><span class="fu">### t-SNE visualization</span></span>
<span id="cb25-583"><a href="#cb25-583" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-584"><a href="#cb25-584" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_features_tsne(features, labels<span class="op">=</span><span class="va">None</span>, perplexity<span class="op">=</span><span class="dv">30</span>, random_state<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb25-585"><a href="#cb25-585" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Visualize features using t-SNE"""</span></span>
<span id="cb25-586"><a href="#cb25-586" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-587"><a href="#cb25-587" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten features if needed</span></span>
<span id="cb25-588"><a href="#cb25-588" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb25-589"><a href="#cb25-589" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features.view(features.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb25-590"><a href="#cb25-590" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb25-591"><a href="#cb25-591" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features</span>
<span id="cb25-592"><a href="#cb25-592" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-593"><a href="#cb25-593" aria-hidden="true" tabindex="-1"></a>    features_np <span class="op">=</span> features_flat.numpy()</span>
<span id="cb25-594"><a href="#cb25-594" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-595"><a href="#cb25-595" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply t-SNE</span></span>
<span id="cb25-596"><a href="#cb25-596" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Computing t-SNE embedding..."</span>)</span>
<span id="cb25-597"><a href="#cb25-597" aria-hidden="true" tabindex="-1"></a>    tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, perplexity<span class="op">=</span>perplexity, random_state<span class="op">=</span>random_state)</span>
<span id="cb25-598"><a href="#cb25-598" aria-hidden="true" tabindex="-1"></a>    features_tsne <span class="op">=</span> tsne.fit_transform(features_np)</span>
<span id="cb25-599"><a href="#cb25-599" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-600"><a href="#cb25-600" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create visualization</span></span>
<span id="cb25-601"><a href="#cb25-601" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb25-602"><a href="#cb25-602" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-603"><a href="#cb25-603" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb25-604"><a href="#cb25-604" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Color by labels</span></span>
<span id="cb25-605"><a href="#cb25-605" aria-hidden="true" tabindex="-1"></a>        unique_labels <span class="op">=</span> np.unique(labels)</span>
<span id="cb25-606"><a href="#cb25-606" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> plt.cm.tab10(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="bu">len</span>(unique_labels)))</span>
<span id="cb25-607"><a href="#cb25-607" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-608"><a href="#cb25-608" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(unique_labels):</span>
<span id="cb25-609"><a href="#cb25-609" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> labels <span class="op">==</span> label</span>
<span id="cb25-610"><a href="#cb25-610" aria-hidden="true" tabindex="-1"></a>            plt.scatter(features_tsne[mask, <span class="dv">0</span>], features_tsne[mask, <span class="dv">1</span>], </span>
<span id="cb25-611"><a href="#cb25-611" aria-hidden="true" tabindex="-1"></a>                       c<span class="op">=</span>[colors[i]], label<span class="op">=</span><span class="ss">f'Class </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb25-612"><a href="#cb25-612" aria-hidden="true" tabindex="-1"></a>        plt.legend()</span>
<span id="cb25-613"><a href="#cb25-613" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb25-614"><a href="#cb25-614" aria-hidden="true" tabindex="-1"></a>        plt.scatter(features_tsne[:, <span class="dv">0</span>], features_tsne[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb25-615"><a href="#cb25-615" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-616"><a href="#cb25-616" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'t-SNE Visualization of Features'</span>)</span>
<span id="cb25-617"><a href="#cb25-617" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'t-SNE 1'</span>)</span>
<span id="cb25-618"><a href="#cb25-618" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'t-SNE 2'</span>)</span>
<span id="cb25-619"><a href="#cb25-619" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb25-620"><a href="#cb25-620" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb25-621"><a href="#cb25-621" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-622"><a href="#cb25-622" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> features_tsne</span>
<span id="cb25-623"><a href="#cb25-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-624"><a href="#cb25-624" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample data with labels</span></span>
<span id="cb25-625"><a href="#cb25-625" aria-hidden="true" tabindex="-1"></a>sample_features <span class="op">=</span> torch.randn(<span class="dv">200</span>, <span class="dv">256</span>)</span>
<span id="cb25-626"><a href="#cb25-626" aria-hidden="true" tabindex="-1"></a>sample_labels <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">200</span>)  <span class="co"># 5 classes</span></span>
<span id="cb25-627"><a href="#cb25-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-628"><a href="#cb25-628" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with t-SNE</span></span>
<span id="cb25-629"><a href="#cb25-629" aria-hidden="true" tabindex="-1"></a>tsne_features <span class="op">=</span> visualize_features_tsne(sample_features, sample_labels)</span>
<span id="cb25-630"><a href="#cb25-630" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"t-SNE features shape: </span><span class="sc">{</span>tsne_features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-631"><a href="#cb25-631" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-632"><a href="#cb25-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-633"><a href="#cb25-633" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference Optimization</span></span>
<span id="cb25-634"><a href="#cb25-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-635"><a href="#cb25-635" aria-hidden="true" tabindex="-1"></a><span class="fu">### Model quantization for faster inference</span></span>
<span id="cb25-638"><a href="#cb25-638" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-639"><a href="#cb25-639" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantize_model(model, calibration_data<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb25-640"><a href="#cb25-640" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Quantize model for faster inference"""</span></span>
<span id="cb25-641"><a href="#cb25-641" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-642"><a href="#cb25-642" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dynamic quantization (post-training)</span></span>
<span id="cb25-643"><a href="#cb25-643" aria-hidden="true" tabindex="-1"></a>    quantized_model <span class="op">=</span> torch.quantization.quantize_dynamic(</span>
<span id="cb25-644"><a href="#cb25-644" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb25-645"><a href="#cb25-645" aria-hidden="true" tabindex="-1"></a>        {nn.Linear, nn.Conv2d},  <span class="co"># Layers to quantize</span></span>
<span id="cb25-646"><a href="#cb25-646" aria-hidden="true" tabindex="-1"></a>        dtype<span class="op">=</span>torch.qint8</span>
<span id="cb25-647"><a href="#cb25-647" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb25-648"><a href="#cb25-648" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-649"><a href="#cb25-649" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Applied dynamic quantization"</span>)</span>
<span id="cb25-650"><a href="#cb25-650" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-651"><a href="#cb25-651" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_model</span>
<span id="cb25-652"><a href="#cb25-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-653"><a href="#cb25-653" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_inference_speed(original_model, quantized_model, test_input, num_runs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb25-654"><a href="#cb25-654" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare inference speed between models"""</span></span>
<span id="cb25-655"><a href="#cb25-655" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-656"><a href="#cb25-656" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> time</span>
<span id="cb25-657"><a href="#cb25-657" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-658"><a href="#cb25-658" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warm up</span></span>
<span id="cb25-659"><a href="#cb25-659" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb25-660"><a href="#cb25-660" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-661"><a href="#cb25-661" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> original_model(test_input)</span>
<span id="cb25-662"><a href="#cb25-662" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> quantized_model(test_input)</span>
<span id="cb25-663"><a href="#cb25-663" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-664"><a href="#cb25-664" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Time original model</span></span>
<span id="cb25-665"><a href="#cb25-665" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb25-666"><a href="#cb25-666" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb25-667"><a href="#cb25-667" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-668"><a href="#cb25-668" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> original_model(test_input)</span>
<span id="cb25-669"><a href="#cb25-669" aria-hidden="true" tabindex="-1"></a>    original_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb25-670"><a href="#cb25-670" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-671"><a href="#cb25-671" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Time quantized model</span></span>
<span id="cb25-672"><a href="#cb25-672" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb25-673"><a href="#cb25-673" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb25-674"><a href="#cb25-674" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-675"><a href="#cb25-675" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> quantized_model(test_input)</span>
<span id="cb25-676"><a href="#cb25-676" aria-hidden="true" tabindex="-1"></a>    quantized_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb25-677"><a href="#cb25-677" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-678"><a href="#cb25-678" aria-hidden="true" tabindex="-1"></a>    speedup <span class="op">=</span> original_time <span class="op">/</span> quantized_time</span>
<span id="cb25-679"><a href="#cb25-679" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-680"><a href="#cb25-680" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Original model: </span><span class="sc">{</span>original_time<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb25-681"><a href="#cb25-681" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Quantized model: </span><span class="sc">{</span>quantized_time<span class="sc">:.3f}</span><span class="ss">s"</span>) </span>
<span id="cb25-682"><a href="#cb25-682" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Speedup: </span><span class="sc">{</span>speedup<span class="sc">:.2f}</span><span class="ss">x"</span>)</span>
<span id="cb25-683"><a href="#cb25-683" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-684"><a href="#cb25-684" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> speedup</span>
<span id="cb25-685"><a href="#cb25-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-686"><a href="#cb25-686" aria-hidden="true" tabindex="-1"></a><span class="co"># Create quantized version</span></span>
<span id="cb25-687"><a href="#cb25-687" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()  <span class="co"># Important: set to eval mode</span></span>
<span id="cb25-688"><a href="#cb25-688" aria-hidden="true" tabindex="-1"></a>quantized_model <span class="op">=</span> quantize_model(model)</span>
<span id="cb25-689"><a href="#cb25-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-690"><a href="#cb25-690" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare speeds</span></span>
<span id="cb25-691"><a href="#cb25-691" aria-hidden="true" tabindex="-1"></a>test_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb25-692"><a href="#cb25-692" aria-hidden="true" tabindex="-1"></a>speedup <span class="op">=</span> compare_inference_speed(model, quantized_model, test_input, num_runs<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb25-693"><a href="#cb25-693" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-694"><a href="#cb25-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-695"><a href="#cb25-695" aria-hidden="true" tabindex="-1"></a><span class="fu">### Batch size optimization</span></span>
<span id="cb25-698"><a href="#cb25-698" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb25-699"><a href="#cb25-699" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_optimal_batch_size(model, input_shape, device<span class="op">=</span><span class="st">'cpu'</span>, max_batch_size<span class="op">=</span><span class="dv">128</span>):</span>
<span id="cb25-700"><a href="#cb25-700" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Find optimal batch size for memory and speed"""</span></span>
<span id="cb25-701"><a href="#cb25-701" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-702"><a href="#cb25-702" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb25-703"><a href="#cb25-703" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb25-704"><a href="#cb25-704" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-705"><a href="#cb25-705" aria-hidden="true" tabindex="-1"></a>    batch_sizes <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>]</span>
<span id="cb25-706"><a href="#cb25-706" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> max_batch_size <span class="op">&gt;</span> <span class="dv">64</span>:</span>
<span id="cb25-707"><a href="#cb25-707" aria-hidden="true" tabindex="-1"></a>        batch_sizes.extend([<span class="dv">128</span>, <span class="dv">256</span>])</span>
<span id="cb25-708"><a href="#cb25-708" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-709"><a href="#cb25-709" aria-hidden="true" tabindex="-1"></a>    batch_sizes <span class="op">=</span> [bs <span class="cf">for</span> bs <span class="kw">in</span> batch_sizes <span class="cf">if</span> bs <span class="op">&lt;=</span> max_batch_size]</span>
<span id="cb25-710"><a href="#cb25-710" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-711"><a href="#cb25-711" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> {}</span>
<span id="cb25-712"><a href="#cb25-712" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-713"><a href="#cb25-713" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_size <span class="kw">in</span> batch_sizes:</span>
<span id="cb25-714"><a href="#cb25-714" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb25-715"><a href="#cb25-715" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create test batch</span></span>
<span id="cb25-716"><a href="#cb25-716" aria-hidden="true" tabindex="-1"></a>            test_batch <span class="op">=</span> torch.randn(batch_size, <span class="op">*</span>input_shape[<span class="dv">1</span>:]).to(device)</span>
<span id="cb25-717"><a href="#cb25-717" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-718"><a href="#cb25-718" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Measure memory and time</span></span>
<span id="cb25-719"><a href="#cb25-719" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device <span class="op">!=</span> <span class="st">'cpu'</span> <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb25-720"><a href="#cb25-720" aria-hidden="true" tabindex="-1"></a>                torch.cuda.reset_peak_memory_stats()</span>
<span id="cb25-721"><a href="#cb25-721" aria-hidden="true" tabindex="-1"></a>                start_memory <span class="op">=</span> torch.cuda.memory_allocated()</span>
<span id="cb25-722"><a href="#cb25-722" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-723"><a href="#cb25-723" aria-hidden="true" tabindex="-1"></a>            <span class="im">import</span> time</span>
<span id="cb25-724"><a href="#cb25-724" aria-hidden="true" tabindex="-1"></a>            start_time <span class="op">=</span> time.time()</span>
<span id="cb25-725"><a href="#cb25-725" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-726"><a href="#cb25-726" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-727"><a href="#cb25-727" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):  <span class="co"># Average over multiple runs</span></span>
<span id="cb25-728"><a href="#cb25-728" aria-hidden="true" tabindex="-1"></a>                    outputs <span class="op">=</span> model(test_batch)</span>
<span id="cb25-729"><a href="#cb25-729" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-730"><a href="#cb25-730" aria-hidden="true" tabindex="-1"></a>            elapsed_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb25-731"><a href="#cb25-731" aria-hidden="true" tabindex="-1"></a>            throughput <span class="op">=</span> (batch_size <span class="op">*</span> <span class="dv">10</span>) <span class="op">/</span> elapsed_time  <span class="co"># samples per second</span></span>
<span id="cb25-732"><a href="#cb25-732" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-733"><a href="#cb25-733" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device <span class="op">!=</span> <span class="st">'cpu'</span> <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb25-734"><a href="#cb25-734" aria-hidden="true" tabindex="-1"></a>                peak_memory <span class="op">=</span> torch.cuda.max_memory_allocated()</span>
<span id="cb25-735"><a href="#cb25-735" aria-hidden="true" tabindex="-1"></a>                memory_per_sample <span class="op">=</span> (peak_memory <span class="op">-</span> start_memory) <span class="op">/</span> batch_size</span>
<span id="cb25-736"><a href="#cb25-736" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb25-737"><a href="#cb25-737" aria-hidden="true" tabindex="-1"></a>                memory_per_sample <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-738"><a href="#cb25-738" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-739"><a href="#cb25-739" aria-hidden="true" tabindex="-1"></a>            results[batch_size] <span class="op">=</span> {</span>
<span id="cb25-740"><a href="#cb25-740" aria-hidden="true" tabindex="-1"></a>                <span class="st">'throughput'</span>: throughput,</span>
<span id="cb25-741"><a href="#cb25-741" aria-hidden="true" tabindex="-1"></a>                <span class="st">'time_per_sample'</span>: elapsed_time <span class="op">/</span> (batch_size <span class="op">*</span> <span class="dv">10</span>),</span>
<span id="cb25-742"><a href="#cb25-742" aria-hidden="true" tabindex="-1"></a>                <span class="st">'memory_per_sample'</span>: memory_per_sample <span class="op">/</span> (<span class="dv">1024</span><span class="op">**</span><span class="dv">2</span>)  <span class="co"># MB</span></span>
<span id="cb25-743"><a href="#cb25-743" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb25-744"><a href="#cb25-744" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-745"><a href="#cb25-745" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Batch size </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>throughput<span class="sc">:.1f}</span><span class="ss"> samples/sec, "</span></span>
<span id="cb25-746"><a href="#cb25-746" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f"</span><span class="sc">{</span>memory_per_sample <span class="op">/</span> (<span class="dv">1024</span><span class="op">**</span><span class="dv">2</span>)<span class="sc">:.1f}</span><span class="ss"> MB/sample"</span>)</span>
<span id="cb25-747"><a href="#cb25-747" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb25-748"><a href="#cb25-748" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb25-749"><a href="#cb25-749" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">"out of memory"</span> <span class="kw">in</span> <span class="bu">str</span>(e):</span>
<span id="cb25-750"><a href="#cb25-750" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Batch size </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">: Out of memory"</span>)</span>
<span id="cb25-751"><a href="#cb25-751" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb25-752"><a href="#cb25-752" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb25-753"><a href="#cb25-753" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> e</span>
<span id="cb25-754"><a href="#cb25-754" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-755"><a href="#cb25-755" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find optimal batch size (highest throughput)</span></span>
<span id="cb25-756"><a href="#cb25-756" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> results:</span>
<span id="cb25-757"><a href="#cb25-757" aria-hidden="true" tabindex="-1"></a>        optimal_batch_size <span class="op">=</span> <span class="bu">max</span>(results.keys(), key<span class="op">=</span><span class="kw">lambda</span> k: results[k][<span class="st">'throughput'</span>])</span>
<span id="cb25-758"><a href="#cb25-758" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Optimal batch size: </span><span class="sc">{</span>optimal_batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-759"><a href="#cb25-759" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> optimal_batch_size, results</span>
<span id="cb25-760"><a href="#cb25-760" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-761"><a href="#cb25-761" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span>, results</span>
<span id="cb25-762"><a href="#cb25-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-763"><a href="#cb25-763" aria-hidden="true" tabindex="-1"></a><span class="co"># Find optimal batch size</span></span>
<span id="cb25-764"><a href="#cb25-764" aria-hidden="true" tabindex="-1"></a>optimal_bs, batch_results <span class="op">=</span> find_optimal_batch_size(</span>
<span id="cb25-765"><a href="#cb25-765" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb25-766"><a href="#cb25-766" aria-hidden="true" tabindex="-1"></a>    input_shape<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb25-767"><a href="#cb25-767" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span><span class="st">'cpu'</span>,</span>
<span id="cb25-768"><a href="#cb25-768" aria-hidden="true" tabindex="-1"></a>    max_batch_size<span class="op">=</span><span class="dv">64</span></span>
<span id="cb25-769"><a href="#cb25-769" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-770"><a href="#cb25-770" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb25-771"><a href="#cb25-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-772"><a href="#cb25-772" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb25-773"><a href="#cb25-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-774"><a href="#cb25-774" aria-hidden="true" tabindex="-1"></a>Key inference and feature extraction techniques:</span>
<span id="cb25-775"><a href="#cb25-775" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Basic inference**: Single image and batch processing</span>
<span id="cb25-776"><a href="#cb25-776" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feature extraction**: Layer-wise and multi-scale features  </span>
<span id="cb25-777"><a href="#cb25-777" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Attention visualization**: Understanding model focus</span>
<span id="cb25-778"><a href="#cb25-778" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gradient explanations**: Grad-CAM for interpretability</span>
<span id="cb25-779"><a href="#cb25-779" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dimensionality reduction**: PCA and t-SNE analysis</span>
<span id="cb25-780"><a href="#cb25-780" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Optimization**: Quantization and batch size tuning</span>
<span id="cb25-781"><a href="#cb25-781" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Performance monitoring**: Speed and memory profiling</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/geog-logo.png" class="img-fluid figure-img" width="250"></p>
<figcaption>Department of Geography logo</figcaption>
</figure>
</div>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This website is built with <a href="https://github.com/kcaylor/GEOG-288KC-geospatial-foundation-models"><i class="fa-brands fa-github" title="the github octocat logo" aria-label="github"></i></a> and <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>