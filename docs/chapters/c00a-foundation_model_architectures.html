<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Foundation Model Architectures</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">GEOG 288KC</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">🏠 home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../Syllabus.html"> 
<span class="menu-text">📋 syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-weekly-sessions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">💻 weekly sessions</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-weekly-sessions">    
        <li>
    <a class="dropdown-item" href="../chapters/c01-geospatial-data-foundations.html">
 <span class="dropdown-text">Week 1 - 🚀 Core Tools and Data Access</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c02-spatial-temporal-attention-mechanisms.html">
 <span class="dropdown-text">Week 2 - ⚡ Rapid Remote Sensing Preprocessing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c03a-terratorch-foundations.html">
 <span class="dropdown-text">Week 3a - 🌍 TerraTorch Foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c03-complete-gfm-architecture.html">
 <span class="dropdown-text">Week 3b - 🤖 Machine Learning on Remote Sensing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c04-pretraining-implementation.html">
 <span class="dropdown-text">Week 4 - 🏗️ Foundation Models in Practice</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c05-training-loop-optimization.html">
 <span class="dropdown-text">Week 5 - 🔧 Fine-Tuning &amp; Transfer Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c06-model-evaluation-analysis.html">
 <span class="dropdown-text">Week 6 - ⏰ Spatiotemporal Modeling &amp; Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-cheatsheets" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">👀 cheatsheets</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-cheatsheets">    
        <li>
    <a class="dropdown-item" href="../cheatsheets.html">
 <span class="dropdown-text">📋 All Cheatsheets</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">⚡ Quick Starts</li>
        <li>
    <a class="dropdown-item" href="../extras/cheatsheets/week01_imports.html">
 <span class="dropdown-text">Week 01: Import Guide</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-explainers" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">🧩 explainers</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-explainers">    
        <li class="dropdown-header">1️⃣ Week 1</li>
        <li>
    <a class="dropdown-item" href="../extras/ai-ml-dl-fm-hierarchy.html">
 <span class="dropdown-text">🤖 AI/ML/DL/FM Hierarchy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/geospatial-foundation-model-predictions-standalone.html">
 <span class="dropdown-text">🎯 GFM Predictions (Standalone)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/geospatial-prediction-hierarchy.html">
 <span class="dropdown-text">✅ Geospatial Task/Prediction Types</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/neural_networks_explainer.html">
 <span class="dropdown-text">🧠 Neural Networks: Neurons to Transformers</span></a>
  </li>  
        <li class="dropdown-header">2️⃣ Week 2</li>
        <li>
    <a class="dropdown-item" href="../chapters/c00a-foundation_model_architectures.html">
 <span class="dropdown-text">🏗️ Foundation Model Architectures</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c00b-introduction-to-deeplearning-architecture.html">
 <span class="dropdown-text">🎓 Introduction to Deep Learning Architecture</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-extras" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">📖 extras</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-extras">    
        <li class="dropdown-header">🎯 Practical Examples</li>
        <li>
    <a class="dropdown-item" href="../extras/examples/normalization_comparison.html">
 <span class="dropdown-text">Normalization Comparison</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/resnet.html">
 <span class="dropdown-text">ResNet Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/text_encoder.html">
 <span class="dropdown-text">Text Encoder</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/tiling-and-patches.html">
 <span class="dropdown-text">Tiling and Patches</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/terratorch_workflows.html">
 <span class="dropdown-text">TerraTorch Workflows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/resources/course_resources.html">
 <span class="dropdown-text">📚 Reference Materials</span></a>
  </li>  
        <li class="dropdown-header">📁 Project Templates</li>
        <li>
    <a class="dropdown-item" href="../extras/projects/project-proposal-template.html">
 <span class="dropdown-text">Project Proposal Template</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/projects/mvp-template.html">
 <span class="dropdown-text">Project Results Template</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gfms-from-scratch/gfms-from-scratch.github.io" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Foundation Model Architectures</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">LLMs vs.&nbsp;Geospatial Foundation Models (GFMs)</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-foundation-model-architectures" id="toc-introduction-to-foundation-model-architectures" class="nav-link active" data-scroll-target="#introduction-to-foundation-model-architectures">Introduction to Foundation Model Architectures</a>
  <ul class="collapse">
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#evolution-from-ai-to-transformers" id="toc-evolution-from-ai-to-transformers" class="nav-link" data-scroll-target="#evolution-from-ai-to-transformers">Evolution from AI to Transformers</a></li>
  <li><a href="#transformer-architecture-essentials" id="toc-transformer-architecture-essentials" class="nav-link" data-scroll-target="#transformer-architecture-essentials">Transformer Architecture Essentials</a></li>
  </ul></li>
  <li><a href="#llms-vs-gfms" id="toc-llms-vs-gfms" class="nav-link" data-scroll-target="#llms-vs-gfms">LLMs vs GFMs</a>
  <ul class="collapse">
  <li><a href="#step-development-pipeline-comparison" id="toc-step-development-pipeline-comparison" class="nav-link" data-scroll-target="#step-development-pipeline-comparison">9-Step Development Pipeline Comparison</a></li>
  <li><a href="#step-by-step-detailed-comparison" id="toc-step-by-step-detailed-comparison" class="nav-link" data-scroll-target="#step-by-step-detailed-comparison">Step-by-Step Detailed Comparison</a></li>
  <li><a href="#data-requirements-and-constraints" id="toc-data-requirements-and-constraints" class="nav-link" data-scroll-target="#data-requirements-and-constraints">Data Requirements and Constraints</a></li>
  <li><a href="#implementation-examples" id="toc-implementation-examples" class="nav-link" data-scroll-target="#implementation-examples">Implementation Examples</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="introduction-to-foundation-model-architectures" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-foundation-model-architectures">Introduction to Foundation Model Architectures</h2>
<p>Foundation models represent a paradigm shift in machine learning, where large-scale models are trained on diverse, unlabeled data and then adapted to various downstream tasks. The term was coined by the <a href="https://hai.stanford.edu/news/reflections-foundation-models">Stanford HAI Center</a> to describe models like GPT-3, BERT, and CLIP that serve as a “foundation” for numerous applications.</p>
<p>This cheatsheet provides a comprehensive comparison between <strong>Language Models (LLMs)</strong> and <strong>Geospatial Foundation Models (GFMs)</strong>, examining their architectures, training procedures, and deployment strategies. While LLMs process discrete text tokens, GFMs work with continuous multi-spectral imagery, requiring fundamentally different approaches to tokenization, positional encoding, and objective functions.</p>
<p><strong>Key Resources:</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> - Original Transformer paper</li>
<li><a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words</a> - Vision Transformer (ViT)</li>
<li><a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a> - MAE approach</li>
<li><a href="https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M">Prithvi Foundation Model</a> - IBM/NASA geospatial model</li>
</ul>
<section id="setup" class="level3">
<h3 class="anchored" data-anchor-id="setup">Setup</h3>
<div id="46ed7d4a" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoConfig</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CUDA available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># Always False on Mac</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>mps_available <span class="op">=</span> torch.backends.mps.is_available()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MPS available: </span><span class="sc">{</span>mps_available<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> mps_available:</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch version: 2.7.1
CUDA available: False
MPS available: True
Using device: mps</code></pre>
</div>
</div>
</section>
<section id="evolution-from-ai-to-transformers" class="level3">
<h3 class="anchored" data-anchor-id="evolution-from-ai-to-transformers">Evolution from AI to Transformers</h3>
<p>The development of foundation models represents a convergence of several key technological breakthroughs. The journey from symbolic AI to modern foundation models reflects both advances in computational power and fundamental insights about learning representations from large-scale data.</p>
<section id="key-historical-milestones" class="level4">
<h4 class="anchored" data-anchor-id="key-historical-milestones">Key Historical Milestones</h4>
<p>The transformer architecture emerged from decades of research in neural networks, attention mechanisms, and sequence modeling. The 2017 paper “<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>” introduced the transformer, which became the foundation for both language and vision models.</p>
<p><strong>Critical Developments:</strong></p>
<ul>
<li><strong>1950s-1990s</strong>: Symbolic AI dominated, with rule-based systems and early neural networks</li>
<li><strong>2012</strong>: <a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">AlexNet’s ImageNet victory</a> sparked the deep learning revolution</li>
<li><strong>2017</strong>: The Transformer architecture introduced self-attention and parallelizable training</li>
<li><strong>2018-2020</strong>: BERT and GPT families demonstrated the power of pre-trained language models</li>
<li><strong>2021-2024</strong>: Scaling laws, <a href="https://openai.com/blog/chatgpt">ChatGPT</a>, and multimodal models like <a href="https://arxiv.org/abs/2103.00020">CLIP</a></li>
</ul>
<p><strong>Foundation Model Evolution Timeline:</strong></p>
<ul>
<li><strong>2017 - Transformer</strong>: Base architecture introduced</li>
<li><strong>2018 - BERT-Base</strong>: 110M parameters</li>
<li><strong>2019 - GPT-2</strong>: 1.5B parameters<br>
</li>
<li><strong>2020 - GPT-3</strong>: 175B parameters</li>
<li><strong>2021 - ViT-Large</strong>: 307M parameters for vision</li>
<li><strong>2022 - PaLM</strong>: 540B parameters</li>
<li><strong>2023 - GPT-4</strong>: ~1T parameters (estimated)</li>
<li><strong>2024 - Claude-3</strong>: Multi-modal capabilities</li>
</ul>
</section>
</section>
<section id="transformer-architecture-essentials" class="level3">
<h3 class="anchored" data-anchor-id="transformer-architecture-essentials">Transformer Architecture Essentials</h3>
<p>The transformer architecture revolutionized deep learning by introducing the <strong>self-attention mechanism</strong>, which allows models to process sequences in parallel rather than sequentially. This architecture forms the backbone of both language models (GPT, BERT) and vision models (ViT, DINO).</p>
<p><strong>Core Components:</strong></p>
<ul>
<li><strong>Multi-Head Attention</strong>: Allows the model to attend to different representation subspaces (see <a href="https://en.wikipedia.org/wiki/Self-attention">Self-attention</a>)</li>
<li><strong>Feed-Forward Networks</strong>: Point-wise processing with GELU activation (see <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron</a>)</li>
<li><strong>Residual Connections</strong>: Enable training of very deep networks (see <a href="https://en.wikipedia.org/wiki/Residual_neural_network">Residual neural network</a>)</li>
<li><strong>Layer Normalization</strong>: Stabilizes training and improves convergence (see <a href="https://en.wikipedia.org/wiki/Layer_normalization">Layer normalization</a>)</li>
</ul>
<p><strong>Key terms (quick definitions):</strong></p>
<ul>
<li><strong>Self-Attention (SA)</strong>: A mechanism where each token attends to every other token (including itself) to compute a weighted combination of their representations.</li>
<li><strong>Queries (Q), Keys (K), Values (V)</strong>: Learned linear projections of the input. Attention scores are computed from Q·Kᵀ; those scores weight V to produce the output (see <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">Attention (machine learning)</a>).</li>
<li><strong>Multi-Head Attention (MHA)</strong>: Runs several SA “heads” in parallel on different subspaces, then concatenates and projects the results. This lets the model capture diverse relationships (see <a href="https://en.wikipedia.org/wiki/Self-attention">Self-attention</a>).</li>
<li><strong>Feed-Forward Network (FFN/MLP)</strong>: A small two-layer MLP (Multi-Layer Perceptron) applied to each token independently; expands the dimension then projects back (often with GELU activation) (see <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron</a>).</li>
<li><strong>Residual Connection (Skip)</strong>: Adds the input of a sub-layer to its output, helping gradients flow and preserving information (see <a href="https://en.wikipedia.org/wiki/Residual_neural_network">Residual neural network</a>).</li>
<li><strong>Layer Normalization (LayerNorm, LN)</strong>: Normalizes features per token to stabilize training. The ordering with residuals defines Post-LN vs Pre-LN variants (see <a href="https://en.wikipedia.org/wiki/Layer_normalization">Layer normalization</a>).</li>
<li><strong>GELU</strong>: Gaussian Error Linear Unit activation; smoother than ReLU and standard in transformers (see <a href="https://en.wikipedia.org/wiki/Activation_function#Gaussian_error_linear_unit_(GELU)">Gaussian error linear unit</a>).</li>
<li><strong>Tensor shape convention</strong>: Using <code>batch_first=True</code>, tensors are <code>[batch_size, seq_len, embed_dim]</code>.</li>
<li><strong>Abbreviations used</strong>: <code>B</code> = batch size, <code>S</code> = sequence length (or number of tokens/patches), <code>E</code> = embedding dimension, <code>H</code> = number of attention heads.</li>
</ul>
<p>The following implementation demonstrates a simplified transformer block following the <a href="https://arxiv.org/abs/1706.03762">original architecture</a>:</p>
<div id="0fcd5e83" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleTransformerBlock(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Transformer block with Multi-Head Self-Attention (MHSA) and MLP.</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs/Outputs:</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">      - x: [batch_size, seq_len, embed_dim]</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">      - returns: same shape as input</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Structure (Post-LN variant):</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">      x = LN(x + MHSA(x))</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">      x = LN(x + MLP(x))</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">8</span>, mlp_ratio<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multi-Head Self-Attention (MHSA)</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># - batch_first=True → tensors are [B, S, E]</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># - Self-attention uses Q=K=V=linear(x) by default</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> nn.MultiheadAttention(</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            embed_dim<span class="op">=</span>embed_dim,</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads,</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Position-wise feed-forward network (applied independently to each token)</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        hidden_dim <span class="op">=</span> <span class="bu">int</span>(embed_dim <span class="op">*</span> mlp_ratio)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>            nn.Linear(embed_dim, hidden_dim),  <span class="co"># expand</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),                         <span class="co"># nonlinearity</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_dim, embed_dim),  <span class="co"># project back</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># LayerNorms used after residual additions (Post-LN)</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x: [B, S, E]</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multi-head self-attention</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># attn_out: [B, S, E]; attn_weights (ignored here) would be [B, num_heads, S, S]</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        attn_out, _ <span class="op">=</span> <span class="va">self</span>.attention(x, x, x)  <span class="co"># Q=K=V=x (self-attention)</span></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Residual connection + LayerNorm (Post-LN)</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> attn_out)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Position-wise MLP</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>        mlp_out <span class="op">=</span> <span class="va">self</span>.mlp(x)  <span class="co"># [B, S, E]</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Residual connection + LayerNorm (Post-LN)</span></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> mlp_out)</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x  <span class="co"># shape preserved: [B, S, E]</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage and shape checks</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>transformer_block <span class="op">=</span> SimpleTransformerBlock(embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">100</span>, <span class="dv">768</span>)  <span class="co"># [batch_size, seq_len, embed_dim]</span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> transformer_block(sample_input)</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape:  </span><span class="sc">{</span>sample_input<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># should match input</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Parameters:  </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> transformer_block.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape:  torch.Size([2, 100, 768])
Output shape: torch.Size([2, 100, 768])
Parameters:  7,087,872</code></pre>
</div>
</div>
<p>What to notice: - Shapes are stable through the block: <code>[B, S, E] → [B, S, E]</code>. - Residuals + LayerNorm appear after each sublayer (Post-LN variant). - MLP expands to <code>mlp_ratio × embed_dim</code> then projects back.</p>
</section>
</section>
<section id="llms-vs-gfms" class="level2">
<h2 class="anchored" data-anchor-id="llms-vs-gfms">LLMs vs GFMs</h2>
<p>Both LLMs and GFMs follow similar high-level development pipelines, but differ in the details because language and satellite imagery are very different kinds of data.</p>
<section id="step-development-pipeline-comparison" class="level3">
<h3 class="anchored" data-anchor-id="step-development-pipeline-comparison">9-Step Development Pipeline Comparison</h3>
<ol type="1">
<li><strong>Data Preparation</strong>: Gather raw data and clean it up so the model can learn useful patterns.</li>
<li><strong>Tokenization (turning inputs into pieces the model can handle)</strong>: Decide how to chop inputs into small parts the model can process.</li>
<li><strong>Architecture (the model blueprint)</strong>: Choose how many layers, how wide/tall the model is, and how it connects information.</li>
<li><strong>Pretraining Objective (what the model practices)</strong>: Pick the learning task the model does before any specific application.</li>
<li><strong>Training Loop (how learning happens)</strong>: Decide optimizers, learning rate, precision, and how to stabilize training.</li>
<li><strong>Evaluation (how we check learning)</strong>: Use simple tests to see if the model is improving in the right ways.</li>
<li><strong>Pretrained Weights (starting point)</strong>: Load existing model parameters to avoid training from scratch.</li>
<li><strong>Finetuning (adapting the model)</strong>: Add a small head or nudge the model for a specific task with labeled examples.</li>
<li><strong>Deployment (using the model in practice)</strong>: Serve the model efficiently and handle real-world input sizes.</li>
</ol>
<section id="llm-development-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="llm-development-pipeline">LLM Development Pipeline</h4>
<p>Language models like <a href="https://openai.com/research/gpt-4">GPT</a> and <a href="https://arxiv.org/abs/1810.04805">BERT</a> have established a mature development pipeline refined through years of research. The process emphasizes text quality, vocabulary optimization, and language understanding benchmarks.</p>
<p><strong>Key References:</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> - GPT-3 methodology</li>
<li><a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions</a> - InstructGPT</li>
<li><a href="https://arxiv.org/abs/2204.02311">PaLM: Scaling Language Modeling</a> - Large-scale training</li>
</ul>
</section>
<section id="gfm-development-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="gfm-development-pipeline">GFM Development Pipeline</h4>
<p>Geospatial foundation models adapt the transformer architecture for Earth observation data, requiring specialized handling of multi-spectral imagery, temporal sequences, and spatial relationships.</p>
<p><strong>Key References:</strong></p>
<ul>
<li><a href="https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M">Prithvi Foundation Model</a> - IBM/NASA collaboration</li>
<li><a href="https://arxiv.org/abs/2207.08051">SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery</a></li>
<li><a href="https://clay-foundation.github.io/model/">Clay Foundation Model</a> - Open-source geospatial model</li>
</ul>
<p><strong>Side-by-side (LLMs vs GFMs)</strong></p>
<div class="table-responsive">
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 29%">
<col style="width: 57%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>LLMs (text)</th>
<th>GFMs (satellite imagery)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1. Data Preparation</td>
<td>Collect large text sets, remove duplicates and low-quality content</td>
<td>Collect scenes from sensors, correct for atmosphere/sensor, remove clouds, tile into manageable chips</td>
</tr>
<tr class="even">
<td>2. Tokenization</td>
<td>Break text into subword tokens; build a vocabulary</td>
<td>Cut images into patches; turn each patch into a vector; add 2D (and time) positions</td>
</tr>
<tr class="odd">
<td>3. Architecture</td>
<td>Transformer layers over token sequences (often decoder-only for GPT, encoder-only for BERT)</td>
<td>Vision Transformer-style encoders over patch sequences; may include temporal attention for time series</td>
</tr>
<tr class="even">
<td>4. Pretraining Objective</td>
<td>Predict the next/missing word to learn language patterns</td>
<td>Reconstruct masked image patches or learn similarities across views/time to learn visual patterns</td>
</tr>
<tr class="odd">
<td>5. Training Loop</td>
<td>AdamW, learning-rate schedule, mixed precision; long sequences can stress memory</td>
<td>Similar tools, but memory shaped by patch size, bands, and image tiling; may mask clouds or invalid pixels</td>
</tr>
<tr class="even">
<td>6. Evaluation</td>
<td>Quick checks like “how surprised is the model?” (e.g., next-word loss) and small downstream tasks</td>
<td>Quick checks like reconstruction quality and small downstream tasks (e.g., linear probes on land cover)</td>
</tr>
<tr class="odd">
<td>7. Pretrained Weights</td>
<td>Download weights and matching tokenizer from model hubs</td>
<td>Download weights from hubs (e.g., Prithvi, SatMAE); ensure band order and preprocessing match</td>
</tr>
<tr class="even">
<td>8. Finetuning</td>
<td>Add a small head or adapters; few labeled examples can go far</td>
<td>Add a task head (classification/segmentation); often freeze encoder and train a light head on small datasets</td>
</tr>
<tr class="odd">
<td>9. Deployment</td>
<td>Serve via APIs; speed up with caching of past context</td>
<td>Run sliding-window/tiling over large scenes; export results as geospatial rasters/vectors</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="step-by-step-detailed-comparison" class="level3">
<h3 class="anchored" data-anchor-id="step-by-step-detailed-comparison">Step-by-Step Detailed Comparison</h3>
<p>Let’s look at more detailed comparisons beetween each development pipeline step, highlighting the fundamental differences between text and geospatial data processing.</p>
<section id="data-preparation-differences" class="level4">
<h4 class="anchored" data-anchor-id="data-preparation-differences">Data Preparation Differences</h4>
<p>Data preparation represents one of the most significant differences between LLMs and GFMs. LLMs work with human-generated text that requires quality filtering and deduplication, while GFMs process sensor data that requires physical corrections and calibration.</p>
<p><strong>LLM Data Challenges:</strong></p>
<ul>
<li><strong>Scale</strong>: Training datasets like <a href="https://commoncrawl.org/">CommonCrawl</a> contain hundreds of terabytes</li>
<li><strong>Quality</strong>: Filtering toxic content, spam, and low-quality text</li>
<li><strong>Deduplication</strong>: Removing exact and near-duplicate documents</li>
<li><strong>Language Detection</strong>: Identifying and filtering by language</li>
</ul>
<p><strong>GFM Data Challenges:</strong></p>
<ul>
<li><strong>Sensor Calibration</strong>: Converting raw digital numbers to physical units</li>
<li><strong>Atmospheric Correction</strong>: Removing atmospheric effects from satellite imagery</li>
<li><strong>Cloud Masking</strong>: Identifying and handling cloudy pixels</li>
<li><strong>Georegistration</strong>: Aligning images to geographic coordinate systems</li>
</ul>
<div id="b13df0e3" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LLM text preprocessing example</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>sample_texts <span class="op">=</span> [</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The quick brown fox jumps over the lazy dog."</span>,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Machine learning is transforming many industries."</span>, </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Climate change requires urgent global action."</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic tokenization for vocabulary construction</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> sample_texts:</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    vocab.update(text.lower().replace(<span class="st">'.'</span>, <span class="st">''</span>).split())</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LLM Data Processing:"</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample vocabulary size: </span><span class="sc">{</span><span class="bu">len</span>(vocab)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample tokens: </span><span class="sc">{</span><span class="bu">list</span>(vocab)[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># GFM satellite data preprocessing example</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>num_bands <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate raw satellite patch (typical 12-bit values)</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>satellite_patch <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">4096</span>, (num_bands, patch_size, patch_size))</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate cloud mask (20% cloud coverage)</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>cloud_mask <span class="op">=</span> np.random.random((patch_size, patch_size)) <span class="op">&gt;</span> <span class="fl">0.8</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply atmospheric correction (normalize to [0,1])</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>corrected_patch <span class="op">=</span> satellite_patch.astype(np.float32) <span class="op">/</span> <span class="fl">4095.0</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>corrected_patch[:, cloud_mask] <span class="op">=</span> np.nan  <span class="co"># Mask cloudy pixels</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GFM Data Processing:"</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Satellite patch shape: </span><span class="sc">{</span>satellite_patch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> (bands, height, width)"</span>)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cloud coverage: </span><span class="sc">{</span>cloud_mask<span class="sc">.</span><span class="bu">sum</span>() <span class="op">/</span> cloud_mask<span class="sc">.</span>size <span class="op">*</span> <span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%"</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Valid pixels per band: </span><span class="sc">{</span>(<span class="op">~</span>np.isnan(corrected_patch[<span class="dv">0</span>]))<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LLM Data Processing:
Sample vocabulary size: 20
Sample tokens: ['climate', 'requires', 'industries', 'urgent', 'machine', 'learning', 'fox', 'quick', 'is', 'transforming']

==================================================

GFM Data Processing:
Satellite patch shape: (6, 64, 64) (bands, height, width)
Cloud coverage: 20.3%
Valid pixels per band: 3,265</code></pre>
</div>
</div>
</section>
<section id="tokenization-approaches" class="level4">
<h4 class="anchored" data-anchor-id="tokenization-approaches">Tokenization Approaches</h4>
<p>Tokenization represents a fundamental difference between language and vision models. LLMs use <strong>discrete tokenization</strong> with learned vocabularies (like <a href="https://arxiv.org/abs/1508.07909">BPE</a>), while GFMs use <strong>continuous tokenization</strong> through patch embeddings inspired by <a href="https://arxiv.org/abs/2010.11929">Vision Transformers</a>.</p>
<p><strong>LLM Tokenization:</strong></p>
<ul>
<li><strong>Byte-Pair Encoding (BPE)</strong>: Learns subword units to handle out-of-vocabulary words</li>
<li><strong>Vocabulary Size</strong>: Typically 30K-100K tokens balancing efficiency and coverage</li>
<li><strong>Special Tokens</strong>: <code>[CLS]</code>, <code>[SEP]</code>, <code>[PAD]</code>, <code>[MASK]</code> for different tasks</li>
</ul>
<p><strong>GFM Tokenization:</strong></p>
<ul>
<li><strong>Patch Embedding</strong>: Divides images into fixed-size patches (e.g., 16×16 pixels)</li>
<li><strong>Linear Projection</strong>: Maps high-dimensional patches to embedding space</li>
<li><strong>Positional Encoding</strong>: 2D spatial positions rather than 1D sequence positions</li>
</ul>
<div id="2619008c" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LLM discrete tokenization example</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>vocab_size, embed_dim <span class="op">=</span> <span class="dv">50000</span>, <span class="dv">768</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>token_ids <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">234</span>, <span class="dv">5678</span>, <span class="dv">2</span>])  <span class="co"># [CLS, word1, word2, word3, SEP]</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>token_embeddings <span class="op">=</span> embedding_layer(token_ids)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LLM Tokenization (Discrete):"</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Token IDs: </span><span class="sc">{</span>token_ids<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Token embeddings shape: </span><span class="sc">{</span>token_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Vocabulary size: </span><span class="sc">{</span>vocab_size<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">40</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># GFM continuous patch tokenization</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>num_bands <span class="op">=</span> <span class="dv">6</span>  <span class="co"># Multi-spectral bands</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">768</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>num_patches <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>patch_dim <span class="op">=</span> patch_size <span class="op">*</span> patch_size <span class="op">*</span> num_bands</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>patches <span class="op">=</span> torch.randn(num_patches, patch_dim)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear projection for patch embedding</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>patch_projection <span class="op">=</span> nn.Linear(patch_dim, embed_dim)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>patch_embeddings <span class="op">=</span> patch_projection(patches)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GFM Tokenization (Continuous Patches):"</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch dimensions: </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">×</span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">×</span><span class="sc">{</span>num_bands<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>patch_dim<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch embeddings shape: </span><span class="sc">{</span>patch_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"No discrete vocabulary - continuous projection"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LLM Tokenization (Discrete):
Token IDs: [1, 15, 234, 5678, 2]
Token embeddings shape: torch.Size([5, 768])
Vocabulary size: 50,000

----------------------------------------

GFM Tokenization (Continuous Patches):
Patch dimensions: 16×16×6 = 1536
Patch embeddings shape: torch.Size([4, 768])
No discrete vocabulary - continuous projection</code></pre>
</div>
</div>
</section>
<section id="architecture-comparison" class="level4">
<h4 class="anchored" data-anchor-id="architecture-comparison">Architecture Comparison</h4>
<p>While both LLMs and GFMs use transformer architectures, they differ in input processing, positional encoding, and output heads. LLMs like <a href="https://openai.com/research/gpt-4">GPT</a> use causal attention for autoregressive generation, while GFMs like <a href="https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M">Prithvi</a> use bidirectional attention for representation learning.</p>
<p><strong>Key Architectural Differences:</strong></p>
<ul>
<li><strong>Input Processing</strong>: 1D token sequences vs.&nbsp;2D spatial patches</li>
<li><strong>Positional Encoding</strong>: 1D learned positions vs.&nbsp;2D spatial coordinates</li>
<li><strong>Attention Patterns</strong>: Causal masking vs.&nbsp;full bidirectional attention</li>
<li><strong>Output Heads</strong>: Language modeling head vs.&nbsp;reconstruction/classification heads</li>
</ul>
<div id="e4ae015a" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LLMArchitecture(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simplified LLM architecture (GPT-style)"""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size<span class="op">=</span><span class="dv">50000</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_layers<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_encoding <span class="op">=</span> nn.Embedding(<span class="dv">2048</span>, embed_dim)  <span class="co"># Max sequence length</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>            SimpleTransformerBlock(embed_dim, num_heads) </span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_final <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_head <span class="op">=</span> nn.Linear(embed_dim, vocab_size)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids):</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> input_ids.shape[<span class="dv">1</span>]</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> torch.arange(seq_len, device<span class="op">=</span>input_ids.device)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Token + positional embeddings</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(input_ids) <span class="op">+</span> <span class="va">self</span>.positional_encoding(positions)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transformer layers</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_final(x)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.output_head(x)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GFMArchitecture(nn.Module):</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simplified GFM architecture (ViT-style)"""</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, patch_size<span class="op">=</span><span class="dv">16</span>, num_bands<span class="op">=</span><span class="dv">6</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_layers<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_bands <span class="op">=</span> num_bands</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        patch_dim <span class="op">=</span> patch_size <span class="op">*</span> patch_size <span class="op">*</span> num_bands</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embedding <span class="op">=</span> nn.Linear(patch_dim, embed_dim)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2D positional embedding</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed_h <span class="op">=</span> nn.Embedding(<span class="dv">100</span>, embed_dim <span class="op">//</span> <span class="dv">2</span>)  <span class="co"># Height positions</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed_w <span class="op">=</span> nn.Embedding(<span class="dv">100</span>, embed_dim <span class="op">//</span> <span class="dv">2</span>)  <span class="co"># Width positions</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>            SimpleTransformerBlock(embed_dim, num_heads) </span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_final <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, patches, patch_positions):</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>        batch_size, num_patches, patch_dim <span class="op">=</span> patches.shape</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch embeddings</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embedding(patches)</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2D positional embeddings</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>        pos_h, pos_w <span class="op">=</span> patch_positions[:, :, <span class="dv">0</span>], patch_positions[:, :, <span class="dv">1</span>]</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>        pos_emb <span class="op">=</span> torch.cat([</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pos_embed_h(pos_h),</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pos_embed_w(pos_w)</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>        ], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> pos_emb</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transformer layers</span></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_final(x)</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare architectures</span></span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>llm_model <span class="op">=</span> LLMArchitecture(vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">384</span>, num_layers<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>gfm_model <span class="op">=</span> GFMArchitecture(patch_size<span class="op">=</span><span class="dv">16</span>, num_bands<span class="op">=</span><span class="dv">6</span>, embed_dim<span class="op">=</span><span class="dv">384</span>, num_layers<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>llm_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> llm_model.parameters())</span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>gfm_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> gfm_model.parameters())</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Architecture Comparison:"</span>)</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LLM parameters: </span><span class="sc">{</span>llm_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GFM parameters: </span><span class="sc">{</span>gfm_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Test forward passes</span></span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>sample_tokens <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10000</span>, (<span class="dv">2</span>, <span class="dv">50</span>))  <span class="co"># [batch_size, seq_len]</span></span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a>sample_patches <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">16</span>, <span class="dv">16</span><span class="op">*</span><span class="dv">16</span><span class="op">*</span><span class="dv">6</span>)  <span class="co"># [batch_size, num_patches, patch_dim]</span></span>
<span id="cb9-92"><a href="#cb9-92" aria-hidden="true" tabindex="-1"></a>sample_positions <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">2</span>, <span class="dv">16</span>, <span class="dv">2</span>))  <span class="co"># [batch_size, num_patches, 2]</span></span>
<span id="cb9-93"><a href="#cb9-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-94"><a href="#cb9-94" aria-hidden="true" tabindex="-1"></a>llm_output <span class="op">=</span> llm_model(sample_tokens)</span>
<span id="cb9-95"><a href="#cb9-95" aria-hidden="true" tabindex="-1"></a>gfm_output <span class="op">=</span> gfm_model(sample_patches, sample_positions)</span>
<span id="cb9-96"><a href="#cb9-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-97"><a href="#cb9-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">LLM output shape: </span><span class="sc">{</span>llm_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-98"><a href="#cb9-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GFM output shape: </span><span class="sc">{</span>gfm_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Architecture Comparison:
LLM parameters: 19,123,984
GFM parameters: 11,276,160

LLM output shape: torch.Size([2, 50, 10000])
GFM output shape: torch.Size([2, 16, 384])</code></pre>
</div>
</div>
</section>
<section id="pretraining-objectives" class="level4">
<h4 class="anchored" data-anchor-id="pretraining-objectives">Pretraining Objectives</h4>
<p>The pretraining objectives differ fundamentally between text and visual domains. LLMs excel at <strong>predictive modeling</strong> (predicting the next token), while GFMs focus on <strong>reconstructive modeling</strong> (rebuilding masked image patches).</p>
<p><strong>LLM Objectives:</strong></p>
<ul>
<li><strong>Next-Token Prediction</strong>: GPT-style autoregressive modeling for text generation</li>
<li><strong>Masked Language Modeling</strong>: BERT-style bidirectional understanding</li>
<li><strong>Instruction Following</strong>: Learning to follow human instructions (InstructGPT)</li>
</ul>
<p><strong>GFM Objectives:</strong></p>
<ul>
<li><strong>Masked Patch Reconstruction</strong>: MAE-style learning of visual representations</li>
<li><strong>Contrastive Learning</strong>: Learning invariances across time and space (SimCLR, CLIP)</li>
<li><strong>Multi-task Pretraining</strong>: Combining reconstruction with auxiliary tasks</li>
</ul>
<p><strong>Key References:</strong></p>
<ul>
<li><a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a> - MAE methodology</li>
<li><a href="https://arxiv.org/abs/2207.08051">SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery</a></li>
</ul>
<div id="0e315b4b" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LLM next-token prediction objective</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>sequence <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]])</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor([[<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]])  <span class="co"># Shifted by one position</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">5</span>, vocab_size)  <span class="co"># Model predictions</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>ce_loss <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>next_token_loss <span class="op">=</span> ce_loss(logits.view(<span class="op">-</span><span class="dv">1</span>, vocab_size), targets.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LLM Pretraining Objectives:"</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Next-token prediction loss: </span><span class="sc">{</span>next_token_loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">50</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co"># GFM masked patch reconstruction objective</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>batch_size, num_patches, patch_dim <span class="op">=</span> <span class="dv">2</span>, <span class="dv">64</span>, <span class="dv">768</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>original_patches <span class="op">=</span> torch.randn(batch_size, num_patches, patch_dim)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Random masking (75% typical for MAE)</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>mask_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>num_masked <span class="op">=</span> <span class="bu">int</span>(num_patches <span class="op">*</span> mask_ratio)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.zeros(batch_size, num_patches, dtype<span class="op">=</span>torch.<span class="bu">bool</span>)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    masked_indices <span class="op">=</span> torch.randperm(num_patches)[:num_masked]</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    mask[i, masked_indices] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Reconstruction loss on masked patches only</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>reconstructed_patches <span class="op">=</span> torch.randn_like(original_patches)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>reconstruction_loss <span class="op">=</span> nn.MSELoss()(</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    reconstructed_patches[mask], </span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    original_patches[mask]</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GFM Pretraining Objectives:"</span>)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mask ratio: </span><span class="sc">{</span>mask_ratio<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Masked patches per sample: </span><span class="sc">{</span>num_masked<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Reconstruction loss: </span><span class="sc">{</span>reconstruction_loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LLM Pretraining Objectives:
Next-token prediction loss: 6.7583

--------------------------------------------------

GFM Pretraining Objectives:
Mask ratio: 75.0%
Masked patches per sample: 48
Reconstruction loss: 2.0107</code></pre>
</div>
</div>
</section>
<section id="scaling-and-evolution" class="level4">
<h4 class="anchored" data-anchor-id="scaling-and-evolution">Scaling and Evolution</h4>
<p>The scaling trends between LLMs and GFMs reveal different optimization strategies. LLMs focus on <strong>parameter scaling</strong> (billions of parameters) while GFMs emphasize <strong>data modality scaling</strong> (spectral, spatial, and temporal dimensions).</p>
</section>
<section id="parameter-scaling-comparison" class="level4">
<h4 class="anchored" data-anchor-id="parameter-scaling-comparison">Parameter Scaling Comparison</h4>
<p><strong>LLM Scaling Milestones:</strong></p>
<ul>
<li><strong>GPT-1 (2018)</strong>: 117M parameters - Demonstrated unsupervised pretraining potential</li>
<li><strong>BERT-Base (2018)</strong>: 110M parameters - Bidirectional language understanding</li>
<li><strong>GPT-2 (2019)</strong>: 1.5B parameters - First signs of emergent capabilities</li>
<li><strong>GPT-3 (2020)</strong>: 175B parameters - Few-shot learning breakthrough</li>
<li><strong>PaLM (2022)</strong>: 540B parameters - Advanced reasoning capabilities</li>
<li><strong>GPT-4 (2023)</strong>: ~1T parameters - Multimodal understanding</li>
</ul>
<p><strong>GFM Scaling Examples:</strong></p>
<ul>
<li><strong>SatMAE-Base</strong>: 86M parameters - Satellite imagery foundation</li>
<li><strong>Prithvi-100M</strong>: 100M parameters - IBM/NASA Earth observation model</li>
<li><strong>Clay-v0.1</strong>: 139M parameters - Open-source geospatial foundation model</li>
<li><strong>Scale-MAE</strong>: 600M parameters - Largest published geospatial transformer</li>
</ul>
<p><strong>Context/Input Scaling Differences:</strong></p>
<p><strong>LLMs:</strong></p>
<ul>
<li>Context length: 512 → 2K → 8K → 128K+ tokens</li>
<li>Training data: Web text, books, code (curated datasets)</li>
<li>Focus: Language understanding and generation</li>
</ul>
<p><strong>GFMs:</strong></p>
<ul>
<li>Input bands: 3 (RGB) → 6+ (multispectral) → hyperspectral</li>
<li>Spatial resolution: Various (10m to 0.3m pixel sizes)</li>
<li>Temporal dimension: Single → time series → multi-temporal</li>
<li>Focus: Earth observation and environmental monitoring</li>
</ul>
<div id="b97bf52e" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize parameter scaling comparison</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>llm_milestones <span class="op">=</span> {</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'GPT-1'</span>: <span class="fl">117e6</span>,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'BERT-Base'</span>: <span class="fl">110e6</span>,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'GPT-2'</span>: <span class="fl">1.5e9</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'GPT-3'</span>: <span class="fl">175e9</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'PaLM'</span>: <span class="fl">540e9</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'GPT-4'</span>: <span class="fl">1000e9</span>  <span class="co"># Estimated</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>gfm_milestones <span class="op">=</span> {</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SatMAE-Base'</span>: <span class="fl">86e6</span>,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Prithvi-100M'</span>: <span class="fl">100e6</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Clay-v0.1'</span>: <span class="fl">139e6</span>,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SatLas-Base'</span>: <span class="fl">300e6</span>,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Scale-MAE'</span>: <span class="fl">600e6</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create visualization</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a><span class="co"># LLM scaling</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> <span class="bu">list</span>(llm_milestones.keys())</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> [llm_milestones[m]<span class="op">/</span><span class="fl">1e9</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>ax1.bar(models, params, color<span class="op">=</span><span class="st">'skyblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>ax1.set_yscale(<span class="st">'log'</span>)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Parameters (Billions)'</span>)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'LLM Parameter Scaling'</span>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>ax1.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co"># GFM scaling</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> <span class="bu">list</span>(gfm_milestones.keys())</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> [gfm_milestones[m]<span class="op">/</span><span class="fl">1e6</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>ax2.bar(models, params, color<span class="op">=</span><span class="st">'lightcoral'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Parameters (Millions)'</span>)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'GFM Parameter Scaling'</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>ax2.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="c00a-foundation_model_architectures_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="data-requirements-and-constraints" class="level3">
<h3 class="anchored" data-anchor-id="data-requirements-and-constraints">Data Requirements and Constraints</h3>
<p>The data infrastructure requirements for LLMs and GFMs differ dramatically due to the nature of text versus imagery data. Understanding these constraints is crucial for development planning.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Aspect</th>
<th>LLMs</th>
<th>GFMs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Data Volume</strong></td>
<td>Terabytes of text data (web crawls, books, code repositories)</td>
<td>Petabytes of satellite imagery (constrained by storage/IO bandwidth)</td>
</tr>
<tr class="even">
<td><strong>Data Quality Challenges</strong></td>
<td>Deduplication algorithms, toxicity filtering, language detection</td>
<td>Cloud masking, atmospheric correction, sensor calibration</td>
</tr>
<tr class="odd">
<td><strong>Preprocessing Requirements</strong></td>
<td>Tokenization, sequence packing, attention mask generation</td>
<td>Patch extraction, normalization, spatial/temporal alignment</td>
</tr>
<tr class="even">
<td><strong>Storage Format Optimization</strong></td>
<td>Compressed text files, pre-tokenized sequences</td>
<td>Cloud-optimized formats (<a href="https://www.cogeo.org/">COG</a>, <a href="https://zarr.readthedocs.io/">Zarr</a>), tiled storage</td>
</tr>
<tr class="odd">
<td><strong>Access Pattern Differences</strong></td>
<td>Sequential text processing, random document sampling</td>
<td>Spatial/temporal queries, patch-based sampling, geographic tiling</td>
</tr>
</tbody>
</table>
</section>
<section id="implementation-examples" class="level3">
<h3 class="anchored" data-anchor-id="implementation-examples">Implementation Examples</h3>
<section id="embedding-creation" class="level4">
<h4 class="anchored" data-anchor-id="embedding-creation">Embedding Creation</h4>
<p>Embeddings are the foundation of both LLMs and GFMs, but they differ in how raw inputs are converted to dense vector representations. This section demonstrates practical implementation patterns for both domains.</p>
<div id="d1eeacc9" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Text embedding creation (LLM approach)</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"The forest shows signs of deforestation."</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> text.lower().replace(<span class="st">'.'</span>, <span class="st">''</span>).split()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create simple vocabulary mapping</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> {word: i <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">set</span>(tokens))}</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>vocab[<span class="st">'&lt;PAD&gt;'</span>] <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>vocab[<span class="st">'&lt;UNK&gt;'</span>] <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert text to token IDs</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>token_ids <span class="op">=</span> [vocab.get(token, vocab[<span class="st">'&lt;UNK&gt;'</span>]) <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>token_tensor <span class="op">=</span> torch.tensor(token_ids).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Embedding layer lookup</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>embed_layer <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(vocab), <span class="dv">256</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>text_embeddings <span class="op">=</span> embed_layer(token_tensor)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Embeddings (LLM):"</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Token IDs: </span><span class="sc">{</span>token_ids<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Embeddings shape: </span><span class="sc">{</span>text_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">50</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch embedding creation (GFM approach)</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>num_bands <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sample multi-spectral satellite patch</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>satellite_patch <span class="op">=</span> torch.randn(<span class="dv">1</span>, num_bands, patch_size, patch_size)</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten patch for linear projection</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>patch_flat <span class="op">=</span> satellite_patch.view(<span class="dv">1</span>, num_bands <span class="op">*</span> patch_size <span class="op">*</span> patch_size)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear projection to embedding space</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>patch_projection <span class="op">=</span> nn.Linear(num_bands <span class="op">*</span> patch_size <span class="op">*</span> patch_size, <span class="dv">256</span>)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>patch_embeddings <span class="op">=</span> patch_projection(patch_flat)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Patch Embeddings (GFM):"</span>)</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original patch shape: </span><span class="sc">{</span>satellite_patch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Flattened patch shape: </span><span class="sc">{</span>patch_flat<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch embeddings shape: </span><span class="sc">{</span>patch_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Text Embeddings (LLM):
Original text: The forest shows signs of deforestation.
Tokens: ['the', 'forest', 'shows', 'signs', 'of', 'deforestation']
Token IDs: [5, 0, 2, 4, 3, 1]
Embeddings shape: torch.Size([1, 6, 256])

--------------------------------------------------

Patch Embeddings (GFM):
Original patch shape: torch.Size([1, 6, 16, 16])
Flattened patch shape: torch.Size([1, 1536])
Patch embeddings shape: torch.Size([1, 256])</code></pre>
</div>
</div>
</section>
<section id="positional-encoding-comparison" class="level4">
<h4 class="anchored" data-anchor-id="positional-encoding-comparison">Positional Encoding Comparison</h4>
<p>Positional encodings enable transformers to understand sequence order (LLMs) or spatial relationships (GFMs). The fundamental difference lies in 1D sequential positions versus 2D spatial coordinates.</p>
<p><strong>Key Differences:</strong></p>
<ul>
<li><strong>LLM</strong>: 1D sinusoidal encoding for sequence positions</li>
<li><strong>GFM</strong>: 2D spatial encoding combining height/width coordinates</li>
<li><strong>Learned vs Fixed</strong>: Both approaches can use learned or fixed encodings</li>
</ul>
<div id="adbb0354" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1D positional encoding for language models</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sinusoidal_positional_encoding(seq_len, embed_dim):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create sinusoidal positional encodings for sequence data"""</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> torch.zeros(seq_len, embed_dim)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    position <span class="op">=</span> torch.arange(<span class="dv">0</span>, seq_len).unsqueeze(<span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, embed_dim, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> </span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>                       <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> embed_dim))</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 2D positional encoding for geospatial models  </span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_2d_positional_encoding(height, width, embed_dim):</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create 2D positional encodings for spatial data"""</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> torch.zeros(height, width, embed_dim)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create position grids</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    y_pos <span class="op">=</span> torch.arange(height).unsqueeze(<span class="dv">1</span>).repeat(<span class="dv">1</span>, width).<span class="bu">float</span>()</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    x_pos <span class="op">=</span> torch.arange(width).unsqueeze(<span class="dv">0</span>).repeat(height, <span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encode Y positions in first half of embedding</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embed_dim <span class="op">//</span> <span class="dv">2</span>):</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>            pe[:, :, i] <span class="op">=</span> torch.sin(y_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (i <span class="op">/</span> embed_dim)))</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>            pe[:, :, i] <span class="op">=</span> torch.cos(y_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (i <span class="op">/</span> embed_dim)))</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encode X positions in second half of embedding</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embed_dim <span class="op">//</span> <span class="dv">2</span>, embed_dim):</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>        j <span class="op">=</span> i <span class="op">-</span> embed_dim <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> j <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>            pe[:, :, i] <span class="op">=</span> torch.sin(x_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (j <span class="op">/</span> embed_dim)))</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>            pe[:, :, i] <span class="op">=</span> torch.cos(x_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (j <span class="op">/</span> embed_dim)))</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate and visualize both encodings</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>seq_len, embed_dim <span class="op">=</span> <span class="dv">100</span>, <span class="dv">256</span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>pos_encoding_1d <span class="op">=</span> sinusoidal_positional_encoding(seq_len, embed_dim)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>height, width <span class="op">=</span> <span class="dv">8</span>, <span class="dv">8</span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>pos_encoding_2d <span class="op">=</span> create_2d_positional_encoding(height, width, embed_dim)</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"1D positional encoding shape: </span><span class="sc">{</span>pos_encoding_1d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"2D positional encoding shape: </span><span class="sc">{</span>pos_encoding_2d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize both encodings</span></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>plt.imshow(pos_encoding_1d[:<span class="dv">50</span>, :<span class="dv">50</span>].T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'1D Positional Encoding (LLM)'</span>)</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Sequence Position'</span>)</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Embedding Dimension'</span>)</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>pos_2d_viz <span class="op">=</span> pos_encoding_2d[:, :, :<span class="dv">64</span>].mean(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>plt.imshow(pos_2d_viz, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'equal'</span>)</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'2D Positional Encoding (GFM)'</span>)</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Width'</span>)</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Height'</span>)</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1D positional encoding shape: torch.Size([100, 256])
2D positional encoding shape: torch.Size([8, 8, 256])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="c00a-foundation_model_architectures_files/figure-html/cell-10-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


<!-- -->

</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kcaylor\.github\.io\/GEOG-288KC-geospatial-foundation-models");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb18" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Foundation Model Architectures"</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "LLMs vs. Geospatial Foundation Models (GFMs)"</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> geoai</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction to Foundation Model Architectures</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>Foundation models represent a paradigm shift in machine learning, where large-scale models are trained on diverse, unlabeled data and then adapted to various downstream tasks. The term was coined by the <span class="co">[</span><span class="ot">Stanford HAI Center</span><span class="co">](https://hai.stanford.edu/news/reflections-foundation-models)</span> to describe models like GPT-3, BERT, and CLIP that serve as a "foundation" for numerous applications.</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>This cheatsheet provides a comprehensive comparison between **Language Models (LLMs)** and **Geospatial Foundation Models (GFMs)**, examining their architectures, training procedures, and deployment strategies. While LLMs process discrete text tokens, GFMs work with continuous multi-spectral imagery, requiring fundamentally different approaches to tokenization, positional encoding, and objective functions.</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>**Key Resources:**</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Attention Is All You Need</span><span class="co">](https://arxiv.org/abs/1706.03762)</span> - Original Transformer paper</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">An Image is Worth 16x16 Words</span><span class="co">](https://arxiv.org/abs/2010.11929)</span> - Vision Transformer (ViT)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Masked Autoencoders Are Scalable Vision Learners</span><span class="co">](https://arxiv.org/abs/2111.06377)</span> - MAE approach</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Prithvi Foundation Model</span><span class="co">](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M)</span> - IBM/NASA geospatial model</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a><span class="fu">### Setup</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoConfig</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CUDA available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># Always False on Mac</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>mps_available <span class="op">=</span> torch.backends.mps.is_available()</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MPS available: </span><span class="sc">{</span>mps_available<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> mps_available:</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a><span class="fu">### Evolution from AI to Transformers</span></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>The development of foundation models represents a convergence of several key technological breakthroughs. The journey from symbolic AI to modern foundation models reflects both advances in computational power and fundamental insights about learning representations from large-scale data.</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Key Historical Milestones</span></span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>The transformer architecture emerged from decades of research in neural networks, attention mechanisms, and sequence modeling. The 2017 paper "<span class="co">[</span><span class="ot">Attention Is All You Need</span><span class="co">](https://arxiv.org/abs/1706.03762)</span>" introduced the transformer, which became the foundation for both language and vision models.</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>**Critical Developments:**</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**1950s-1990s**: Symbolic AI dominated, with rule-based systems and early neural networks</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2012**: <span class="co">[</span><span class="ot">AlexNet's ImageNet victory</span><span class="co">](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)</span> sparked the deep learning revolution</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2017**: The Transformer architecture introduced self-attention and parallelizable training</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2018-2020**: BERT and GPT families demonstrated the power of pre-trained language models</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2021-2024**: Scaling laws, <span class="co">[</span><span class="ot">ChatGPT</span><span class="co">](https://openai.com/blog/chatgpt)</span>, and multimodal models like <span class="co">[</span><span class="ot">CLIP</span><span class="co">](https://arxiv.org/abs/2103.00020)</span></span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>**Foundation Model Evolution Timeline:**</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2017 - Transformer**: Base architecture introduced</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2018 - BERT-Base**: 110M parameters</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2019 - GPT-2**: 1.5B parameters  </span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2020 - GPT-3**: 175B parameters</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2021 - ViT-Large**: 307M parameters for vision</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2022 - PaLM**: 540B parameters</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2023 - GPT-4**: ~1T parameters (estimated)</span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2024 - Claude-3**: Multi-modal capabilities</span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a><span class="fu">### Transformer Architecture Essentials</span></span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>The transformer architecture revolutionized deep learning by introducing the **self-attention mechanism**, which allows models to process sequences in parallel rather than sequentially. This architecture forms the backbone of both language models (GPT, BERT) and vision models (ViT, DINO).</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a>**Core Components:**</span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-Head Attention**: Allows the model to attend to different representation subspaces (see <span class="co">[</span><span class="ot">Self-attention</span><span class="co">](https://en.wikipedia.org/wiki/Self-attention)</span>)</span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feed-Forward Networks**: Point-wise processing with GELU activation (see <span class="co">[</span><span class="ot">Multilayer perceptron</span><span class="co">](https://en.wikipedia.org/wiki/Multilayer_perceptron)</span>)</span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Residual Connections**: Enable training of very deep networks (see <span class="co">[</span><span class="ot">Residual neural network</span><span class="co">](https://en.wikipedia.org/wiki/Residual_neural_network)</span>)</span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer Normalization**: Stabilizes training and improves convergence (see <span class="co">[</span><span class="ot">Layer normalization</span><span class="co">](https://en.wikipedia.org/wiki/Layer_normalization)</span>)</span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>**Key terms (quick definitions):**</span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Self-Attention (SA)**: A mechanism where each token attends to every other token (including itself) to compute a weighted combination of their representations.</span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Queries (Q), Keys (K), Values (V)**: Learned linear projections of the input. Attention scores are computed from Q·Kᵀ; those scores weight V to produce the output (see <span class="co">[</span><span class="ot">Attention (machine learning)</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Attention_(machine_learning))).</span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-Head Attention (MHA)**: Runs several SA “heads” in parallel on different subspaces, then concatenates and projects the results. This lets the model capture diverse relationships (see <span class="co">[</span><span class="ot">Self-attention</span><span class="co">](https://en.wikipedia.org/wiki/Self-attention)</span>).</span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feed-Forward Network (FFN/MLP)**: A small two-layer MLP (Multi-Layer Perceptron) applied to each token independently; expands the dimension then projects back (often with GELU activation) (see <span class="co">[</span><span class="ot">Multilayer perceptron</span><span class="co">](https://en.wikipedia.org/wiki/Multilayer_perceptron)</span>).</span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Residual Connection (Skip)**: Adds the input of a sub-layer to its output, helping gradients flow and preserving information (see <span class="co">[</span><span class="ot">Residual neural network</span><span class="co">](https://en.wikipedia.org/wiki/Residual_neural_network)</span>).</span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer Normalization (LayerNorm, LN)**: Normalizes features per token to stabilize training. The ordering with residuals defines Post-LN vs Pre-LN variants (see <span class="co">[</span><span class="ot">Layer normalization</span><span class="co">](https://en.wikipedia.org/wiki/Layer_normalization)</span>).</span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GELU**: Gaussian Error Linear Unit activation; smoother than ReLU and standard in transformers (see <span class="co">[</span><span class="ot">Gaussian error linear unit</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Activation_function#Gaussian_error_linear_unit_(GELU))).</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Tensor shape convention**: Using <span class="in">`batch_first=True`</span>, tensors are <span class="in">`[batch_size, seq_len, embed_dim]`</span>.</span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Abbreviations used**: <span class="in">`B`</span> = batch size, <span class="in">`S`</span> = sequence length (or number of tokens/patches), <span class="in">`E`</span> = embedding dimension, <span class="in">`H`</span> = number of attention heads.</span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a>The following implementation demonstrates a simplified transformer block following the <span class="co">[</span><span class="ot">original architecture</span><span class="co">](https://arxiv.org/abs/1706.03762)</span>:</span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleTransformerBlock(nn.Module):</span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Transformer block with Multi-Head Self-Attention (MHSA) and MLP.</span></span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs/Outputs:</span></span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a><span class="co">      - x: [batch_size, seq_len, embed_dim]</span></span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a><span class="co">      - returns: same shape as input</span></span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-111"><a href="#cb18-111" aria-hidden="true" tabindex="-1"></a><span class="co">    Structure (Post-LN variant):</span></span>
<span id="cb18-112"><a href="#cb18-112" aria-hidden="true" tabindex="-1"></a><span class="co">      x = LN(x + MHSA(x))</span></span>
<span id="cb18-113"><a href="#cb18-113" aria-hidden="true" tabindex="-1"></a><span class="co">      x = LN(x + MLP(x))</span></span>
<span id="cb18-114"><a href="#cb18-114" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-115"><a href="#cb18-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-116"><a href="#cb18-116" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">8</span>, mlp_ratio<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb18-117"><a href="#cb18-117" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-118"><a href="#cb18-118" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb18-119"><a href="#cb18-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-120"><a href="#cb18-120" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multi-Head Self-Attention (MHSA)</span></span>
<span id="cb18-121"><a href="#cb18-121" aria-hidden="true" tabindex="-1"></a>        <span class="co"># - batch_first=True → tensors are [B, S, E]</span></span>
<span id="cb18-122"><a href="#cb18-122" aria-hidden="true" tabindex="-1"></a>        <span class="co"># - Self-attention uses Q=K=V=linear(x) by default</span></span>
<span id="cb18-123"><a href="#cb18-123" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> nn.MultiheadAttention(</span>
<span id="cb18-124"><a href="#cb18-124" aria-hidden="true" tabindex="-1"></a>            embed_dim<span class="op">=</span>embed_dim,</span>
<span id="cb18-125"><a href="#cb18-125" aria-hidden="true" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads,</span>
<span id="cb18-126"><a href="#cb18-126" aria-hidden="true" tabindex="-1"></a>            batch_first<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-127"><a href="#cb18-127" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-128"><a href="#cb18-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-129"><a href="#cb18-129" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Position-wise feed-forward network (applied independently to each token)</span></span>
<span id="cb18-130"><a href="#cb18-130" aria-hidden="true" tabindex="-1"></a>        hidden_dim <span class="op">=</span> <span class="bu">int</span>(embed_dim <span class="op">*</span> mlp_ratio)</span>
<span id="cb18-131"><a href="#cb18-131" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> nn.Sequential(</span>
<span id="cb18-132"><a href="#cb18-132" aria-hidden="true" tabindex="-1"></a>            nn.Linear(embed_dim, hidden_dim),  <span class="co"># expand</span></span>
<span id="cb18-133"><a href="#cb18-133" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),                         <span class="co"># nonlinearity</span></span>
<span id="cb18-134"><a href="#cb18-134" aria-hidden="true" tabindex="-1"></a>            nn.Linear(hidden_dim, embed_dim),  <span class="co"># project back</span></span>
<span id="cb18-135"><a href="#cb18-135" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-136"><a href="#cb18-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-137"><a href="#cb18-137" aria-hidden="true" tabindex="-1"></a>        <span class="co"># LayerNorms used after residual additions (Post-LN)</span></span>
<span id="cb18-138"><a href="#cb18-138" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb18-139"><a href="#cb18-139" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb18-140"><a href="#cb18-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-141"><a href="#cb18-141" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb18-142"><a href="#cb18-142" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x: [B, S, E]</span></span>
<span id="cb18-143"><a href="#cb18-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-144"><a href="#cb18-144" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multi-head self-attention</span></span>
<span id="cb18-145"><a href="#cb18-145" aria-hidden="true" tabindex="-1"></a>        <span class="co"># attn_out: [B, S, E]; attn_weights (ignored here) would be [B, num_heads, S, S]</span></span>
<span id="cb18-146"><a href="#cb18-146" aria-hidden="true" tabindex="-1"></a>        attn_out, _ <span class="op">=</span> <span class="va">self</span>.attention(x, x, x)  <span class="co"># Q=K=V=x (self-attention)</span></span>
<span id="cb18-147"><a href="#cb18-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-148"><a href="#cb18-148" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Residual connection + LayerNorm (Post-LN)</span></span>
<span id="cb18-149"><a href="#cb18-149" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> attn_out)</span>
<span id="cb18-150"><a href="#cb18-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-151"><a href="#cb18-151" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Position-wise MLP</span></span>
<span id="cb18-152"><a href="#cb18-152" aria-hidden="true" tabindex="-1"></a>        mlp_out <span class="op">=</span> <span class="va">self</span>.mlp(x)  <span class="co"># [B, S, E]</span></span>
<span id="cb18-153"><a href="#cb18-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-154"><a href="#cb18-154" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Residual connection + LayerNorm (Post-LN)</span></span>
<span id="cb18-155"><a href="#cb18-155" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> mlp_out)</span>
<span id="cb18-156"><a href="#cb18-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-157"><a href="#cb18-157" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x  <span class="co"># shape preserved: [B, S, E]</span></span>
<span id="cb18-158"><a href="#cb18-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-159"><a href="#cb18-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-160"><a href="#cb18-160" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage and shape checks</span></span>
<span id="cb18-161"><a href="#cb18-161" aria-hidden="true" tabindex="-1"></a>transformer_block <span class="op">=</span> SimpleTransformerBlock(embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb18-162"><a href="#cb18-162" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">100</span>, <span class="dv">768</span>)  <span class="co"># [batch_size, seq_len, embed_dim]</span></span>
<span id="cb18-163"><a href="#cb18-163" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> transformer_block(sample_input)</span>
<span id="cb18-164"><a href="#cb18-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-165"><a href="#cb18-165" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape:  </span><span class="sc">{</span>sample_input<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-166"><a href="#cb18-166" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># should match input</span></span>
<span id="cb18-167"><a href="#cb18-167" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Parameters:  </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> transformer_block.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb18-168"><a href="#cb18-168" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-169"><a href="#cb18-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-170"><a href="#cb18-170" aria-hidden="true" tabindex="-1"></a>What to notice:</span>
<span id="cb18-171"><a href="#cb18-171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Shapes are stable through the block: <span class="in">`[B, S, E] → [B, S, E]`</span>.</span>
<span id="cb18-172"><a href="#cb18-172" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Residuals + LayerNorm appear after each sublayer (Post-LN variant).</span>
<span id="cb18-173"><a href="#cb18-173" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>MLP expands to <span class="in">`mlp_ratio × embed_dim`</span> then projects back.</span>
<span id="cb18-174"><a href="#cb18-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-175"><a href="#cb18-175" aria-hidden="true" tabindex="-1"></a><span class="fu">## LLMs vs GFMs</span></span>
<span id="cb18-176"><a href="#cb18-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-177"><a href="#cb18-177" aria-hidden="true" tabindex="-1"></a>Both LLMs and GFMs follow similar high-level development pipelines, but differ in the details because language and satellite imagery are very different kinds of data.</span>
<span id="cb18-178"><a href="#cb18-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-179"><a href="#cb18-179" aria-hidden="true" tabindex="-1"></a><span class="fu">### 9-Step Development Pipeline Comparison</span></span>
<span id="cb18-180"><a href="#cb18-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-181"><a href="#cb18-181" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Data Preparation**: Gather raw data and clean it up so the model can learn useful patterns.</span>
<span id="cb18-182"><a href="#cb18-182" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Tokenization (turning inputs into pieces the model can handle)**: Decide how to chop inputs into small parts the model can process.</span>
<span id="cb18-183"><a href="#cb18-183" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Architecture (the model blueprint)**: Choose how many layers, how wide/tall the model is, and how it connects information.</span>
<span id="cb18-184"><a href="#cb18-184" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Pretraining Objective (what the model practices)**: Pick the learning task the model does before any specific application.</span>
<span id="cb18-185"><a href="#cb18-185" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Training Loop (how learning happens)**: Decide optimizers, learning rate, precision, and how to stabilize training.</span>
<span id="cb18-186"><a href="#cb18-186" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Evaluation (how we check learning)**: Use simple tests to see if the model is improving in the right ways.</span>
<span id="cb18-187"><a href="#cb18-187" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**Pretrained Weights (starting point)**: Load existing model parameters to avoid training from scratch.</span>
<span id="cb18-188"><a href="#cb18-188" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>**Finetuning (adapting the model)**: Add a small head or nudge the model for a specific task with labeled examples.</span>
<span id="cb18-189"><a href="#cb18-189" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>**Deployment (using the model in practice)**: Serve the model efficiently and handle real-world input sizes.</span>
<span id="cb18-190"><a href="#cb18-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-191"><a href="#cb18-191" aria-hidden="true" tabindex="-1"></a><span class="fu">#### LLM Development Pipeline</span></span>
<span id="cb18-192"><a href="#cb18-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-193"><a href="#cb18-193" aria-hidden="true" tabindex="-1"></a>Language models like <span class="co">[</span><span class="ot">GPT</span><span class="co">](https://openai.com/research/gpt-4)</span> and <span class="co">[</span><span class="ot">BERT</span><span class="co">](https://arxiv.org/abs/1810.04805)</span> have established a mature development pipeline refined through years of research. The process emphasizes text quality, vocabulary optimization, and language understanding benchmarks.</span>
<span id="cb18-194"><a href="#cb18-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-195"><a href="#cb18-195" aria-hidden="true" tabindex="-1"></a>**Key References:**</span>
<span id="cb18-196"><a href="#cb18-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-197"><a href="#cb18-197" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Language Models are Few-Shot Learners</span><span class="co">](https://arxiv.org/abs/2005.14165)</span> - GPT-3 methodology</span>
<span id="cb18-198"><a href="#cb18-198" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Training language models to follow instructions</span><span class="co">](https://arxiv.org/abs/2203.02155)</span> - InstructGPT</span>
<span id="cb18-199"><a href="#cb18-199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">PaLM: Scaling Language Modeling</span><span class="co">](https://arxiv.org/abs/2204.02311)</span> - Large-scale training</span>
<span id="cb18-200"><a href="#cb18-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-201"><a href="#cb18-201" aria-hidden="true" tabindex="-1"></a><span class="fu">#### GFM Development Pipeline</span></span>
<span id="cb18-202"><a href="#cb18-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-203"><a href="#cb18-203" aria-hidden="true" tabindex="-1"></a>Geospatial foundation models adapt the transformer architecture for Earth observation data, requiring specialized handling of multi-spectral imagery, temporal sequences, and spatial relationships.</span>
<span id="cb18-204"><a href="#cb18-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-205"><a href="#cb18-205" aria-hidden="true" tabindex="-1"></a>**Key References:**</span>
<span id="cb18-206"><a href="#cb18-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-207"><a href="#cb18-207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Prithvi Foundation Model</span><span class="co">](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M)</span> - IBM/NASA collaboration</span>
<span id="cb18-208"><a href="#cb18-208" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery</span><span class="co">](https://arxiv.org/abs/2207.08051)</span></span>
<span id="cb18-209"><a href="#cb18-209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Clay Foundation Model</span><span class="co">](https://clay-foundation.github.io/model/)</span> - Open-source geospatial model</span>
<span id="cb18-210"><a href="#cb18-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-211"><a href="#cb18-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-212"><a href="#cb18-212" aria-hidden="true" tabindex="-1"></a>**Side-by-side (LLMs vs GFMs)**</span>
<span id="cb18-213"><a href="#cb18-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-214"><a href="#cb18-214" aria-hidden="true" tabindex="-1"></a>::: {.table-responsive}</span>
<span id="cb18-215"><a href="#cb18-215" aria-hidden="true" tabindex="-1"></a>| Step | LLMs (text) | GFMs (satellite imagery) |</span>
<span id="cb18-216"><a href="#cb18-216" aria-hidden="true" tabindex="-1"></a>|------|--------------|---------------------------|</span>
<span id="cb18-217"><a href="#cb18-217" aria-hidden="true" tabindex="-1"></a>| 1. Data Preparation | Collect large text sets, remove duplicates and low-quality content | Collect scenes from sensors, correct for atmosphere/sensor, remove clouds, tile into manageable chips |</span>
<span id="cb18-218"><a href="#cb18-218" aria-hidden="true" tabindex="-1"></a>| 2. Tokenization | Break text into subword tokens; build a vocabulary | Cut images into patches; turn each patch into a vector; add 2D (and time) positions |</span>
<span id="cb18-219"><a href="#cb18-219" aria-hidden="true" tabindex="-1"></a>| 3. Architecture | Transformer layers over token sequences (often decoder-only for GPT, encoder-only for BERT) | Vision Transformer-style encoders over patch sequences; may include temporal attention for time series |</span>
<span id="cb18-220"><a href="#cb18-220" aria-hidden="true" tabindex="-1"></a>| 4. Pretraining Objective | Predict the next/missing word to learn language patterns | Reconstruct masked image patches or learn similarities across views/time to learn visual patterns |</span>
<span id="cb18-221"><a href="#cb18-221" aria-hidden="true" tabindex="-1"></a>| 5. Training Loop | AdamW, learning-rate schedule, mixed precision; long sequences can stress memory | Similar tools, but memory shaped by patch size, bands, and image tiling; may mask clouds or invalid pixels |</span>
<span id="cb18-222"><a href="#cb18-222" aria-hidden="true" tabindex="-1"></a>| 6. Evaluation | Quick checks like “how surprised is the model?” (e.g., next-word loss) and small downstream tasks | Quick checks like reconstruction quality and small downstream tasks (e.g., linear probes on land cover) |</span>
<span id="cb18-223"><a href="#cb18-223" aria-hidden="true" tabindex="-1"></a>| 7. Pretrained Weights | Download weights and matching tokenizer from model hubs | Download weights from hubs (e.g., Prithvi, SatMAE); ensure band order and preprocessing match |</span>
<span id="cb18-224"><a href="#cb18-224" aria-hidden="true" tabindex="-1"></a>| 8. Finetuning | Add a small head or adapters; few labeled examples can go far | Add a task head (classification/segmentation); often freeze encoder and train a light head on small datasets |</span>
<span id="cb18-225"><a href="#cb18-225" aria-hidden="true" tabindex="-1"></a>| 9. Deployment | Serve via APIs; speed up with caching of past context | Run sliding-window/tiling over large scenes; export results as geospatial rasters/vectors |</span>
<span id="cb18-226"><a href="#cb18-226" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-227"><a href="#cb18-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-228"><a href="#cb18-228" aria-hidden="true" tabindex="-1"></a><span class="fu">### Step-by-Step Detailed Comparison</span></span>
<span id="cb18-229"><a href="#cb18-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-230"><a href="#cb18-230" aria-hidden="true" tabindex="-1"></a>Let's look at more detailed comparisons beetween each development pipeline step, highlighting the fundamental differences between text and geospatial data processing.</span>
<span id="cb18-231"><a href="#cb18-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-232"><a href="#cb18-232" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Data Preparation Differences</span></span>
<span id="cb18-233"><a href="#cb18-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-234"><a href="#cb18-234" aria-hidden="true" tabindex="-1"></a>Data preparation represents one of the most significant differences between LLMs and GFMs. LLMs work with human-generated text that requires quality filtering and deduplication, while GFMs process sensor data that requires physical corrections and calibration.</span>
<span id="cb18-235"><a href="#cb18-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-236"><a href="#cb18-236" aria-hidden="true" tabindex="-1"></a>**LLM Data Challenges:**</span>
<span id="cb18-237"><a href="#cb18-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-238"><a href="#cb18-238" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scale**: Training datasets like <span class="co">[</span><span class="ot">CommonCrawl</span><span class="co">](https://commoncrawl.org/)</span> contain hundreds of terabytes</span>
<span id="cb18-239"><a href="#cb18-239" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Quality**: Filtering toxic content, spam, and low-quality text</span>
<span id="cb18-240"><a href="#cb18-240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deduplication**: Removing exact and near-duplicate documents</span>
<span id="cb18-241"><a href="#cb18-241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Language Detection**: Identifying and filtering by language</span>
<span id="cb18-242"><a href="#cb18-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-243"><a href="#cb18-243" aria-hidden="true" tabindex="-1"></a>**GFM Data Challenges:**</span>
<span id="cb18-244"><a href="#cb18-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-245"><a href="#cb18-245" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sensor Calibration**: Converting raw digital numbers to physical units</span>
<span id="cb18-246"><a href="#cb18-246" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Atmospheric Correction**: Removing atmospheric effects from satellite imagery</span>
<span id="cb18-247"><a href="#cb18-247" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cloud Masking**: Identifying and handling cloudy pixels</span>
<span id="cb18-248"><a href="#cb18-248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Georegistration**: Aligning images to geographic coordinate systems</span>
<span id="cb18-251"><a href="#cb18-251" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-252"><a href="#cb18-252" aria-hidden="true" tabindex="-1"></a><span class="co"># LLM text preprocessing example</span></span>
<span id="cb18-253"><a href="#cb18-253" aria-hidden="true" tabindex="-1"></a>sample_texts <span class="op">=</span> [</span>
<span id="cb18-254"><a href="#cb18-254" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The quick brown fox jumps over the lazy dog."</span>,</span>
<span id="cb18-255"><a href="#cb18-255" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Machine learning is transforming many industries."</span>, </span>
<span id="cb18-256"><a href="#cb18-256" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Climate change requires urgent global action."</span></span>
<span id="cb18-257"><a href="#cb18-257" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb18-258"><a href="#cb18-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-259"><a href="#cb18-259" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic tokenization for vocabulary construction</span></span>
<span id="cb18-260"><a href="#cb18-260" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb18-261"><a href="#cb18-261" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> sample_texts:</span>
<span id="cb18-262"><a href="#cb18-262" aria-hidden="true" tabindex="-1"></a>    vocab.update(text.lower().replace(<span class="st">'.'</span>, <span class="st">''</span>).split())</span>
<span id="cb18-263"><a href="#cb18-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-264"><a href="#cb18-264" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LLM Data Processing:"</span>)</span>
<span id="cb18-265"><a href="#cb18-265" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample vocabulary size: </span><span class="sc">{</span><span class="bu">len</span>(vocab)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-266"><a href="#cb18-266" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sample tokens: </span><span class="sc">{</span><span class="bu">list</span>(vocab)[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-267"><a href="#cb18-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-268"><a href="#cb18-268" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb18-269"><a href="#cb18-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-270"><a href="#cb18-270" aria-hidden="true" tabindex="-1"></a><span class="co"># GFM satellite data preprocessing example</span></span>
<span id="cb18-271"><a href="#cb18-271" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb18-272"><a href="#cb18-272" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb18-273"><a href="#cb18-273" aria-hidden="true" tabindex="-1"></a>num_bands <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb18-274"><a href="#cb18-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-275"><a href="#cb18-275" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate raw satellite patch (typical 12-bit values)</span></span>
<span id="cb18-276"><a href="#cb18-276" aria-hidden="true" tabindex="-1"></a>satellite_patch <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">4096</span>, (num_bands, patch_size, patch_size))</span>
<span id="cb18-277"><a href="#cb18-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-278"><a href="#cb18-278" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate cloud mask (20% cloud coverage)</span></span>
<span id="cb18-279"><a href="#cb18-279" aria-hidden="true" tabindex="-1"></a>cloud_mask <span class="op">=</span> np.random.random((patch_size, patch_size)) <span class="op">&gt;</span> <span class="fl">0.8</span></span>
<span id="cb18-280"><a href="#cb18-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-281"><a href="#cb18-281" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply atmospheric correction (normalize to [0,1])</span></span>
<span id="cb18-282"><a href="#cb18-282" aria-hidden="true" tabindex="-1"></a>corrected_patch <span class="op">=</span> satellite_patch.astype(np.float32) <span class="op">/</span> <span class="fl">4095.0</span></span>
<span id="cb18-283"><a href="#cb18-283" aria-hidden="true" tabindex="-1"></a>corrected_patch[:, cloud_mask] <span class="op">=</span> np.nan  <span class="co"># Mask cloudy pixels</span></span>
<span id="cb18-284"><a href="#cb18-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-285"><a href="#cb18-285" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GFM Data Processing:"</span>)</span>
<span id="cb18-286"><a href="#cb18-286" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Satellite patch shape: </span><span class="sc">{</span>satellite_patch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> (bands, height, width)"</span>)</span>
<span id="cb18-287"><a href="#cb18-287" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cloud coverage: </span><span class="sc">{</span>cloud_mask<span class="sc">.</span><span class="bu">sum</span>() <span class="op">/</span> cloud_mask<span class="sc">.</span>size <span class="op">*</span> <span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%"</span>)</span>
<span id="cb18-288"><a href="#cb18-288" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Valid pixels per band: </span><span class="sc">{</span>(<span class="op">~</span>np.isnan(corrected_patch[<span class="dv">0</span>]))<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb18-289"><a href="#cb18-289" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-290"><a href="#cb18-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-291"><a href="#cb18-291" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Tokenization Approaches</span></span>
<span id="cb18-292"><a href="#cb18-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-293"><a href="#cb18-293" aria-hidden="true" tabindex="-1"></a>Tokenization represents a fundamental difference between language and vision models. LLMs use **discrete tokenization** with learned vocabularies (like [BPE](https://arxiv.org/abs/1508.07909)), while GFMs use **continuous tokenization** through patch embeddings inspired by <span class="co">[</span><span class="ot">Vision Transformers</span><span class="co">](https://arxiv.org/abs/2010.11929)</span>.</span>
<span id="cb18-294"><a href="#cb18-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-295"><a href="#cb18-295" aria-hidden="true" tabindex="-1"></a>**LLM Tokenization:**</span>
<span id="cb18-296"><a href="#cb18-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-297"><a href="#cb18-297" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Byte-Pair Encoding (BPE)**: Learns subword units to handle out-of-vocabulary words</span>
<span id="cb18-298"><a href="#cb18-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Vocabulary Size**: Typically 30K-100K tokens balancing efficiency and coverage</span>
<span id="cb18-299"><a href="#cb18-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Special Tokens**: <span class="in">`[CLS]`</span>, <span class="in">`[SEP]`</span>, <span class="in">`[PAD]`</span>, <span class="in">`[MASK]`</span> for different tasks</span>
<span id="cb18-300"><a href="#cb18-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-301"><a href="#cb18-301" aria-hidden="true" tabindex="-1"></a>**GFM Tokenization:**</span>
<span id="cb18-302"><a href="#cb18-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-303"><a href="#cb18-303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Patch Embedding**: Divides images into fixed-size patches (e.g., 16×16 pixels)</span>
<span id="cb18-304"><a href="#cb18-304" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Linear Projection**: Maps high-dimensional patches to embedding space</span>
<span id="cb18-305"><a href="#cb18-305" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Positional Encoding**: 2D spatial positions rather than 1D sequence positions</span>
<span id="cb18-306"><a href="#cb18-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-309"><a href="#cb18-309" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-310"><a href="#cb18-310" aria-hidden="true" tabindex="-1"></a><span class="co"># LLM discrete tokenization example</span></span>
<span id="cb18-311"><a href="#cb18-311" aria-hidden="true" tabindex="-1"></a>vocab_size, embed_dim <span class="op">=</span> <span class="dv">50000</span>, <span class="dv">768</span></span>
<span id="cb18-312"><a href="#cb18-312" aria-hidden="true" tabindex="-1"></a>token_ids <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">234</span>, <span class="dv">5678</span>, <span class="dv">2</span>])  <span class="co"># [CLS, word1, word2, word3, SEP]</span></span>
<span id="cb18-313"><a href="#cb18-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-314"><a href="#cb18-314" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb18-315"><a href="#cb18-315" aria-hidden="true" tabindex="-1"></a>token_embeddings <span class="op">=</span> embedding_layer(token_ids)</span>
<span id="cb18-316"><a href="#cb18-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-317"><a href="#cb18-317" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LLM Tokenization (Discrete):"</span>)</span>
<span id="cb18-318"><a href="#cb18-318" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Token IDs: </span><span class="sc">{</span>token_ids<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-319"><a href="#cb18-319" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Token embeddings shape: </span><span class="sc">{</span>token_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-320"><a href="#cb18-320" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Vocabulary size: </span><span class="sc">{</span>vocab_size<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb18-321"><a href="#cb18-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-322"><a href="#cb18-322" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">40</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb18-323"><a href="#cb18-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-324"><a href="#cb18-324" aria-hidden="true" tabindex="-1"></a><span class="co"># GFM continuous patch tokenization</span></span>
<span id="cb18-325"><a href="#cb18-325" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb18-326"><a href="#cb18-326" aria-hidden="true" tabindex="-1"></a>num_bands <span class="op">=</span> <span class="dv">6</span>  <span class="co"># Multi-spectral bands</span></span>
<span id="cb18-327"><a href="#cb18-327" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">768</span></span>
<span id="cb18-328"><a href="#cb18-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-329"><a href="#cb18-329" aria-hidden="true" tabindex="-1"></a>num_patches <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb18-330"><a href="#cb18-330" aria-hidden="true" tabindex="-1"></a>patch_dim <span class="op">=</span> patch_size <span class="op">*</span> patch_size <span class="op">*</span> num_bands</span>
<span id="cb18-331"><a href="#cb18-331" aria-hidden="true" tabindex="-1"></a>patches <span class="op">=</span> torch.randn(num_patches, patch_dim)</span>
<span id="cb18-332"><a href="#cb18-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-333"><a href="#cb18-333" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear projection for patch embedding</span></span>
<span id="cb18-334"><a href="#cb18-334" aria-hidden="true" tabindex="-1"></a>patch_projection <span class="op">=</span> nn.Linear(patch_dim, embed_dim)</span>
<span id="cb18-335"><a href="#cb18-335" aria-hidden="true" tabindex="-1"></a>patch_embeddings <span class="op">=</span> patch_projection(patches)</span>
<span id="cb18-336"><a href="#cb18-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-337"><a href="#cb18-337" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GFM Tokenization (Continuous Patches):"</span>)</span>
<span id="cb18-338"><a href="#cb18-338" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch dimensions: </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">×</span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">×</span><span class="sc">{</span>num_bands<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>patch_dim<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-339"><a href="#cb18-339" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch embeddings shape: </span><span class="sc">{</span>patch_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-340"><a href="#cb18-340" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"No discrete vocabulary - continuous projection"</span>)</span>
<span id="cb18-341"><a href="#cb18-341" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-342"><a href="#cb18-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-343"><a href="#cb18-343" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Architecture Comparison</span></span>
<span id="cb18-344"><a href="#cb18-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-345"><a href="#cb18-345" aria-hidden="true" tabindex="-1"></a>While both LLMs and GFMs use transformer architectures, they differ in input processing, positional encoding, and output heads. LLMs like <span class="co">[</span><span class="ot">GPT</span><span class="co">](https://openai.com/research/gpt-4)</span> use causal attention for autoregressive generation, while GFMs like <span class="co">[</span><span class="ot">Prithvi</span><span class="co">](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M)</span> use bidirectional attention for representation learning.</span>
<span id="cb18-346"><a href="#cb18-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-347"><a href="#cb18-347" aria-hidden="true" tabindex="-1"></a>**Key Architectural Differences:**</span>
<span id="cb18-348"><a href="#cb18-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-349"><a href="#cb18-349" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input Processing**: 1D token sequences vs. 2D spatial patches</span>
<span id="cb18-350"><a href="#cb18-350" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Positional Encoding**: 1D learned positions vs. 2D spatial coordinates</span>
<span id="cb18-351"><a href="#cb18-351" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Attention Patterns**: Causal masking vs. full bidirectional attention</span>
<span id="cb18-352"><a href="#cb18-352" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output Heads**: Language modeling head vs. reconstruction/classification heads</span>
<span id="cb18-355"><a href="#cb18-355" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-356"><a href="#cb18-356" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LLMArchitecture(nn.Module):</span>
<span id="cb18-357"><a href="#cb18-357" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simplified LLM architecture (GPT-style)"""</span></span>
<span id="cb18-358"><a href="#cb18-358" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-359"><a href="#cb18-359" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size<span class="op">=</span><span class="dv">50000</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_layers<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb18-360"><a href="#cb18-360" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-361"><a href="#cb18-361" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb18-362"><a href="#cb18-362" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_encoding <span class="op">=</span> nn.Embedding(<span class="dv">2048</span>, embed_dim)  <span class="co"># Max sequence length</span></span>
<span id="cb18-363"><a href="#cb18-363" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-364"><a href="#cb18-364" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb18-365"><a href="#cb18-365" aria-hidden="true" tabindex="-1"></a>            SimpleTransformerBlock(embed_dim, num_heads) </span>
<span id="cb18-366"><a href="#cb18-366" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb18-367"><a href="#cb18-367" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb18-368"><a href="#cb18-368" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-369"><a href="#cb18-369" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_final <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb18-370"><a href="#cb18-370" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_head <span class="op">=</span> nn.Linear(embed_dim, vocab_size)</span>
<span id="cb18-371"><a href="#cb18-371" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-372"><a href="#cb18-372" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids):</span>
<span id="cb18-373"><a href="#cb18-373" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> input_ids.shape[<span class="dv">1</span>]</span>
<span id="cb18-374"><a href="#cb18-374" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> torch.arange(seq_len, device<span class="op">=</span>input_ids.device)</span>
<span id="cb18-375"><a href="#cb18-375" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-376"><a href="#cb18-376" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Token + positional embeddings</span></span>
<span id="cb18-377"><a href="#cb18-377" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(input_ids) <span class="op">+</span> <span class="va">self</span>.positional_encoding(positions)</span>
<span id="cb18-378"><a href="#cb18-378" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-379"><a href="#cb18-379" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transformer layers</span></span>
<span id="cb18-380"><a href="#cb18-380" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb18-381"><a href="#cb18-381" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb18-382"><a href="#cb18-382" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-383"><a href="#cb18-383" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_final(x)</span>
<span id="cb18-384"><a href="#cb18-384" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.output_head(x)</span>
<span id="cb18-385"><a href="#cb18-385" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-386"><a href="#cb18-386" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb18-387"><a href="#cb18-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-388"><a href="#cb18-388" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GFMArchitecture(nn.Module):</span>
<span id="cb18-389"><a href="#cb18-389" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simplified GFM architecture (ViT-style)"""</span></span>
<span id="cb18-390"><a href="#cb18-390" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-391"><a href="#cb18-391" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, patch_size<span class="op">=</span><span class="dv">16</span>, num_bands<span class="op">=</span><span class="dv">6</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_layers<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb18-392"><a href="#cb18-392" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-393"><a href="#cb18-393" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb18-394"><a href="#cb18-394" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_bands <span class="op">=</span> num_bands</span>
<span id="cb18-395"><a href="#cb18-395" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-396"><a href="#cb18-396" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb18-397"><a href="#cb18-397" aria-hidden="true" tabindex="-1"></a>        patch_dim <span class="op">=</span> patch_size <span class="op">*</span> patch_size <span class="op">*</span> num_bands</span>
<span id="cb18-398"><a href="#cb18-398" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embedding <span class="op">=</span> nn.Linear(patch_dim, embed_dim)</span>
<span id="cb18-399"><a href="#cb18-399" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-400"><a href="#cb18-400" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2D positional embedding</span></span>
<span id="cb18-401"><a href="#cb18-401" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed_h <span class="op">=</span> nn.Embedding(<span class="dv">100</span>, embed_dim <span class="op">//</span> <span class="dv">2</span>)  <span class="co"># Height positions</span></span>
<span id="cb18-402"><a href="#cb18-402" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed_w <span class="op">=</span> nn.Embedding(<span class="dv">100</span>, embed_dim <span class="op">//</span> <span class="dv">2</span>)  <span class="co"># Width positions</span></span>
<span id="cb18-403"><a href="#cb18-403" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-404"><a href="#cb18-404" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb18-405"><a href="#cb18-405" aria-hidden="true" tabindex="-1"></a>            SimpleTransformerBlock(embed_dim, num_heads) </span>
<span id="cb18-406"><a href="#cb18-406" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb18-407"><a href="#cb18-407" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb18-408"><a href="#cb18-408" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-409"><a href="#cb18-409" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_final <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb18-410"><a href="#cb18-410" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-411"><a href="#cb18-411" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, patches, patch_positions):</span>
<span id="cb18-412"><a href="#cb18-412" aria-hidden="true" tabindex="-1"></a>        batch_size, num_patches, patch_dim <span class="op">=</span> patches.shape</span>
<span id="cb18-413"><a href="#cb18-413" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-414"><a href="#cb18-414" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch embeddings</span></span>
<span id="cb18-415"><a href="#cb18-415" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embedding(patches)</span>
<span id="cb18-416"><a href="#cb18-416" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-417"><a href="#cb18-417" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2D positional embeddings</span></span>
<span id="cb18-418"><a href="#cb18-418" aria-hidden="true" tabindex="-1"></a>        pos_h, pos_w <span class="op">=</span> patch_positions[:, :, <span class="dv">0</span>], patch_positions[:, :, <span class="dv">1</span>]</span>
<span id="cb18-419"><a href="#cb18-419" aria-hidden="true" tabindex="-1"></a>        pos_emb <span class="op">=</span> torch.cat([</span>
<span id="cb18-420"><a href="#cb18-420" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pos_embed_h(pos_h),</span>
<span id="cb18-421"><a href="#cb18-421" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pos_embed_w(pos_w)</span>
<span id="cb18-422"><a href="#cb18-422" aria-hidden="true" tabindex="-1"></a>        ], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb18-423"><a href="#cb18-423" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-424"><a href="#cb18-424" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> pos_emb</span>
<span id="cb18-425"><a href="#cb18-425" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-426"><a href="#cb18-426" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transformer layers</span></span>
<span id="cb18-427"><a href="#cb18-427" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb18-428"><a href="#cb18-428" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb18-429"><a href="#cb18-429" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-430"><a href="#cb18-430" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_final(x)</span>
<span id="cb18-431"><a href="#cb18-431" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb18-432"><a href="#cb18-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-433"><a href="#cb18-433" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare architectures</span></span>
<span id="cb18-434"><a href="#cb18-434" aria-hidden="true" tabindex="-1"></a>llm_model <span class="op">=</span> LLMArchitecture(vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">384</span>, num_layers<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb18-435"><a href="#cb18-435" aria-hidden="true" tabindex="-1"></a>gfm_model <span class="op">=</span> GFMArchitecture(patch_size<span class="op">=</span><span class="dv">16</span>, num_bands<span class="op">=</span><span class="dv">6</span>, embed_dim<span class="op">=</span><span class="dv">384</span>, num_layers<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb18-436"><a href="#cb18-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-437"><a href="#cb18-437" aria-hidden="true" tabindex="-1"></a>llm_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> llm_model.parameters())</span>
<span id="cb18-438"><a href="#cb18-438" aria-hidden="true" tabindex="-1"></a>gfm_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> gfm_model.parameters())</span>
<span id="cb18-439"><a href="#cb18-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-440"><a href="#cb18-440" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Architecture Comparison:"</span>)</span>
<span id="cb18-441"><a href="#cb18-441" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LLM parameters: </span><span class="sc">{</span>llm_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb18-442"><a href="#cb18-442" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GFM parameters: </span><span class="sc">{</span>gfm_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb18-443"><a href="#cb18-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-444"><a href="#cb18-444" aria-hidden="true" tabindex="-1"></a><span class="co"># Test forward passes</span></span>
<span id="cb18-445"><a href="#cb18-445" aria-hidden="true" tabindex="-1"></a>sample_tokens <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10000</span>, (<span class="dv">2</span>, <span class="dv">50</span>))  <span class="co"># [batch_size, seq_len]</span></span>
<span id="cb18-446"><a href="#cb18-446" aria-hidden="true" tabindex="-1"></a>sample_patches <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">16</span>, <span class="dv">16</span><span class="op">*</span><span class="dv">16</span><span class="op">*</span><span class="dv">6</span>)  <span class="co"># [batch_size, num_patches, patch_dim]</span></span>
<span id="cb18-447"><a href="#cb18-447" aria-hidden="true" tabindex="-1"></a>sample_positions <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">2</span>, <span class="dv">16</span>, <span class="dv">2</span>))  <span class="co"># [batch_size, num_patches, 2]</span></span>
<span id="cb18-448"><a href="#cb18-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-449"><a href="#cb18-449" aria-hidden="true" tabindex="-1"></a>llm_output <span class="op">=</span> llm_model(sample_tokens)</span>
<span id="cb18-450"><a href="#cb18-450" aria-hidden="true" tabindex="-1"></a>gfm_output <span class="op">=</span> gfm_model(sample_patches, sample_positions)</span>
<span id="cb18-451"><a href="#cb18-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-452"><a href="#cb18-452" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">LLM output shape: </span><span class="sc">{</span>llm_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-453"><a href="#cb18-453" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GFM output shape: </span><span class="sc">{</span>gfm_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-454"><a href="#cb18-454" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-455"><a href="#cb18-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-456"><a href="#cb18-456" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pretraining Objectives</span></span>
<span id="cb18-457"><a href="#cb18-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-458"><a href="#cb18-458" aria-hidden="true" tabindex="-1"></a>The pretraining objectives differ fundamentally between text and visual domains. LLMs excel at **predictive modeling** (predicting the next token), while GFMs focus on **reconstructive modeling** (rebuilding masked image patches).</span>
<span id="cb18-459"><a href="#cb18-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-460"><a href="#cb18-460" aria-hidden="true" tabindex="-1"></a>**LLM Objectives:**</span>
<span id="cb18-461"><a href="#cb18-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-462"><a href="#cb18-462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Next-Token Prediction**: GPT-style autoregressive modeling for text generation</span>
<span id="cb18-463"><a href="#cb18-463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Masked Language Modeling**: BERT-style bidirectional understanding</span>
<span id="cb18-464"><a href="#cb18-464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Instruction Following**: Learning to follow human instructions (InstructGPT)</span>
<span id="cb18-465"><a href="#cb18-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-466"><a href="#cb18-466" aria-hidden="true" tabindex="-1"></a>**GFM Objectives:**</span>
<span id="cb18-467"><a href="#cb18-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-468"><a href="#cb18-468" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Masked Patch Reconstruction**: MAE-style learning of visual representations</span>
<span id="cb18-469"><a href="#cb18-469" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Contrastive Learning**: Learning invariances across time and space (SimCLR, CLIP)</span>
<span id="cb18-470"><a href="#cb18-470" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-task Pretraining**: Combining reconstruction with auxiliary tasks</span>
<span id="cb18-471"><a href="#cb18-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-472"><a href="#cb18-472" aria-hidden="true" tabindex="-1"></a>**Key References:**</span>
<span id="cb18-473"><a href="#cb18-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-474"><a href="#cb18-474" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Masked Autoencoders Are Scalable Vision Learners</span><span class="co">](https://arxiv.org/abs/2111.06377)</span> - MAE methodology</span>
<span id="cb18-475"><a href="#cb18-475" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery</span><span class="co">](https://arxiv.org/abs/2207.08051)</span></span>
<span id="cb18-476"><a href="#cb18-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-479"><a href="#cb18-479" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-480"><a href="#cb18-480" aria-hidden="true" tabindex="-1"></a><span class="co"># LLM next-token prediction objective</span></span>
<span id="cb18-481"><a href="#cb18-481" aria-hidden="true" tabindex="-1"></a>sequence <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]])</span>
<span id="cb18-482"><a href="#cb18-482" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> torch.tensor([[<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]])  <span class="co"># Shifted by one position</span></span>
<span id="cb18-483"><a href="#cb18-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-484"><a href="#cb18-484" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb18-485"><a href="#cb18-485" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">5</span>, vocab_size)  <span class="co"># Model predictions</span></span>
<span id="cb18-486"><a href="#cb18-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-487"><a href="#cb18-487" aria-hidden="true" tabindex="-1"></a>ce_loss <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb18-488"><a href="#cb18-488" aria-hidden="true" tabindex="-1"></a>next_token_loss <span class="op">=</span> ce_loss(logits.view(<span class="op">-</span><span class="dv">1</span>, vocab_size), targets.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb18-489"><a href="#cb18-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-490"><a href="#cb18-490" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LLM Pretraining Objectives:"</span>)</span>
<span id="cb18-491"><a href="#cb18-491" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Next-token prediction loss: </span><span class="sc">{</span>next_token_loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-492"><a href="#cb18-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-493"><a href="#cb18-493" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">50</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb18-494"><a href="#cb18-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-495"><a href="#cb18-495" aria-hidden="true" tabindex="-1"></a><span class="co"># GFM masked patch reconstruction objective</span></span>
<span id="cb18-496"><a href="#cb18-496" aria-hidden="true" tabindex="-1"></a>batch_size, num_patches, patch_dim <span class="op">=</span> <span class="dv">2</span>, <span class="dv">64</span>, <span class="dv">768</span></span>
<span id="cb18-497"><a href="#cb18-497" aria-hidden="true" tabindex="-1"></a>original_patches <span class="op">=</span> torch.randn(batch_size, num_patches, patch_dim)</span>
<span id="cb18-498"><a href="#cb18-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-499"><a href="#cb18-499" aria-hidden="true" tabindex="-1"></a><span class="co"># Random masking (75% typical for MAE)</span></span>
<span id="cb18-500"><a href="#cb18-500" aria-hidden="true" tabindex="-1"></a>mask_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb18-501"><a href="#cb18-501" aria-hidden="true" tabindex="-1"></a>num_masked <span class="op">=</span> <span class="bu">int</span>(num_patches <span class="op">*</span> mask_ratio)</span>
<span id="cb18-502"><a href="#cb18-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-503"><a href="#cb18-503" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.zeros(batch_size, num_patches, dtype<span class="op">=</span>torch.<span class="bu">bool</span>)</span>
<span id="cb18-504"><a href="#cb18-504" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb18-505"><a href="#cb18-505" aria-hidden="true" tabindex="-1"></a>    masked_indices <span class="op">=</span> torch.randperm(num_patches)[:num_masked]</span>
<span id="cb18-506"><a href="#cb18-506" aria-hidden="true" tabindex="-1"></a>    mask[i, masked_indices] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb18-507"><a href="#cb18-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-508"><a href="#cb18-508" aria-hidden="true" tabindex="-1"></a><span class="co"># Reconstruction loss on masked patches only</span></span>
<span id="cb18-509"><a href="#cb18-509" aria-hidden="true" tabindex="-1"></a>reconstructed_patches <span class="op">=</span> torch.randn_like(original_patches)</span>
<span id="cb18-510"><a href="#cb18-510" aria-hidden="true" tabindex="-1"></a>reconstruction_loss <span class="op">=</span> nn.MSELoss()(</span>
<span id="cb18-511"><a href="#cb18-511" aria-hidden="true" tabindex="-1"></a>    reconstructed_patches[mask], </span>
<span id="cb18-512"><a href="#cb18-512" aria-hidden="true" tabindex="-1"></a>    original_patches[mask]</span>
<span id="cb18-513"><a href="#cb18-513" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-514"><a href="#cb18-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-515"><a href="#cb18-515" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"GFM Pretraining Objectives:"</span>)</span>
<span id="cb18-516"><a href="#cb18-516" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mask ratio: </span><span class="sc">{</span>mask_ratio<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb18-517"><a href="#cb18-517" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Masked patches per sample: </span><span class="sc">{</span>num_masked<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-518"><a href="#cb18-518" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Reconstruction loss: </span><span class="sc">{</span>reconstruction_loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb18-519"><a href="#cb18-519" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-520"><a href="#cb18-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-521"><a href="#cb18-521" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Scaling and Evolution</span></span>
<span id="cb18-522"><a href="#cb18-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-523"><a href="#cb18-523" aria-hidden="true" tabindex="-1"></a>The scaling trends between LLMs and GFMs reveal different optimization strategies. LLMs focus on **parameter scaling** (billions of parameters) while GFMs emphasize **data modality scaling** (spectral, spatial, and temporal dimensions).</span>
<span id="cb18-524"><a href="#cb18-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-525"><a href="#cb18-525" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Parameter Scaling Comparison</span></span>
<span id="cb18-526"><a href="#cb18-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-527"><a href="#cb18-527" aria-hidden="true" tabindex="-1"></a>**LLM Scaling Milestones:**</span>
<span id="cb18-528"><a href="#cb18-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-529"><a href="#cb18-529" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GPT-1 (2018)**: 117M parameters - Demonstrated unsupervised pretraining potential</span>
<span id="cb18-530"><a href="#cb18-530" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**BERT-Base (2018)**: 110M parameters - Bidirectional language understanding</span>
<span id="cb18-531"><a href="#cb18-531" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GPT-2 (2019)**: 1.5B parameters - First signs of emergent capabilities</span>
<span id="cb18-532"><a href="#cb18-532" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GPT-3 (2020)**: 175B parameters - Few-shot learning breakthrough</span>
<span id="cb18-533"><a href="#cb18-533" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PaLM (2022)**: 540B parameters - Advanced reasoning capabilities</span>
<span id="cb18-534"><a href="#cb18-534" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GPT-4 (2023)**: ~1T parameters - Multimodal understanding</span>
<span id="cb18-535"><a href="#cb18-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-536"><a href="#cb18-536" aria-hidden="true" tabindex="-1"></a>**GFM Scaling Examples:**</span>
<span id="cb18-537"><a href="#cb18-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-538"><a href="#cb18-538" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SatMAE-Base**: 86M parameters - Satellite imagery foundation</span>
<span id="cb18-539"><a href="#cb18-539" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Prithvi-100M**: 100M parameters - IBM/NASA Earth observation model</span>
<span id="cb18-540"><a href="#cb18-540" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Clay-v0.1**: 139M parameters - Open-source geospatial foundation model</span>
<span id="cb18-541"><a href="#cb18-541" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scale-MAE**: 600M parameters - Largest published geospatial transformer</span>
<span id="cb18-542"><a href="#cb18-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-543"><a href="#cb18-543" aria-hidden="true" tabindex="-1"></a>**Context/Input Scaling Differences:**</span>
<span id="cb18-544"><a href="#cb18-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-545"><a href="#cb18-545" aria-hidden="true" tabindex="-1"></a>**LLMs:**</span>
<span id="cb18-546"><a href="#cb18-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-547"><a href="#cb18-547" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Context length: 512 → 2K → 8K → 128K+ tokens</span>
<span id="cb18-548"><a href="#cb18-548" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Training data: Web text, books, code (curated datasets)</span>
<span id="cb18-549"><a href="#cb18-549" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focus: Language understanding and generation</span>
<span id="cb18-550"><a href="#cb18-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-551"><a href="#cb18-551" aria-hidden="true" tabindex="-1"></a>**GFMs:**</span>
<span id="cb18-552"><a href="#cb18-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-553"><a href="#cb18-553" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input bands: 3 (RGB) → 6+ (multispectral) → hyperspectral</span>
<span id="cb18-554"><a href="#cb18-554" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Spatial resolution: Various (10m to 0.3m pixel sizes)</span>
<span id="cb18-555"><a href="#cb18-555" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Temporal dimension: Single → time series → multi-temporal</span>
<span id="cb18-556"><a href="#cb18-556" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focus: Earth observation and environmental monitoring</span>
<span id="cb18-557"><a href="#cb18-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-560"><a href="#cb18-560" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-561"><a href="#cb18-561" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize parameter scaling comparison</span></span>
<span id="cb18-562"><a href="#cb18-562" aria-hidden="true" tabindex="-1"></a>llm_milestones <span class="op">=</span> {</span>
<span id="cb18-563"><a href="#cb18-563" aria-hidden="true" tabindex="-1"></a>    <span class="st">'GPT-1'</span>: <span class="fl">117e6</span>,</span>
<span id="cb18-564"><a href="#cb18-564" aria-hidden="true" tabindex="-1"></a>    <span class="st">'BERT-Base'</span>: <span class="fl">110e6</span>,</span>
<span id="cb18-565"><a href="#cb18-565" aria-hidden="true" tabindex="-1"></a>    <span class="st">'GPT-2'</span>: <span class="fl">1.5e9</span>,</span>
<span id="cb18-566"><a href="#cb18-566" aria-hidden="true" tabindex="-1"></a>    <span class="st">'GPT-3'</span>: <span class="fl">175e9</span>,</span>
<span id="cb18-567"><a href="#cb18-567" aria-hidden="true" tabindex="-1"></a>    <span class="st">'PaLM'</span>: <span class="fl">540e9</span>,</span>
<span id="cb18-568"><a href="#cb18-568" aria-hidden="true" tabindex="-1"></a>    <span class="st">'GPT-4'</span>: <span class="fl">1000e9</span>  <span class="co"># Estimated</span></span>
<span id="cb18-569"><a href="#cb18-569" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-570"><a href="#cb18-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-571"><a href="#cb18-571" aria-hidden="true" tabindex="-1"></a>gfm_milestones <span class="op">=</span> {</span>
<span id="cb18-572"><a href="#cb18-572" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SatMAE-Base'</span>: <span class="fl">86e6</span>,</span>
<span id="cb18-573"><a href="#cb18-573" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Prithvi-100M'</span>: <span class="fl">100e6</span>,</span>
<span id="cb18-574"><a href="#cb18-574" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Clay-v0.1'</span>: <span class="fl">139e6</span>,</span>
<span id="cb18-575"><a href="#cb18-575" aria-hidden="true" tabindex="-1"></a>    <span class="st">'SatLas-Base'</span>: <span class="fl">300e6</span>,</span>
<span id="cb18-576"><a href="#cb18-576" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Scale-MAE'</span>: <span class="fl">600e6</span></span>
<span id="cb18-577"><a href="#cb18-577" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-578"><a href="#cb18-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-579"><a href="#cb18-579" aria-hidden="true" tabindex="-1"></a><span class="co"># Create visualization</span></span>
<span id="cb18-580"><a href="#cb18-580" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb18-581"><a href="#cb18-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-582"><a href="#cb18-582" aria-hidden="true" tabindex="-1"></a><span class="co"># LLM scaling</span></span>
<span id="cb18-583"><a href="#cb18-583" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> <span class="bu">list</span>(llm_milestones.keys())</span>
<span id="cb18-584"><a href="#cb18-584" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> [llm_milestones[m]<span class="op">/</span><span class="fl">1e9</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb18-585"><a href="#cb18-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-586"><a href="#cb18-586" aria-hidden="true" tabindex="-1"></a>ax1.bar(models, params, color<span class="op">=</span><span class="st">'skyblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb18-587"><a href="#cb18-587" aria-hidden="true" tabindex="-1"></a>ax1.set_yscale(<span class="st">'log'</span>)</span>
<span id="cb18-588"><a href="#cb18-588" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Parameters (Billions)'</span>)</span>
<span id="cb18-589"><a href="#cb18-589" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'LLM Parameter Scaling'</span>)</span>
<span id="cb18-590"><a href="#cb18-590" aria-hidden="true" tabindex="-1"></a>ax1.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb18-591"><a href="#cb18-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-592"><a href="#cb18-592" aria-hidden="true" tabindex="-1"></a><span class="co"># GFM scaling</span></span>
<span id="cb18-593"><a href="#cb18-593" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> <span class="bu">list</span>(gfm_milestones.keys())</span>
<span id="cb18-594"><a href="#cb18-594" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> [gfm_milestones[m]<span class="op">/</span><span class="fl">1e6</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb18-595"><a href="#cb18-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-596"><a href="#cb18-596" aria-hidden="true" tabindex="-1"></a>ax2.bar(models, params, color<span class="op">=</span><span class="st">'lightcoral'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb18-597"><a href="#cb18-597" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Parameters (Millions)'</span>)</span>
<span id="cb18-598"><a href="#cb18-598" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'GFM Parameter Scaling'</span>)</span>
<span id="cb18-599"><a href="#cb18-599" aria-hidden="true" tabindex="-1"></a>ax2.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb18-600"><a href="#cb18-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-601"><a href="#cb18-601" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-602"><a href="#cb18-602" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-603"><a href="#cb18-603" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-604"><a href="#cb18-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-605"><a href="#cb18-605" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data Requirements and Constraints</span></span>
<span id="cb18-606"><a href="#cb18-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-607"><a href="#cb18-607" aria-hidden="true" tabindex="-1"></a>The data infrastructure requirements for LLMs and GFMs differ dramatically due to the nature of text versus imagery data. Understanding these constraints is crucial for development planning.</span>
<span id="cb18-608"><a href="#cb18-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-609"><a href="#cb18-609" aria-hidden="true" tabindex="-1"></a>| Aspect | LLMs | GFMs |</span>
<span id="cb18-610"><a href="#cb18-610" aria-hidden="true" tabindex="-1"></a>|--------|------|------|</span>
<span id="cb18-611"><a href="#cb18-611" aria-hidden="true" tabindex="-1"></a>| **Data Volume** | Terabytes of text data (web crawls, books, code repositories) | Petabytes of satellite imagery (constrained by storage/IO bandwidth) |</span>
<span id="cb18-612"><a href="#cb18-612" aria-hidden="true" tabindex="-1"></a>| **Data Quality Challenges** | Deduplication algorithms, toxicity filtering, language detection | Cloud masking, atmospheric correction, sensor calibration |</span>
<span id="cb18-613"><a href="#cb18-613" aria-hidden="true" tabindex="-1"></a>| **Preprocessing Requirements** | Tokenization, sequence packing, attention mask generation | Patch extraction, normalization, spatial/temporal alignment |</span>
<span id="cb18-614"><a href="#cb18-614" aria-hidden="true" tabindex="-1"></a>| **Storage Format Optimization** | Compressed text files, pre-tokenized sequences | Cloud-optimized formats (<span class="co">[</span><span class="ot">COG</span><span class="co">](https://www.cogeo.org/)</span>, <span class="co">[</span><span class="ot">Zarr</span><span class="co">](https://zarr.readthedocs.io/)</span>), tiled storage |</span>
<span id="cb18-615"><a href="#cb18-615" aria-hidden="true" tabindex="-1"></a>| **Access Pattern Differences** | Sequential text processing, random document sampling | Spatial/temporal queries, patch-based sampling, geographic tiling |</span>
<span id="cb18-616"><a href="#cb18-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-617"><a href="#cb18-617" aria-hidden="true" tabindex="-1"></a><span class="fu">### Implementation Examples</span></span>
<span id="cb18-618"><a href="#cb18-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-619"><a href="#cb18-619" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Embedding Creation</span></span>
<span id="cb18-620"><a href="#cb18-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-621"><a href="#cb18-621" aria-hidden="true" tabindex="-1"></a>Embeddings are the foundation of both LLMs and GFMs, but they differ in how raw inputs are converted to dense vector representations. This section demonstrates practical implementation patterns for both domains.</span>
<span id="cb18-622"><a href="#cb18-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-625"><a href="#cb18-625" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-626"><a href="#cb18-626" aria-hidden="true" tabindex="-1"></a><span class="co"># Text embedding creation (LLM approach)</span></span>
<span id="cb18-627"><a href="#cb18-627" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"The forest shows signs of deforestation."</span></span>
<span id="cb18-628"><a href="#cb18-628" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> text.lower().replace(<span class="st">'.'</span>, <span class="st">''</span>).split()</span>
<span id="cb18-629"><a href="#cb18-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-630"><a href="#cb18-630" aria-hidden="true" tabindex="-1"></a><span class="co"># Create simple vocabulary mapping</span></span>
<span id="cb18-631"><a href="#cb18-631" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> {word: i <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">set</span>(tokens))}</span>
<span id="cb18-632"><a href="#cb18-632" aria-hidden="true" tabindex="-1"></a>vocab[<span class="st">'&lt;PAD&gt;'</span>] <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb18-633"><a href="#cb18-633" aria-hidden="true" tabindex="-1"></a>vocab[<span class="st">'&lt;UNK&gt;'</span>] <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb18-634"><a href="#cb18-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-635"><a href="#cb18-635" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert text to token IDs</span></span>
<span id="cb18-636"><a href="#cb18-636" aria-hidden="true" tabindex="-1"></a>token_ids <span class="op">=</span> [vocab.get(token, vocab[<span class="st">'&lt;UNK&gt;'</span>]) <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb18-637"><a href="#cb18-637" aria-hidden="true" tabindex="-1"></a>token_tensor <span class="op">=</span> torch.tensor(token_ids).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb18-638"><a href="#cb18-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-639"><a href="#cb18-639" aria-hidden="true" tabindex="-1"></a><span class="co"># Embedding layer lookup</span></span>
<span id="cb18-640"><a href="#cb18-640" aria-hidden="true" tabindex="-1"></a>embed_layer <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(vocab), <span class="dv">256</span>)</span>
<span id="cb18-641"><a href="#cb18-641" aria-hidden="true" tabindex="-1"></a>text_embeddings <span class="op">=</span> embed_layer(token_tensor)</span>
<span id="cb18-642"><a href="#cb18-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-643"><a href="#cb18-643" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Text Embeddings (LLM):"</span>)</span>
<span id="cb18-644"><a href="#cb18-644" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-645"><a href="#cb18-645" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-646"><a href="#cb18-646" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Token IDs: </span><span class="sc">{</span>token_ids<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-647"><a href="#cb18-647" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Embeddings shape: </span><span class="sc">{</span>text_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-648"><a href="#cb18-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-649"><a href="#cb18-649" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">50</span> <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb18-650"><a href="#cb18-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-651"><a href="#cb18-651" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch embedding creation (GFM approach)</span></span>
<span id="cb18-652"><a href="#cb18-652" aria-hidden="true" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb18-653"><a href="#cb18-653" aria-hidden="true" tabindex="-1"></a>num_bands <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb18-654"><a href="#cb18-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-655"><a href="#cb18-655" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sample multi-spectral satellite patch</span></span>
<span id="cb18-656"><a href="#cb18-656" aria-hidden="true" tabindex="-1"></a>satellite_patch <span class="op">=</span> torch.randn(<span class="dv">1</span>, num_bands, patch_size, patch_size)</span>
<span id="cb18-657"><a href="#cb18-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-658"><a href="#cb18-658" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten patch for linear projection</span></span>
<span id="cb18-659"><a href="#cb18-659" aria-hidden="true" tabindex="-1"></a>patch_flat <span class="op">=</span> satellite_patch.view(<span class="dv">1</span>, num_bands <span class="op">*</span> patch_size <span class="op">*</span> patch_size)</span>
<span id="cb18-660"><a href="#cb18-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-661"><a href="#cb18-661" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear projection to embedding space</span></span>
<span id="cb18-662"><a href="#cb18-662" aria-hidden="true" tabindex="-1"></a>patch_projection <span class="op">=</span> nn.Linear(num_bands <span class="op">*</span> patch_size <span class="op">*</span> patch_size, <span class="dv">256</span>)</span>
<span id="cb18-663"><a href="#cb18-663" aria-hidden="true" tabindex="-1"></a>patch_embeddings <span class="op">=</span> patch_projection(patch_flat)</span>
<span id="cb18-664"><a href="#cb18-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-665"><a href="#cb18-665" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Patch Embeddings (GFM):"</span>)</span>
<span id="cb18-666"><a href="#cb18-666" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original patch shape: </span><span class="sc">{</span>satellite_patch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-667"><a href="#cb18-667" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Flattened patch shape: </span><span class="sc">{</span>patch_flat<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-668"><a href="#cb18-668" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch embeddings shape: </span><span class="sc">{</span>patch_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-669"><a href="#cb18-669" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-670"><a href="#cb18-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-671"><a href="#cb18-671" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Positional Encoding Comparison</span></span>
<span id="cb18-672"><a href="#cb18-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-673"><a href="#cb18-673" aria-hidden="true" tabindex="-1"></a>Positional encodings enable transformers to understand sequence order (LLMs) or spatial relationships (GFMs). The fundamental difference lies in 1D sequential positions versus 2D spatial coordinates.</span>
<span id="cb18-674"><a href="#cb18-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-675"><a href="#cb18-675" aria-hidden="true" tabindex="-1"></a>**Key Differences:**</span>
<span id="cb18-676"><a href="#cb18-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-677"><a href="#cb18-677" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**LLM**: 1D sinusoidal encoding for sequence positions</span>
<span id="cb18-678"><a href="#cb18-678" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GFM**: 2D spatial encoding combining height/width coordinates</span>
<span id="cb18-679"><a href="#cb18-679" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Learned vs Fixed**: Both approaches can use learned or fixed encodings</span>
<span id="cb18-680"><a href="#cb18-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-683"><a href="#cb18-683" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-684"><a href="#cb18-684" aria-hidden="true" tabindex="-1"></a><span class="co"># 1D positional encoding for language models</span></span>
<span id="cb18-685"><a href="#cb18-685" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sinusoidal_positional_encoding(seq_len, embed_dim):</span>
<span id="cb18-686"><a href="#cb18-686" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create sinusoidal positional encodings for sequence data"""</span></span>
<span id="cb18-687"><a href="#cb18-687" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> torch.zeros(seq_len, embed_dim)</span>
<span id="cb18-688"><a href="#cb18-688" aria-hidden="true" tabindex="-1"></a>    position <span class="op">=</span> torch.arange(<span class="dv">0</span>, seq_len).unsqueeze(<span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb18-689"><a href="#cb18-689" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-690"><a href="#cb18-690" aria-hidden="true" tabindex="-1"></a>    div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, embed_dim, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> </span>
<span id="cb18-691"><a href="#cb18-691" aria-hidden="true" tabindex="-1"></a>                       <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> embed_dim))</span>
<span id="cb18-692"><a href="#cb18-692" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-693"><a href="#cb18-693" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb18-694"><a href="#cb18-694" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb18-695"><a href="#cb18-695" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-696"><a href="#cb18-696" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe</span>
<span id="cb18-697"><a href="#cb18-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-698"><a href="#cb18-698" aria-hidden="true" tabindex="-1"></a><span class="co"># 2D positional encoding for geospatial models  </span></span>
<span id="cb18-699"><a href="#cb18-699" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_2d_positional_encoding(height, width, embed_dim):</span>
<span id="cb18-700"><a href="#cb18-700" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Create 2D positional encodings for spatial data"""</span></span>
<span id="cb18-701"><a href="#cb18-701" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> torch.zeros(height, width, embed_dim)</span>
<span id="cb18-702"><a href="#cb18-702" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-703"><a href="#cb18-703" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create position grids</span></span>
<span id="cb18-704"><a href="#cb18-704" aria-hidden="true" tabindex="-1"></a>    y_pos <span class="op">=</span> torch.arange(height).unsqueeze(<span class="dv">1</span>).repeat(<span class="dv">1</span>, width).<span class="bu">float</span>()</span>
<span id="cb18-705"><a href="#cb18-705" aria-hidden="true" tabindex="-1"></a>    x_pos <span class="op">=</span> torch.arange(width).unsqueeze(<span class="dv">0</span>).repeat(height, <span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb18-706"><a href="#cb18-706" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-707"><a href="#cb18-707" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encode Y positions in first half of embedding</span></span>
<span id="cb18-708"><a href="#cb18-708" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embed_dim <span class="op">//</span> <span class="dv">2</span>):</span>
<span id="cb18-709"><a href="#cb18-709" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb18-710"><a href="#cb18-710" aria-hidden="true" tabindex="-1"></a>            pe[:, :, i] <span class="op">=</span> torch.sin(y_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (i <span class="op">/</span> embed_dim)))</span>
<span id="cb18-711"><a href="#cb18-711" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb18-712"><a href="#cb18-712" aria-hidden="true" tabindex="-1"></a>            pe[:, :, i] <span class="op">=</span> torch.cos(y_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (i <span class="op">/</span> embed_dim)))</span>
<span id="cb18-713"><a href="#cb18-713" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-714"><a href="#cb18-714" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encode X positions in second half of embedding</span></span>
<span id="cb18-715"><a href="#cb18-715" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embed_dim <span class="op">//</span> <span class="dv">2</span>, embed_dim):</span>
<span id="cb18-716"><a href="#cb18-716" aria-hidden="true" tabindex="-1"></a>        j <span class="op">=</span> i <span class="op">-</span> embed_dim <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb18-717"><a href="#cb18-717" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> j <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb18-718"><a href="#cb18-718" aria-hidden="true" tabindex="-1"></a>            pe[:, :, i] <span class="op">=</span> torch.sin(x_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (j <span class="op">/</span> embed_dim)))</span>
<span id="cb18-719"><a href="#cb18-719" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb18-720"><a href="#cb18-720" aria-hidden="true" tabindex="-1"></a>            pe[:, :, i] <span class="op">=</span> torch.cos(x_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (j <span class="op">/</span> embed_dim)))</span>
<span id="cb18-721"><a href="#cb18-721" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-722"><a href="#cb18-722" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe</span>
<span id="cb18-723"><a href="#cb18-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-724"><a href="#cb18-724" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate and visualize both encodings</span></span>
<span id="cb18-725"><a href="#cb18-725" aria-hidden="true" tabindex="-1"></a>seq_len, embed_dim <span class="op">=</span> <span class="dv">100</span>, <span class="dv">256</span></span>
<span id="cb18-726"><a href="#cb18-726" aria-hidden="true" tabindex="-1"></a>pos_encoding_1d <span class="op">=</span> sinusoidal_positional_encoding(seq_len, embed_dim)</span>
<span id="cb18-727"><a href="#cb18-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-728"><a href="#cb18-728" aria-hidden="true" tabindex="-1"></a>height, width <span class="op">=</span> <span class="dv">8</span>, <span class="dv">8</span></span>
<span id="cb18-729"><a href="#cb18-729" aria-hidden="true" tabindex="-1"></a>pos_encoding_2d <span class="op">=</span> create_2d_positional_encoding(height, width, embed_dim)</span>
<span id="cb18-730"><a href="#cb18-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-731"><a href="#cb18-731" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"1D positional encoding shape: </span><span class="sc">{</span>pos_encoding_1d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-732"><a href="#cb18-732" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"2D positional encoding shape: </span><span class="sc">{</span>pos_encoding_2d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-733"><a href="#cb18-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-734"><a href="#cb18-734" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize both encodings</span></span>
<span id="cb18-735"><a href="#cb18-735" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb18-736"><a href="#cb18-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-737"><a href="#cb18-737" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb18-738"><a href="#cb18-738" aria-hidden="true" tabindex="-1"></a>plt.imshow(pos_encoding_1d[:<span class="dv">50</span>, :<span class="dv">50</span>].T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb18-739"><a href="#cb18-739" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'1D Positional Encoding (LLM)'</span>)</span>
<span id="cb18-740"><a href="#cb18-740" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Sequence Position'</span>)</span>
<span id="cb18-741"><a href="#cb18-741" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Embedding Dimension'</span>)</span>
<span id="cb18-742"><a href="#cb18-742" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb18-743"><a href="#cb18-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-744"><a href="#cb18-744" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-745"><a href="#cb18-745" aria-hidden="true" tabindex="-1"></a>pos_2d_viz <span class="op">=</span> pos_encoding_2d[:, :, :<span class="dv">64</span>].mean(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb18-746"><a href="#cb18-746" aria-hidden="true" tabindex="-1"></a>plt.imshow(pos_2d_viz, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'equal'</span>)</span>
<span id="cb18-747"><a href="#cb18-747" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'2D Positional Encoding (GFM)'</span>)</span>
<span id="cb18-748"><a href="#cb18-748" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Width'</span>)</span>
<span id="cb18-749"><a href="#cb18-749" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Height'</span>)</span>
<span id="cb18-750"><a href="#cb18-750" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span>
<span id="cb18-751"><a href="#cb18-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-752"><a href="#cb18-752" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-753"><a href="#cb18-753" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-754"><a href="#cb18-754" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/geog-logo.png" class="img-fluid figure-img" width="250"></p>
<figcaption>Department of Geography logo</figcaption>
</figure>
</div>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This website is built with <a href="https://github.com/kcaylor/GEOG-288KC-geospatial-foundation-models"><i class="fa-brands fa-github" title="the github octocat logo" aria-label="github"></i></a> and <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>