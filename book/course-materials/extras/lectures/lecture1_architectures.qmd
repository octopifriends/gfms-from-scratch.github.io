---
title: "Lecture 1: Architectures, Histories, and Development Pipelines"
subtitle: "LLMs vs. Geospatial Foundation Models (GFMs)"
jupyter: geoai
format:
  revealjs:
    theme: ../../meds-website-styles.scss
    slide-number: true
    toc: false
    incremental: true
    code-overflow: wrap
execute:
  echo: true
  warning: false
  freeze: auto
---

## Welcome

- Course focus: Building Geospatial Foundation Models (GFMs)
- Today: compare LLM and GFM development pipelines; key architecture ideas; brief history

## Agenda

- AI/ML â†’ Transformers: a 10,000â€‘ft history
- LLM development pipeline (9 steps)
- GFM development pipeline (9 steps, geospatialized)
- Key differences and implications

## From AI to Transformers (Very Brief History)

- 1950sâ€“1990s: Symbolic AI, early neural nets
- 2012: Deep learning (ImageNet)
- 2017: Transformers ("Attention Is All You Need")
- 2018â€“2020: BERT/GPT families
- 2021â€“2024: Scaling laws; instruction tuning; multimodality; diffusion

## Transformer Essentials

- Tokenization â†’ Embeddings â†’ Positional encodings â†’ Attention â†’ MLP â†’ Stacking â†’ Pretraining â†’ Finetuning
- Key superpower: learn longâ€‘range dependencies via attention

## 9â€‘Step Development Pipeline (LLMs)

1. Data preparation & sampling (text corpora, dedup, mixing)
2. Tokenization (BPE, vocab; special tokens)
3. Architecture (GPT/BERT; depth/width; context length)
4. Pretraining objective (nextâ€‘token, masked LM)
5. Training loop (optimizers, LR schedule, mixed precision)
6. Evaluation (perplexity, downstream probes)
7. Load/use pretrained weights
8. Finetuning (taskâ€‘specific; PEFT)
9. Implementation & deployment (APIs, inference optimizations)

## 9â€‘Step Development Pipeline (GFMs)

1. Data preparation & sampling (multiâ€‘spectral, georegistration, tiling, cloud handling)
2. Tokenization (patchâ€‘based; continuous to embeddings; 2D/temporal positions)
3. Architecture (ViT encoders; spatial/temporal attention; memory constraints)
4. Pretraining objective (masked patch reconstruction; contrastive for multimodal)
5. Training loop (cloud masks, mixed precision, gradient strategies)
6. Evaluation (reconstruction, linear probing, spatial/temporal generalization)
7. Load/use pretrained weights (Prithvi, SatMAE; adapters)
8. Finetuning (task heads; PEFT; fewâ€‘shot for limited labels)
9. Implementation & deployment (tiling inference; APIs; geospatial UIs)

## Stepâ€‘byâ€‘Step: LLM vs. GFM Differences

### 1) Data Preparation
- LLMs: text scraping, deduplication, quality filters
- GFMs: sensor calibration, atmospheric correction, cloud masks, georegistration, temporal compositing

### 2) Tokenization
- LLMs: discrete vocab; BPE; special tokens
- GFMs: no discrete vocab; image patches â†’ linear projection; 2D + time encodings

### 3) Architecture
- LLMs: decoderâ€‘only (GPT) or encoder (BERT)
- GFMs: ViT encoders; spatial/temporal attention; multispectral embeddings

### 4) Pretraining Objective
- LLMs: nextâ€‘token (AR) or masked LM (denoise text)
- GFMs: masked patch reconstruction; contrastive with text or modalities (optional)

### 5) Training Loop
- LLMs: LR schedules, batch packing, AMP
- GFMs: all of the above + cloud/validity masks; patch sampling from rasters

### 6) Evaluation
- LLMs: perplexity; downstream tasks (classification, QA)
- GFMs: reconstruction metrics; linear probes; spatial/temporal OOD generalization

### 7) Pretrained Weights
- LLMs: model hubs; tokenizer alignment
- GFMs: Prithvi/SatMAE weights; band order & resolution alignment

### 8) Finetuning
- LLMs: instruction tuning; PEFT
- GFMs: task heads (segmentation, regression); adapters/LoRA; few labels

### 9) Deployment
- LLMs: token streaming; KV cache; latency budgets
- GFMs: tiling/overlap; geospatial projections; retrieval by time/area; batch inference

## Scaling and Evolution (LLMs)

- Parameters: 100M â†’ 10B â†’ 70B+
- Context windows: 512 â†’ 128k+
- Training data: curated web, books, code; cleaner data â‰ˆ better sample efficiency

## Scaling and Evolution (GFMs)

- Parameters: 50M â†’ 500M (varies); encoder focus
- Inputs: 6+ bands; 2D + time; multiâ€‘sensor fusion (optical/SAR)
- Data volume constrained by storage and tiling/IO throughput

## Example: Tokenization Contrast

LLM (discrete): tokens â†’ embedding lookup

```python
# | echo: true
vocab_size, embed_dim = 50_000, 768
import numpy as np
try:
    import torch
    tok_ids = torch.tensor([1, 2, 3, 4])
    emb = torch.nn.Embedding(vocab_size, embed_dim)(tok_ids)
    print(emb.shape)
except Exception as e:
    print("LLM embedding example skipped:", e)
```

GFM (continuous patches): patch â†’ linear projection

```python
# | echo: true
import numpy as np
patch_dim, embed_dim = 16*16*6, 768
rng = np.random.default_rng(42)
patches = rng.normal(size=(4, patch_dim))
try:
    import torch
    proj = torch.nn.Linear(patch_dim, embed_dim)
    out = proj(torch.from_numpy(patches).float())
    print(out.shape)
except Exception as e:
    print("GFM projection example skipped:", e)
```

## Putting It Together (Course Mapping)

- Weeks 1â€“3: Data â†’ Attention â†’ Architecture
- Weeks 4â€“7: Pretraining â†’ Training loop â†’ Evaluation â†’ Pretrained integration
- Weeks 8â€“10: Finetuning â†’ Deployment â†’ Synthesis

## References & Further Reading

- Transformers: Vaswani et al. (2017)
- Vision Transformers: Dosovitskiy et al. (2020)
- Masked Autoencoders: He et al. (2021)
- Geospatial FMs: Prithvi, SatMAE (model docs & papers)

## Handsâ€‘On Next

- See Week 1 Interactive Session: Geospatial Data Foundations
- Start building the data pipeline with deterministic outputs


Neural networks expect inputs in a very specific, structured format. The first step of any deep learning workflow is to translate our data into a â€œlanguageâ€ that neural networks understand. This translation process produces embeddings, which are consistent numerical representations that encode our data in a form that makes them ready for model processing.

An embedding is simply a standardized vector representation of raw input data. As a toy example, `[0, 0, 1, 0]` can be thought of as an embedding of the number `2` using a most significant bit (MSB) binary positional encoding scheme. In text processing, embeddings are necessary because tokens like `forest` and `water` are categorical symbols; they must be mapped to consistent numerical values before a neural network can operate on them.

Unlike text, geospatial data values are usually already coded in a numerical scheme. Generally, each pixel contains measured values such as surface reflectance at specific wavelengths, or derived quantities such as NDVI, or categories such as land cover. 

If we donâ€™t need to â€œmake the data numericâ€,  why do we still need embeddings for geospatial data? The answer is that **embeddings make data both interpretable and meaningful**. In a language model, every occurrence of the token `cat` maps to the same embedding (say `1356`), because `cat` usually carries the same semantic meaning, even in different contexts. In contrast, the same pixel valueâ€”for example, `0.15`â€”can mean something entirely different depending on whether it represents surface albedo, slope, or NDVI. Without additional semantic context, the model has no way to disambiguate these cases.

| Data Type            | Semantic Consistency ğŸ§  | Analytical Tractability ğŸ“Š | Primary Goal of Embeddings ğŸ¯ |
|----------------------|------------------------|----------------------------|--------------------------------|
| **Text**             | âœ… Words/tokens have consistent meaning across a diverse contexts | âŒ Raw text is not numeric; cannot be directly processed by neural networks | **Enable computation** â€” convert categorical symbols into a consistent numerical space |
| **Numerical / Geospatial** | âŒ Numeric values can represent different quantities across datasets | âœ… Already stored as numbers; can be directly input to computations | **Add context** â€” encode what, where, and when into numerical features |

### Geospatial Semantics

While text embeddings typically just map words to vectors, geospatial embeddings must carry richer meaning. A pixel or patch value isnâ€™t the whole story; its meaning depends on extra context. Therefore, the goal of embedding in geospatial models is not merely to turn categories into numbers, itâ€™s to transform raw measurements into a representation that is both meaningful and efficient for learning. This might involve:

- Combining multiple spectral bands into a single vector
- Normalizing across different sensors to remove acquisition biases
- Compressing high-dimensional spectral signatures into a lower-dimensional space that preserves the most relevant patterns for the task at hand. 
- Incorporating spatial and temporal context, enabling the model to understand not just what is in a pixel, but where and when it is.


#### Semantic Dimensions for Geospatial Embeddings

| ğŸ“ Semantic Dimension | ğŸ§© What It Is | ğŸš€ Why It Matters for GFMs |
|---|---|---|
| ğŸ—‚ **Source meaning** | The same number can represent different physical measurements depending on where the data came from and how it was processed | Without knowing the source, two identical values might mean very different things |
| ğŸŒ **Location reference** | Information that describes where the measurement is on Earth and how the image is laid out | Changes in location reference affect how measurements line up and can change their meaning |
| ğŸ” **Detail and scale** | How much area each pixel represents, and whether it shows a single thing or a mix of things | Larger pixels mix more features, which changes how the value should be interpreted |
| ğŸ“ **Measurement units and type** | Values can be in different units (meters, degrees, percentages) or represent categories instead of continuous numbers | Mixing units or types can confuse the model without clear handling |
| ğŸ“… **When and how it was collected** | The date, the tool or satellite used, the viewing angle, and environmental conditions | Even the same number can mean different things at different times or with different tools |
| ğŸ§® **Calculated or classified values** | Numbers created from other measurements, like vegetation indexes or land cover labels | These values already include assumptions that influence how they should be used |
| ğŸ“¦ **Extra context** | Additional information like location, time, or sensor type added alongside the pixel values | Helps the model understand what, where, and when each measurement is from |

:::{.callout-important}
## Embedding for learning
Geospatial embeddings must combine the measured values with extra information about **what was measured, where, when, and how**, so the model learns from the *full meaning* of each observation.
:::

