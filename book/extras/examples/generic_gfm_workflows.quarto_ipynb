{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Week 3: Working with Geospatial Foundation Models\"\n",
        "subtitle: \"Hands-on practice with TerraTorch and pretrained models\"\n",
        "jupyter: geoai\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    code-fold: false\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This week we'll work with pretrained geospatial foundation models (GFMs) using TerraTorch, a framework that simplifies fine-tuning and inference with state-of-the-art models. You'll learn to use models like Prithvi and Clay for practical remote sensing tasks.\n",
        "\n",
        ":::{.callout-tip}\n",
        "## Learning Goals\n",
        "By the end of this session, you will:\n",
        "\n",
        "- Understand the architecture of geospatial foundation models\n",
        "- Load and use pretrained models for feature extraction\n",
        "- Fine-tune models on custom datasets\n",
        "- Compare different GFM architectures\n",
        "- Apply models to real satellite imagery\n",
        ":::\n",
        "\n",
        "## Session Overview\n",
        "\n",
        "Today's workflow focuses on practical model usage:\n",
        "\n",
        "| Step | Activity | Tools | Output |\n",
        "|------|----------|-------|--------|\n",
        "| 1 | Model architecture exploration | TerraTorch | Understanding |\n",
        "| 2 | Feature extraction | PyTorch, TerraTorch | Embeddings |\n",
        "| 3 | Simple classification task | TerraTorch, TorchGeo | Trained model |\n",
        "| 4 | Model comparison | Multiple backbones | Performance metrics |\n",
        "| 5 | Real-world inference | Satellite imagery | Predictions |\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Environment Setup and Model Loading\n",
        "\n",
        "Let's start by setting up our environment and understanding the available models.\n",
        "\n",
        "### Import Libraries and Check Setup\n"
      ],
      "id": "a979690a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "id": "6fd3f8ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Utilities → `geogfm/utils/terratorch_utils.py`\n",
        "\n",
        "Let's create reusable utilities for working with TerraTorch models.\n"
      ],
      "id": "b1c4b060"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tangle": "geogfm/utils/terratorch_utils.py"
      },
      "source": [
        "\"\"\"Utilities for working with TerraTorch models.\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Dict, Optional, Tuple, List\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_pretrained_model(\n",
        "    model_name: str,\n",
        "    num_classes: Optional[int] = None,\n",
        "    task: str = \"classification\",\n",
        "    device: str = \"cpu\"\n",
        ") -> nn.Module:\n",
        "    \"\"\"\n",
        "    Load a pretrained model.\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the model (e.g., 'prithvi_100m', 'clay_v1')\n",
        "        num_classes: Number of output classes (for classification/segmentation)\n",
        "        task: Task type ('classification', 'segmentation', 'embedding')\n",
        "        device: Device to load model on\n",
        "\n",
        "    Returns:\n",
        "        Loaded model\n",
        "\n",
        "    Note:\n",
        "        This function requires terratorch to be installed.\n",
        "        For this demo, we use simplified models instead.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from terratorch.models import get_model\n",
        "\n",
        "        model_config = {\n",
        "            'backbone': model_name,\n",
        "            'task': task,\n",
        "        }\n",
        "\n",
        "        if num_classes is not None:\n",
        "            model_config['num_classes'] = num_classes\n",
        "\n",
        "        model = get_model(**model_config)\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        return model\n",
        "    except ImportError:\n",
        "        raise ImportError(\n",
        "            \"terratorch is required for loading foundation models. \"\n",
        "            \"Install with: pip install terratorch\"\n",
        "        )\n",
        "\n",
        "\n",
        "def extract_features(\n",
        "    model: nn.Module,\n",
        "    images: torch.Tensor,\n",
        "    layer: str = \"last\",\n",
        "    device: str = \"cpu\"\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Extract features from a model.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        images: Input images (B, C, H, W)\n",
        "        layer: Which layer to extract from\n",
        "        device: Device to use\n",
        "\n",
        "    Returns:\n",
        "        Feature tensor\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if hasattr(model, 'encode'):\n",
        "            features = model.encode(images)\n",
        "        else:\n",
        "            features = model(images)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def get_model_info(model: nn.Module) -> Dict:\n",
        "    \"\"\"\n",
        "    Get information about a model.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with model info\n",
        "    \"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    return {\n",
        "        'total_parameters': total_params,\n",
        "        'trainable_parameters': trainable_params,\n",
        "        'frozen_parameters': total_params - trainable_params,\n",
        "        'model_type': type(model).__name__\n",
        "    }\n",
        "\n",
        "\n",
        "def prepare_satellite_image(\n",
        "    image_path: str,\n",
        "    target_bands: Optional[List[int]] = None,\n",
        "    normalize: bool = True\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Load and prepare a satellite image for model input.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to image file\n",
        "        target_bands: Band indices to use (None = all)\n",
        "        normalize: Whether to normalize to [0, 1]\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape (1, C, H, W)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import rasterio\n",
        "    except ImportError:\n",
        "        raise ImportError(\"rasterio required for image loading. Install with: pip install rasterio\")\n",
        "\n",
        "    with rasterio.open(image_path) as src:\n",
        "        if target_bands is None:\n",
        "            data = src.read()\n",
        "        else:\n",
        "            data = src.read(target_bands)\n",
        "\n",
        "    # Convert to float and normalize\n",
        "    data = data.astype(np.float32)\n",
        "\n",
        "    if normalize:\n",
        "        # Handle NaN values\n",
        "        valid_mask = ~np.isnan(data)\n",
        "        if valid_mask.any():\n",
        "            data_min = np.nanmin(data)\n",
        "            data_max = np.nanmax(data)\n",
        "            data = (data - data_min) / (data_max - data_min + 1e-8)\n",
        "            data = np.nan_to_num(data, nan=0.0)\n",
        "\n",
        "    # Add batch dimension and convert to tensor\n",
        "    tensor = torch.from_numpy(data).unsqueeze(0)\n",
        "\n",
        "    return tensor"
      ],
      "id": "d231fda7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Comparison Utilities → `geogfm/utils/terratorch_utils.py`\n"
      ],
      "id": "8aaf6d23"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tangle": "geogfm/utils/terratorch_utils.py",
        "mode": "append"
      },
      "source": [
        "def compare_models(\n",
        "    model_names: List[str],\n",
        "    sample_input: torch.Tensor,\n",
        "    device: str = \"cpu\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compare multiple models on the same input.\n",
        "\n",
        "    Args:\n",
        "        model_names: List of model names to compare\n",
        "        sample_input: Sample input tensor\n",
        "        device: Device to use\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with comparison results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for model_name in model_names:\n",
        "        try:\n",
        "            model = load_pretrained_model(\n",
        "                model_name=model_name,\n",
        "                task=\"embedding\",\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            info = get_model_info(model)\n",
        "\n",
        "            # Measure inference time\n",
        "            import time\n",
        "            start = time.time()\n",
        "            with torch.no_grad():\n",
        "                output = extract_features(model, sample_input, device=device)\n",
        "            elapsed = time.time() - start\n",
        "\n",
        "            results.append({\n",
        "                'model': model_name,\n",
        "                'total_params': info['total_parameters'],\n",
        "                'trainable_params': info['trainable_parameters'],\n",
        "                'inference_time_ms': elapsed * 1000,\n",
        "                'output_shape': str(tuple(output.shape))\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {model_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "def visualize_embeddings(\n",
        "    embeddings: np.ndarray,\n",
        "    labels: Optional[np.ndarray] = None,\n",
        "    method: str = \"pca\",\n",
        "    n_components: int = 2\n",
        ") -> plt.Figure:\n",
        "    \"\"\"\n",
        "    Visualize high-dimensional embeddings in 2D.\n",
        "\n",
        "    Args:\n",
        "        embeddings: Feature embeddings (N, D)\n",
        "        labels: Optional labels for coloring\n",
        "        method: Dimensionality reduction method ('pca', 'tsne')\n",
        "        n_components: Number of components\n",
        "\n",
        "    Returns:\n",
        "        Matplotlib figure\n",
        "    \"\"\"\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "\n",
        "    if method == \"pca\":\n",
        "        reducer = PCA(n_components=n_components)\n",
        "    elif method == \"tsne\":\n",
        "        reducer = TSNE(n_components=n_components, random_state=42)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "    reduced = reducer.fit_transform(embeddings)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    if labels is not None:\n",
        "        scatter = ax.scatter(\n",
        "            reduced[:, 0], reduced[:, 1],\n",
        "            c=labels, cmap='tab10', alpha=0.6, s=50\n",
        "        )\n",
        "        plt.colorbar(scatter, ax=ax, label='Class')\n",
        "    else:\n",
        "        ax.scatter(\n",
        "            reduced[:, 0], reduced[:, 1],\n",
        "            alpha=0.6, s=50\n",
        "        )\n",
        "\n",
        "    ax.set_xlabel(f'{method.upper()} Component 1')\n",
        "    ax.set_ylabel(f'{method.upper()} Component 2')\n",
        "    ax.set_title(f'Embedding Visualization ({method.upper()})')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    return fig"
      ],
      "id": "a0a8d5eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Working with Pretrained Models\n",
        "\n",
        "Now let's load and explore a simple pretrained model.\n",
        "\n",
        "### Demo: Load and Inspect a Model\n"
      ],
      "id": "3a92ee68"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper function for model info\n",
        "def get_model_info(model):\n",
        "    \"\"\"Get information about a model.\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return {\n",
        "        'total_parameters': total_params,\n",
        "        'trainable_parameters': trainable_params,\n",
        "        'frozen_parameters': total_params - trainable_params,\n",
        "        'model_type': type(model).__name__\n",
        "    }\n",
        "\n",
        "# Create a simple demonstration model\n",
        "# This represents a feature extractor similar to what foundation models provide\n",
        "class SimpleFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Lightweight feature extractor demonstrating foundation model concepts.\n",
        "\n",
        "    In practice, you would load pretrained models like Prithvi or Clay.\n",
        "    This simplified version shows the same workflow without dependencies.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=4, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, embed_dim, kernel_size=3, padding=1),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x.squeeze(-1).squeeze(-1)\n",
        "\n",
        "model = SimpleFeatureExtractor(in_channels=4).to(device)  # 4 bands: RGB+NIR\n",
        "model_info = get_model_info(model)\n",
        "\n",
        "print(\"Feature Extractor Model Created\")\n",
        "print(f\"Total parameters: {model_info['total_parameters']:,}\")\n",
        "print(f\"Trainable parameters: {model_info['trainable_parameters']:,}\")\n",
        "print(f\"Model type: {model_info['model_type']}\")\n",
        "\n",
        "print(\"\\nThis model demonstrates the same workflow as full foundation models:\")\n",
        "print(\"- Feature extraction from satellite imagery\")\n",
        "print(\"- Transfer learning for downstream tasks\")\n",
        "print(\"- Fine-tuning strategies\")"
      ],
      "id": "41510dac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Real Satellite Data\n",
        "\n",
        "Let's use actual Sentinel-2 imagery instead of synthetic data. We'll use our `geogfm` library to efficiently download and process a small area.\n",
        "\n",
        ":::{.callout-note}\n",
        "## Download Optimization Strategy\n",
        "To keep downloads fast and manageable:\n",
        "\n",
        "1. **Small AOI**: ~5km x 5km area around Santa Barbara\n",
        "2. **Low cloud cover**: < 5% ensures clean data\n",
        "3. **20m resolution**: Faster than 10m, still good quality\n",
        "4. **4 bands only**: RGB + NIR (B02, B03, B04, B08)\n",
        "5. **Cloud masking**: Filters out contaminated pixels\n",
        "6. **Patch extraction**: Small 64x64 pixel patches\n",
        "\n",
        "Total download: ~50-100 MB instead of several GB!\n",
        ":::\n"
      ],
      "id": "03437be6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import geogfm functions for data loading\n",
        "from geogfm.c01 import (\n",
        "    setup_planetary_computer_auth,\n",
        "    search_sentinel2_scenes,\n",
        "    load_sentinel2_bands,\n",
        "    get_subset_from_scene\n",
        ")\n",
        "from geogfm.c02 import load_scene_with_cloudmask\n",
        "\n",
        "# Setup authentication\n",
        "setup_planetary_computer_auth()\n",
        "\n",
        "# Define a small AOI in Santa Barbara (coastal area with diverse land cover)\n",
        "# This small area (~5km x 5km) downloads quickly but has good variety\n",
        "santa_barbara_bbox = [-119.85, 34.40, -119.75, 34.48]\n",
        "\n",
        "print(\"Searching for high-quality Sentinel-2 scenes...\")\n",
        "print(f\"AOI: Santa Barbara coastal area\")\n",
        "\n",
        "# Search for clear scenes (low cloud cover)\n",
        "scenes = search_sentinel2_scenes(\n",
        "    bbox=santa_barbara_bbox,\n",
        "    date_range=\"2023-06-01/2023-08-31\",  # Summer for clear weather\n",
        "    cloud_cover_max=5,  # Very low cloud cover\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "print(f\"\\nFound {len(scenes)} clear scenes\")\n",
        "if scenes:\n",
        "    best_scene = scenes[0]\n",
        "    print(f\"Using scene: {best_scene.id}\")\n",
        "    print(f\"Date: {best_scene.properties['datetime'][:10]}\")\n",
        "    print(f\"Cloud cover: {best_scene.properties['eo:cloud_cover']:.1f}%\")"
      ],
      "id": "fb78ae7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract Patches from Real Scene\n",
        "\n",
        "Now let's create a function to extract small patches from the satellite scene. This approach keeps memory usage low while providing real spectral data.\n"
      ],
      "id": "c2869a1d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_patches_from_scene(\n",
        "    item,\n",
        "    subset_bbox,\n",
        "    patch_size=64,\n",
        "    stride=32,\n",
        "    bands=['B02', 'B03', 'B04', 'B08'],\n",
        "    target_patches=200\n",
        "):\n",
        "    \"\"\"\n",
        "    Extract fixed-size patches from a satellite scene.\n",
        "\n",
        "    Args:\n",
        "        item: STAC item\n",
        "        subset_bbox: Spatial subset to load\n",
        "        patch_size: Size of each patch in pixels\n",
        "        stride: Stride between patches (smaller = more overlap)\n",
        "        bands: Bands to load\n",
        "        target_patches: Approximate number of patches to extract\n",
        "\n",
        "    Returns:\n",
        "        patches: Tensor of shape (N, C, H, W)\n",
        "        valid_mask: Boolean mask indicating valid (non-cloudy) patches\n",
        "    \"\"\"\n",
        "    # Load scene data with cloud masking\n",
        "    print(\"Loading scene data...\")\n",
        "    scene_data, valid_fraction = load_scene_with_cloudmask(\n",
        "        item,\n",
        "        target_crs='EPSG:32610',  # UTM Zone 10N for Santa Barbara\n",
        "        target_resolution=20,  # 20m resolution for faster processing\n",
        "        good_pixel_classes=[4, 5, 6],  # Vegetation, bare soil, water\n",
        "        subset_bbox=subset_bbox\n",
        "    )\n",
        "\n",
        "    if scene_data is None or valid_fraction < 0.3:\n",
        "        raise ValueError(f\"Insufficient valid data (fraction: {valid_fraction:.2f})\")\n",
        "\n",
        "    print(f\"Valid pixel fraction: {valid_fraction:.1%}\")\n",
        "\n",
        "    # Get band data and cloud mask\n",
        "    band_arrays = []\n",
        "    for band_name in ['red', 'green', 'blue', 'nir']:\n",
        "        if band_name in scene_data:\n",
        "            band_data = scene_data[band_name].values\n",
        "            band_arrays.append(band_data)\n",
        "\n",
        "    cloud_mask = scene_data['cloud_mask'].values\n",
        "\n",
        "    # Stack bands: (C, H, W)\n",
        "    scene_array = np.stack(band_arrays, axis=0)\n",
        "    print(f\"Scene array shape: {scene_array.shape}\")\n",
        "\n",
        "    # Extract patches\n",
        "    patches = []\n",
        "    valid_masks = []\n",
        "\n",
        "    C, H, W = scene_array.shape\n",
        "\n",
        "    # Calculate patch positions\n",
        "    patch_positions = []\n",
        "    for y in range(0, H - patch_size + 1, stride):\n",
        "        for x in range(0, W - patch_size + 1, stride):\n",
        "            patch_positions.append((y, x))\n",
        "\n",
        "    # Randomly sample if we have too many\n",
        "    if len(patch_positions) > target_patches:\n",
        "        indices = np.random.choice(len(patch_positions), target_patches, replace=False)\n",
        "        patch_positions = [patch_positions[i] for i in indices]\n",
        "\n",
        "    print(f\"Extracting {len(patch_positions)} patches...\")\n",
        "\n",
        "    for y, x in patch_positions:\n",
        "        # Extract patch\n",
        "        patch = scene_array[:, y:y+patch_size, x:x+patch_size]\n",
        "        mask_patch = cloud_mask[y:y+patch_size, x:x+patch_size]\n",
        "\n",
        "        # Check if patch is mostly valid\n",
        "        valid_fraction = np.mean(mask_patch)\n",
        "\n",
        "        if valid_fraction > 0.8:  # At least 80% valid pixels\n",
        "            # Normalize patch to [0, 1] using percentile scaling\n",
        "            patch_normalized = np.zeros_like(patch)\n",
        "            for c in range(patch.shape[0]):\n",
        "                band = patch[c]\n",
        "                valid_data = band[~np.isnan(band)]\n",
        "                if len(valid_data) > 0:\n",
        "                    p2, p98 = np.percentile(valid_data, [2, 98])\n",
        "                    band_norm = (band - p2) / (p98 - p2 + 1e-8)\n",
        "                    band_norm = np.clip(band_norm, 0, 1)\n",
        "                    band_norm = np.nan_to_num(band_norm, 0)\n",
        "                    patch_normalized[c] = band_norm\n",
        "\n",
        "            patches.append(patch_normalized)\n",
        "            valid_masks.append(True)\n",
        "        else:\n",
        "            valid_masks.append(False)\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    if patches:\n",
        "        patches_tensor = torch.from_numpy(np.stack(patches)).float()\n",
        "        print(f\"Extracted {len(patches)} valid patches of shape {patches_tensor[0].shape}\")\n",
        "        return patches_tensor, np.array(valid_masks)\n",
        "    else:\n",
        "        raise ValueError(\"No valid patches extracted\")\n",
        "\n",
        "# Extract patches from the scene\n",
        "print(\"\\nExtracting patches from satellite scene...\")\n",
        "sample_patches, valid_mask = extract_patches_from_scene(\n",
        "    best_scene,\n",
        "    subset_bbox=santa_barbara_bbox,\n",
        "    patch_size=64,\n",
        "    stride=48,  # Some overlap for diversity\n",
        "    target_patches=200\n",
        ")\n",
        "\n",
        "print(f\"\\nPatch dataset created:\")\n",
        "print(f\"Number of patches: {len(sample_patches)}\")\n",
        "print(f\"Patch shape: {sample_patches[0].shape}\")\n",
        "print(f\"Data type: {sample_patches.dtype}\")"
      ],
      "id": "fae060fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Pseudo-Labels for Classification\n",
        "\n",
        "Since we're using real data without ground truth labels, we'll create pseudo-labels based on spectral characteristics. This demonstrates how foundation models can learn from spectral patterns.\n"
      ],
      "id": "bae73bdc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_spectral_labels(patches):\n",
        "    \"\"\"\n",
        "    Create pseudo-labels based on spectral characteristics.\n",
        "\n",
        "    Uses NDVI and other indices to classify into approximate land cover types.\n",
        "    This is for demonstration - real applications would use ground truth.\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "\n",
        "    for patch in patches:\n",
        "        # Calculate spectral indices\n",
        "        red = patch[0].numpy()  # B02\n",
        "        green = patch[1].numpy()  # B03\n",
        "        blue = patch[2].numpy()  # B04\n",
        "        nir = patch[3].numpy()  # B08\n",
        "\n",
        "        # NDVI\n",
        "        ndvi = (nir - red) / (nir + red + 1e-8)\n",
        "\n",
        "        # Mean values\n",
        "        ndvi_mean = np.mean(ndvi)\n",
        "        nir_mean = np.mean(nir)\n",
        "        red_mean = np.mean(red)\n",
        "        blue_mean = np.mean(blue)\n",
        "\n",
        "        # Simple classification based on spectral signatures\n",
        "        if blue_mean > 0.6 and ndvi_mean < 0:\n",
        "            label = 0  # Water (high blue, low NDVI)\n",
        "        elif ndvi_mean > 0.5:\n",
        "            label = 1  # Dense vegetation (high NDVI)\n",
        "        elif nir_mean < 0.3 and red_mean > 0.4:\n",
        "            label = 2  # Urban/built-up (low NIR, moderate red)\n",
        "        elif ndvi_mean < 0.2 and red_mean > 0.5:\n",
        "            label = 3  # Bare soil (low NDVI, high red)\n",
        "        else:\n",
        "            label = 4  # Mixed/other (everything else)\n",
        "\n",
        "        labels.append(label)\n",
        "\n",
        "    return torch.tensor(labels)\n",
        "\n",
        "# Create labels\n",
        "sample_labels = create_spectral_labels(sample_patches)\n",
        "\n",
        "print(f\"Created spectral-based labels\")\n",
        "print(f\"Label distribution:\")\n",
        "class_names = ['Water', 'Vegetation', 'Urban', 'Bare Soil', 'Mixed']\n",
        "for i, name in enumerate(class_names):\n",
        "    count = (sample_labels == i).sum().item()\n",
        "    print(f\"  {name}: {count} patches ({count/len(sample_labels)*100:.1f}%)\")"
      ],
      "id": "1450bc1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Real Satellite Patches\n",
        "\n",
        "Let's look at some of the patches we extracted from the satellite scene.\n"
      ],
      "id": "381d2da9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize a sample of patches with their labels\n",
        "fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Sample patches from each class\n",
        "for i, class_idx in enumerate(range(5)):\n",
        "    # Get patches of this class\n",
        "    class_mask = sample_labels == class_idx\n",
        "    class_indices = torch.where(class_mask)[0]\n",
        "\n",
        "    # Show up to 3 examples per class\n",
        "    n_show = min(3, len(class_indices))\n",
        "    for j in range(n_show):\n",
        "        idx = class_indices[j].item()\n",
        "        ax = axes[i * 3 + j]\n",
        "\n",
        "        # Create false color composite (NIR-R-G)\n",
        "        nir = sample_patches[idx, 3].numpy()\n",
        "        red = sample_patches[idx, 0].numpy()\n",
        "        green = sample_patches[idx, 1].numpy()\n",
        "\n",
        "        img = np.stack([nir, red, green], axis=2)\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f\"{class_names[class_idx]}\", fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Hide unused axes\n",
        "    for j in range(n_show, 3):\n",
        "        axes[i * 3 + j].axis('off')\n",
        "\n",
        "# Hide remaining axes\n",
        "for i in range(15, 18):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Sample Patches by Land Cover Type (False Color: NIR-R-G)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nReal satellite data loaded successfully!\")\n",
        "print(\"Each patch is 64x64 pixels at 20m resolution (1.28km x 1.28km)\")"
      ],
      "id": "b355e111",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo: Feature Extraction\n"
      ],
      "id": "fdf5f3c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper function for feature extraction\n",
        "def extract_features(model, images, device='cpu'):\n",
        "    \"\"\"Extract features from a model.\"\"\"\n",
        "    model.eval()\n",
        "    images = images.to(device)\n",
        "    with torch.no_grad():\n",
        "        if hasattr(model, 'encode'):\n",
        "            features = model.encode(images)\n",
        "        else:\n",
        "            features = model(images)\n",
        "    return features\n",
        "\n",
        "# Extract features from all patches\n",
        "all_features = []\n",
        "batch_size = 32\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(sample_patches), batch_size):\n",
        "        batch = sample_patches[i:i+batch_size].to(device)\n",
        "        features = extract_features(model, batch, device=str(device))\n",
        "        all_features.append(features.cpu())\n",
        "\n",
        "embeddings = torch.cat(all_features, dim=0).numpy()\n",
        "\n",
        "print(f\"Extracted embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Embedding dimension: {embeddings.shape[1]}\")"
      ],
      "id": "1fa2632e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo: Visualize Embeddings\n"
      ],
      "id": "ebadd192"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper function for visualization\n",
        "def visualize_embeddings(embeddings, labels=None, method='pca', n_components=2):\n",
        "    \"\"\"Visualize high-dimensional embeddings in 2D.\"\"\"\n",
        "    from sklearn.decomposition import PCA\n",
        "    from sklearn.manifold import TSNE\n",
        "\n",
        "    if method == \"pca\":\n",
        "        reducer = PCA(n_components=n_components)\n",
        "    elif method == \"tsne\":\n",
        "        reducer = TSNE(n_components=n_components, random_state=42)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "    reduced = reducer.fit_transform(embeddings)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    if labels is not None:\n",
        "        scatter = ax.scatter(\n",
        "            reduced[:, 0], reduced[:, 1],\n",
        "            c=labels, cmap='tab10', alpha=0.6, s=50\n",
        "        )\n",
        "        plt.colorbar(scatter, ax=ax, label='Class')\n",
        "    else:\n",
        "        ax.scatter(reduced[:, 0], reduced[:, 1], alpha=0.6, s=50)\n",
        "\n",
        "    ax.set_xlabel(f'{method.upper()} Component 1')\n",
        "    ax.set_ylabel(f'{method.upper()} Component 2')\n",
        "    ax.set_title(f'Embedding Visualization ({method.upper()})')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Convert labels to numpy\n",
        "labels_np = sample_labels.numpy()\n",
        "\n",
        "# Visualize with PCA\n",
        "fig = visualize_embeddings(\n",
        "    embeddings,\n",
        "    labels=labels_np,\n",
        "    method='pca',\n",
        "    n_components=2\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Feature embeddings visualized using PCA\")\n",
        "print(\"Points with similar colors represent similar land cover types\")"
      ],
      "id": "12632e2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: Simple Classification Task\n",
        "\n",
        "Let's fine-tune a model on our sample data.\n",
        "\n",
        "### Training Utilities → `geogfm/training/simple_trainer.py`\n"
      ],
      "id": "2b63ac8c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tangle": "geogfm/training/simple_trainer.py"
      },
      "source": [
        "\"\"\"Simple training utilities for classification tasks.\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Dict, Optional, Tuple\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    \"\"\"Simple dataset for image patches and labels.\"\"\"\n",
        "\n",
        "    def __init__(self, images: torch.Tensor, labels: torch.Tensor):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "def train_classifier(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: Optional[DataLoader] = None,\n",
        "    n_epochs: int = 10,\n",
        "    lr: float = 1e-4,\n",
        "    device: str = \"cpu\"\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Train a classifier.\n",
        "\n",
        "    Args:\n",
        "        model: Model to train\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        n_epochs: Number of epochs\n",
        "        lr: Learning rate\n",
        "        device: Device to use\n",
        "\n",
        "    Returns:\n",
        "        Training history\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}: \"\n",
        "                  f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f}\")\n",
        "            if val_loader is not None:\n",
        "                print(f\"  Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model: nn.Module,\n",
        "    test_loader: DataLoader,\n",
        "    device: str = \"cpu\"\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Evaluate a model.\n",
        "\n",
        "    Args:\n",
        "        model: Model to evaluate\n",
        "        test_loader: Test data loader\n",
        "        device: Device to use\n",
        "\n",
        "    Returns:\n",
        "        Test loss and accuracy\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = test_correct / test_total\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "id": "e2949561",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo: Train a Simple Classifier\n"
      ],
      "id": "eb5a1ffd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Helper classes and functions for training\n",
        "class SimpleDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Simple dataset for image patches and labels.\"\"\"\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]\n",
        "\n",
        "def train_classifier(model, train_loader, val_loader=None, n_epochs=10, lr=1e-4, device='cpu'):\n",
        "    \"\"\"Train a classifier.\"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{n_epochs}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f}\")\n",
        "            if val_loader is not None:\n",
        "                print(f\"  Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# Split data\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(sample_patches)),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=sample_labels.numpy()\n",
        ")\n",
        "\n",
        "train_images = sample_patches[train_idx]\n",
        "train_labels = sample_labels[train_idx]\n",
        "val_images = sample_patches[val_idx]\n",
        "val_labels = sample_labels[val_idx]\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SimpleDataset(train_images, train_labels)\n",
        "val_dataset = SimpleDataset(val_images, val_labels)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Training set: {len(train_dataset)} samples\")\n",
        "print(f\"Validation set: {len(val_dataset)} samples\")\n",
        "\n",
        "# Create a simple classifier\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, feature_extractor, num_classes=5):\n",
        "        super().__init__()\n",
        "        self.features = feature_extractor\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "classifier = SimpleClassifier(model, num_classes=5).to(device)\n",
        "\n",
        "print(f\"\\nTraining classifier...\")\n",
        "history = train_classifier(\n",
        "    classifier,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    n_epochs=15,\n",
        "    lr=1e-3,\n",
        "    device=str(device)\n",
        ")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ],
      "id": "49c7f237",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo: Visualize Training Progress\n"
      ],
      "id": "ddb5faee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "ax1.plot(epochs, history['train_loss'], label='Train Loss')\n",
        "ax1.plot(epochs, history['val_loss'], label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(epochs, history['train_acc'], label='Train Accuracy')\n",
        "ax2.plot(epochs, history['val_acc'], label='Val Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final validation accuracy: {history['val_acc'][-1]:.3f}\")"
      ],
      "id": "2abf9501",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4: Model Comparison\n",
        "\n",
        "Let's compare different approaches to understand trade-offs.\n",
        "\n",
        "### Demo: Compare Feature Extraction vs. End-to-End Training\n"
      ],
      "id": "c0618586"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare frozen features vs. fine-tuned model\n",
        "\n",
        "# Approach 1: Frozen feature extractor\n",
        "frozen_model = SimpleClassifier(model, num_classes=5).to(device)\n",
        "for param in frozen_model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\"Training with frozen features...\")\n",
        "frozen_history = train_classifier(\n",
        "    frozen_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    n_epochs=10,\n",
        "    lr=1e-3,\n",
        "    device=str(device)\n",
        ")\n",
        "\n",
        "# Approach 2: End-to-end fine-tuning (we already have this from before)\n",
        "print(\"\\nComparing approaches:\")\n",
        "print(f\"Frozen features - Final val acc: {frozen_history['val_acc'][-1]:.3f}\")\n",
        "print(f\"Fine-tuned model - Final val acc: {history['val_acc'][-1]:.3f}\")\n",
        "\n",
        "# Visualize comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.plot(range(1, len(frozen_history['val_acc']) + 1),\n",
        "        frozen_history['val_acc'],\n",
        "        label='Frozen Features', marker='o')\n",
        "ax.plot(range(1, len(history['val_acc']) + 1),\n",
        "        history['val_acc'],\n",
        "        label='Fine-tuned', marker='s')\n",
        "\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Validation Accuracy')\n",
        "ax.set_title('Comparison: Frozen vs. Fine-tuned Features')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "133f7450",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5: Real-World Application\n",
        "\n",
        "Let's apply our trained model to make predictions on new data.\n",
        "\n",
        "### Demo: Inference on New Patches\n"
      ],
      "id": "92ee533b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract additional patches from the scene for testing\n",
        "# Use a different stride to get different patches\n",
        "print(\"Extracting test patches...\")\n",
        "test_patches, _ = extract_patches_from_scene(\n",
        "    best_scene,\n",
        "    subset_bbox=santa_barbara_bbox,\n",
        "    patch_size=64,\n",
        "    stride=60,  # Different stride to get non-overlapping patches\n",
        "    target_patches=50\n",
        ")\n",
        "\n",
        "test_labels = create_spectral_labels(test_patches)\n",
        "\n",
        "print(f\"Test set: {len(test_patches)} patches\")\n",
        "\n",
        "# Make predictions\n",
        "classifier.eval()\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_patches), 32):\n",
        "        batch = test_patches[i:i+32].to(device)\n",
        "        outputs = classifier(batch)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_probs.append(probs.cpu())\n",
        "\n",
        "predictions = torch.cat(all_preds).numpy()\n",
        "probabilities = torch.cat(all_probs).numpy()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = (predictions == test_labels.numpy()).mean()\n",
        "print(f\"\\nTest accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Show confidence distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "max_probs = probabilities.max(axis=1)\n",
        "plt.hist(max_probs, bins=30, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('Maximum Prediction Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Prediction Confidence Distribution')\n",
        "plt.axvline(max_probs.mean(), color='red', linestyle='--',\n",
        "            label=f'Mean: {max_probs.mean():.3f}')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "5d5e6d9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Demo: Visualize Predictions\n"
      ],
      "id": "63521469"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize some predictions\n",
        "class_names = ['Water', 'Vegetation', 'Urban', 'Bare Soil', 'Mixed']\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx in range(10):\n",
        "    # Create false color composite (NIR, Red, Green) for better visualization\n",
        "    # Channels: 0=Red, 1=Green, 2=Blue, 3=NIR\n",
        "    nir = test_patches[idx, 3].numpy()\n",
        "    red = test_patches[idx, 0].numpy()\n",
        "    green = test_patches[idx, 1].numpy()\n",
        "\n",
        "    # False color: NIR as red, red as green, green as blue\n",
        "    img = np.stack([nir, red, green], axis=2)\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    true_label = test_labels[idx].item()\n",
        "    pred_label = predictions[idx]\n",
        "    confidence = probabilities[idx, pred_label]\n",
        "\n",
        "    axes[idx].imshow(img)\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    title = f\"True: {class_names[true_label]}\\n\"\n",
        "    title += f\"Pred: {class_names[pred_label]} ({confidence:.2f})\"\n",
        "    axes[idx].set_title(title, color=color, fontsize=9)\n",
        "\n",
        "plt.suptitle('Sample Predictions - False Color (NIR-R-G)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNote: False color composite shown (NIR=Red, Red=Green, Green=Blue)\")\n",
        "print(\"Vegetation appears bright red due to high NIR reflectance\")"
      ],
      "id": "7985f694",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "You've successfully worked with geospatial foundation models using TerraTorch!\n",
        "\n",
        "### What You Accomplished\n",
        "\n",
        "1. **Model Loading**: Loaded and inspected pretrained models\n",
        "2. **Feature Extraction**: Extracted embeddings from satellite imagery\n",
        "3. **Transfer Learning**: Fine-tuned models for classification\n",
        "4. **Model Comparison**: Compared different training approaches\n",
        "5. **Inference**: Applied models to make predictions on new data\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Pretrained models** provide powerful feature representations\n",
        "- **Transfer learning** is more efficient than training from scratch\n",
        "- **Feature extraction** vs. **fine-tuning** offer different trade-offs\n",
        "- **Model comparison** helps identify the best approach for your task\n",
        "- **TerraTorch** simplifies working with geospatial foundation models\n",
        "\n",
        "### Performance Summary\n"
      ],
      "id": "8982e154"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Training Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Model type: {type(classifier).__name__}\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_patches)}\")\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"Validation accuracy: {history['val_acc'][-1]:.3f}\")\n",
        "print(f\"Test accuracy: {accuracy:.3f}\")\n",
        "print(f\"Average confidence: {max_probs.mean():.3f}\")"
      ],
      "id": "ebacd748",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next Week Preview\n",
        "\n",
        "In **Week 4**, we'll explore:\n",
        "\n",
        "- Multi-modal foundation models (optical + radar)\n",
        "- Temporal modeling with satellite time series\n",
        "- Advanced fine-tuning strategies\n",
        "- Scaling to larger datasets\n",
        "\n",
        "Your understanding of foundation models provides the perfect foundation for these advanced topics!\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [TerraTorch Documentation](https://github.com/IBM/terratorch)\n",
        "- [Prithvi Model](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M)\n",
        "- [Clay Foundation Model](https://clay-foundation.github.io/)\n",
        "- [TorchGeo Documentation](https://torchgeo.readthedocs.io/)"
      ],
      "id": "238928c7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "geoai",
      "language": "python",
      "display_name": "GeoAI",
      "path": "/Users/kellycaylor/Library/Jupyter/kernels/geoai"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}