---
title: "Week 3: TerraTorch Foundation Models"
subtitle: "Using pretrained models for geospatial tasks"
jupyter: geoai
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
---

## Introduction

TerraTorch is a library for working with pretrained geospatial foundation models. This session shows how to use it for practical tasks with real satellite data.

:::{.callout-tip}
## Learning Goals
- Load pretrained foundation models using TerraTorch
- Apply models to classification tasks
- Apply models to segmentation tasks
- Compare different model backbones
- Understand TerraTorch's composable workflow
:::

## Setup

### Installation

```bash
# If not already installed
pip install terratorch
```

### Imports

```{python}

```

```{python}
# | tangle: geogfm/c03.py
import matplotlib.pyplot as plt
import torch
from geogfm.c01 import load_sentinel2_bands
from scipy.ndimage import zoom
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch
import matplotlib.pyplot as plt
import numpy as np
import sys
import os
import warnings

from geogfm.c02 import load_scene_with_cloudmask
from geogfm.c01 import setup_planetary_computer_auth, search_sentinel2_scenes

import logging
logger = logging.getLogger(__name__)

# Setup authentication
setup_planetary_computer_auth()

warnings.filterwarnings('ignore')

# Configure GDAL/PROJ environment before importing rasterio
proj_path = os.path.join(sys.prefix, 'share', 'proj')
if os.path.exists(proj_path):
    os.environ['PROJ_LIB'] = proj_path
    os.environ['PROJ_DATA'] = proj_path


logger.debug(f"PyTorch: {torch.__version__}")
logger.debug(f"CUDA available: {torch.cuda.is_available()}")

# Select best available device: CUDA (NVIDIA GPU) > CPU
# Note: MPS has compatibility issues with some operations (adaptive pooling)
# See: https://github.com/pytorch/pytorch/issues/96056
if torch.cuda.is_available():
    device = torch.device('cuda')
    logger.info("Using CUDA for acceleration")
else:
    device = torch.device('cpu')
    if torch.backends.mps.is_available():
        logger.info("Using CPU (MPS available but has compatibility issues with TerraTorch)")
    else:
        logger.info("Using CPU (no GPU acceleration available)")

```



### Extract Patches

We use the `extract_patches` function to efficiently sample many small, square patches (sub-images) from a larger Sentinel-2 satellite scene. This step is important for deep learning workflows such as our TerraTorch foundation model demo, where a model expects numerous fixed-size training samples rather than huge, unwieldy satellite images.

Specifically, this function:

- Loads six key Sentinel-2 bands (blue, green, red, NIR, SWIR1, SWIR2).
- Resamples 20m bands to 10m to create a consistent resolution stack.
- Randomly extracts small, square regions from the AOI, skipping patches that are heavily masked by clouds or contain mostly invalid data.
- Normalizes patch data (per band) based on within-patch percentiles.

**How we're using it:**  
In the TerraTorch demo, this function provides ready-to-use data patches for model training and experimentation, abstracting away preprocessing and making it easy to sample imagery data batches directly from a STAC/scene object.

**Key limitations and assumptions:**

- Only a fixed set of six bands is extracted. If other bands or other sensors are needed, this will require code changes.
- The function assumes Sentinel-2 naming and band layout, and will not generalize to other satellite products.
- Patch validity is assessed using a simple cloud/nodata mask, with a hardcoded 80% threshold.
- The [normalization](extras/examples/normalization_comparison.qmd) (2nd–98th percentile scaling) is done per patch, not globally, which may harm consistency across patches.
- The function operates on a single scene at a time and does not support multi-temporal or multi-scene sampling.
- Sampling within the AOI does not ensure spatial uniformity or class balance.

Generalization to other dataset shapes, larger patches, different patch selection rules, or global normalization would require further refactoring.

```{python}
#| tangle: geogfm/c03.py
#| mode: append

def extract_patches(scene, bbox, patch_size=64, n_patches=100):
    """
    Extract square patches of Sentinel-2 band data from a scene within a specified bounding box.

    This function loads six selected Sentinel-2 bands (B02, B03, B04, B08, B11, B12),
    resamples 20m bands to 10m resolution, and extracts random patches of a given size,
    ensuring that at least 80% of the pixels in each patch are valid (not cloud-masked or nodata).

    Parameters
    ----------
    scene : pystac.Item or similar
        The STAC item or scene object representing the Sentinel-2 scene.
    bbox : list[float] or tuple[float, float, float, float]
        Bounding box in [min lon, min lat, max lon, max lat] format specifying the area to extract patches from.
    patch_size : int, optional
        The height and width of the square patches to extract. Default is 64.
    n_patches : int, optional
        The number of valid patches to extract. May attempt to extract more to satisfy mask requirements. Default is 100.

    Returns
    -------
    patches : list of ndarray
        List of 3D NumPy arrays of shape (n_bands, patch_size, patch_size), containing normalized patch data.

    Notes
    -----
    - Bands B02, B03, B04, and B08 are at 10m native resolution; B11 and B12 are at 20m and will be resampled.
    - Only patches with at least 80% valid (unmasked) pixels are retained.
    - Bands are normalized per-patch using the 2nd and 98th percentiles of valid pixels.

    Examples
    --------
    >>> from geogfm.c01 import search_sentinel2_scenes
    >>> scenes = search_sentinel2_scenes(bbox=[-119.85, 34.40, -119.75, 34.48], limit=1)
    >>> scene = scenes[0]
    >>> patches = extract_patches(scene, [-119.85, 34.40, -119.75, 34.48], patch_size=64, n_patches=10)
    >>> print(patches[0].shape)
    (6, 64, 64)
    """
    # Load 6 bands to match Prithvi's expected input
    band_data = load_sentinel2_bands(
        scene,
        bands=['B02', 'B03', 'B04', 'B08', 'B11', 'B12'],
        subset_bbox=bbox,
        max_retries=3
    )

    logger.info(
        f"Loaded {len([k for k in band_data.keys() if k.startswith('B')])} bands")

    # Get target shape from 10m bands
    target_shape = band_data['B02'].shape

    # Resample 20m bands (B11, B12) to 10m resolution
    bands_list = []
    for band_name in ['B02', 'B03', 'B04', 'B08', 'B11', 'B12']:
        band = band_data[band_name]
        if band.shape != target_shape:
            # Resample to target shape
            zoom_factors = (
                target_shape[0] / band.shape[0], target_shape[1] / band.shape[1])
            band = zoom(band, zoom_factors, order=1)
        bands_list.append(band)

    # Stack bands
    bands = np.stack(bands_list)

    # Create cloud mask
    mask = ~np.isnan(bands[0])

    # Extract random patches
    _, H, W = bands.shape
    patches = []

    for _ in range(n_patches * 2):  # Try more to ensure enough valid
            y = np.random.randint(0, H - patch_size)
            x = np.random.randint(0, W - patch_size)

            patch = bands[:, y:y+patch_size, x:x+patch_size]
            patch_mask = mask[y:y+patch_size, x:x+patch_size]

            if np.mean(patch_mask) > 0.8:  # 80% valid
                # Normalize
                patch_norm = np.zeros_like(patch)
                for c in range(patch.shape[0]):
                    valid = patch[c][~np.isnan(patch[c])]
                    if len(valid) > 0:
                        p2, p98 = np.percentile(valid, [2, 98])
                        patch_norm[c] = np.clip(
                            (patch[c] - p2) / (p98 - p2 + 1e-8), 0, 1)
                        patch_norm[c] = np.nan_to_num(patch_norm[c], 0)

                patches.append(patch_norm)

            if len(patches) >= n_patches:
                break

    return torch.from_numpy(np.stack(patches)).float()
```

### Create Labels

The `create_labels` function generates simple, automatically assigned labels for satellite image patches based on their spectral characteristics, such as NDVI. These pseudo-labels (e.g., marking high NDVI areas as vegetation) serve as a basic example to help demonstrate and test the TerraTorch workflow. This label generation approach is not intended to be scientifically robust or production-ready—it is included here to showcase key TerraTorch features using a straightforward and easily understood process.

```{python}
#| tangle: geogfm/c03.py
#| mode: append

def create_labels(patches):
    """
    Generate pseudo-labels for input patches based on their spectral signatures.

    Labels are assigned as follows:
        - 0: Vegetation (NDVI mean > 0.5)
        - 1: Water      (NDVI mean < 0)
        - 2: Other      (otherwise)

    Parameters
    ----------
    patches : list or array-like
        Sequence of image patches as numpy arrays or tensors, each with shape (bands, height, width).

    Returns
    -------
    labels : torch.Tensor
        1D tensor of integer labels with length equal to number of patches.

    Examples
    --------
    >>> patches = [np.random.rand(6, 64, 64) for _ in range(10)]
    >>> labels = create_labels(patches)
    >>> print(labels)
    tensor([2, 0, 2, 1, 0, 2, 0, 2, 1, 0])
    """
    labels = []
    for patch in patches:
        # Prithvi bands: Blue, Green, Red, NIR, SWIR1, SWIR2
        blue, green, red, nir, swir1, swir2 = patch
        ndvi = (nir - red) / (nir + red + 1e-8)

        # Simple classification
        if ndvi.mean() > 0.5:
            labels.append(0)  # Vegetation
        elif ndvi.mean() < 0:
            labels.append(1)  # Water
        else:
            labels.append(2)  # Other

    return torch.tensor(labels)
```


### `PatchDataset` Object Definition

PatchDataset is a custom dataset class that inherits from PyTorch's `Dataset` object. By extending `Dataset`, `PatchDataset` can be used seamlessly with PyTorch's data loading utilities, such as `DataLoader`. In this class, we store our patch tensors and their corresponding labels, and implement the `__len__` and `__getitem__` methods required for efficient data batching and shuffling during training and evaluation. In our workflow, `PatchDataset` enables structured, efficient access to labeled image patches for training, validation, and testing deep learning models.


```{python}
#| tangle: geogfm/c03.py
#| mode: append

class PatchDataset(Dataset):
    def __init__(self, patches, labels):
        self.patches = patches
        self.labels = labels

    def __len__(self):
        return len(self.patches)

    def __getitem__(self, idx):
        return self.patches[idx], self.labels[idx]
```


## Create Sample Datasets

### Load Satellite Data

We'll load a small Sentinel-2 scene and extract patches for our examples using our existing library code.

```{python}

# Small AOI in Santa Barbara (5km x 5km)
aoi = [-119.85, 34.40, -119.75, 34.48]

# Search for clear scene
scenes = search_sentinel2_scenes(
    bbox=aoi, 
    date_range="2023-06-01/2023-08-31",
    cloud_cover_max=5,
    limit=1
)

scene = scenes[0]

logger.info(f"Using scene: {scene.id[:30]}...")
logger.info(f"Date: {scene.properties['datetime'][:10]}")
logger.info(f"Cloud cover: {scene.properties['eo:cloud_cover']:.1f}%")
```

### Extract Patches
```{python}
# Extract patches
patches = extract_patches(scene, aoi, patch_size=64, n_patches=200)
logger.info(f"Extracted {len(patches)} patches of shape {patches[0].shape}")
```

### Create Labels:
```{python}
labels = create_labels(patches)
logger.info(f"Label distribution: {torch.bincount(labels)}")
```

### Visualize some example patches and labels:

```{python}
import matplotlib.pyplot as plt

# Map label integers to class names
class_names = {0: 'Vegetation', 1: 'Water', 2: 'Other'}

# Select a few (e.g., 6) random patches to visualize
num_examples = 6
indices = torch.randperm(len(patches))[:num_examples]

fig, axes = plt.subplots(1, num_examples, figsize=(15, 3))
for ax, idx in zip(axes, indices):
    patch = patches[idx]
    # HLS: bands are typically in (C, H, W) format; show an RGB composite
    # Here, assume RGB = bands 2,1,0 (if patch shape[0]>=3)
    if patch.shape[0] >= 3:
        rgb = patch[:3].permute(1, 2, 0)
        # Scale for visualization
        rgb_min = rgb.min().item()
        rgb_max = rgb.max().item()
        if rgb_max > rgb_min:
            rgb_vis = (rgb - rgb_min) / (rgb_max - rgb_min)
        else:
            rgb_vis = rgb
        ax.imshow(rgb_vis)
    else:
        ax.imshow(patch[0].cpu(), cmap='gray')
    label = labels[idx].item() if hasattr(labels[idx], 'item') else labels[idx]
    ax.set_title(class_names[label])
    ax.axis('off')
plt.tight_layout()
plt.show()
```

### Create Training/Validation/Testing Datasets:

# We split our patch dataset into three distinct sets:
# - Training set: Used to fit the model's weights (learn patterns).
# - Validation set: Used for intermediate evaluation to tune hyperparameters and select the best model, preventing overfitting.
# - Test set: Held out entirely until final evaluation to assess model generalization on unseen data.

# Typical allocation ratios are:
#   70% for training,
#   15% for validation,
#   15% for testing.

# Below, we compute indices for these splits and create dataset objects accordingly.

```{python}

# Split data
n_train = int(0.7 * len(patches))
n_val = int(0.15 * len(patches))

train_dataset = PatchDataset(patches[:n_train], labels[:n_train])
val_dataset = PatchDataset(patches[n_train:n_train+n_val], labels[n_train:n_train+n_val])
test_dataset = PatchDataset(patches[n_train+n_val:], labels[n_train+n_val:])

logger.info(f"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")
```

## Using TerraTorch Models

### Load a Pretrained Model

The `EncoderDecoderFactory` is a utility in TerraTorch that streamlines the creation of deep learning models based on the encoder-decoder architecture. This object abstracts away boilerplate and provides a simple interface for constructing models suited for a variety of earth observation tasks, such as classification or segmentation.

When calling `build_model()`, you specify several important arguments to configure the model:

- `task`: The type of problem you are solving (e.g., `"classification"` for assigning a label to an image patch, or `"segmentation"` for pixel-wise classification).
- `backbone`: The encoder neural network that extracts features from the input data. Common options include pretrained vision transformers like `"prithvi_eo_v1_100"` (Prithvi-100M), `"resnet18"`, `"resnet50"`, etc.
- `decoder`: The decoder determines how extracted features are transformed for the target task. Options include `"FCNDecoder"` for simple classification, `"UNetDecoder"`, etc.
- `num_classes`: The number of output classes for your problem (for example, 3 land cover types in our current classification task).

These parameters allow you to flexibly adapt powerful pretrained models to your specific remote sensing task.

**Example usage:**

```python
model = model_factory.build_model(
    task="classification",            # classification or segmentation
    backbone="prithvi_eo_v1_100",     # encoder/feature extractor
    decoder="FCNDecoder",             # type of decoder/classification head
    num_classes=3                     # number of output classes
)
```

Other parameters (see the [TerraTorch documentation](https://terra-torch.readthedocs.io/en/latest/) for more options) can control things like input channels, dropout, and patch size, enabling further customization.


```{python}
try:
    from terratorch.models import EncoderDecoderFactory

    # Create model factory
    model_factory = EncoderDecoderFactory()

    # Build classification model with Prithvi backbone
    model = model_factory.build_model(
        task="classification",
        backbone="prithvi_eo_v1_100",
        decoder="FCNDecoder",
        num_classes=3
    )

    logger.info("Loaded Prithvi-100M model")
    logger.info(f"Parameters: {sum(p.numel() for p in model.parameters()):,}")

except ImportError:
    logger.info("TerraTorch not installed. Install with: pip install terratorch")

model = model.to(device)
```

## Task 1: Classification

### Train the Model

Model training and validation loops are fundamental routines in deep learning workflows. 

- The **training loop** iterates over batches of data, computes predictions using the model, evaluates a loss function, performs backpropagation, and updates model parameters with an optimizer. 
- The **validation loop** evaluates the model on unseen data without updating parameters (i.e., in `eval` mode and without gradient computation). It is used to track performance and detect overfitting.

In the code below, the `train_model` function implements both loops:

- During training, it sets the model to training mode (`model.train()`), zeroes gradients, computes loss and accuracy, backpropagates, and steps the optimizer for each batch.
- During validation, it switches to evaluation mode (`model.eval()`), disables gradient computation (`with torch.no_grad()`), and calculates loss and accuracy.
- Losses and accuracies are accumulated and averaged over all batches for both sets.
- Training and validation history is collected in a dictionary for analysis.

**Limitations and assumptions:**

- The code assumes a simple classification task with standard `CrossEntropyLoss` and that labels are properly formatted for it.
- There's no support for advanced features like data augmentation, learning rate scheduling, early stopping, or mixed-precision training.
- No checkpointing or logging of model weights is implemented.
- The function globally assumes the device (CPU or GPU) context is handled outside (for both model and batches).
- It does not handle distributed/multi-GPU training or non-image data modalities.
- Batch sizes and data shuffling are expected to be handled by the user in DataLoader construction.
- For production, you'd want explicit error handling, flexibility for other tasks/losses, and possibly hooks for callbacks or custom metrics.

This simple structure works well for prototyping and educational purposes, but for robust, scalable applications, more initialization, error resistance, and configurability are needed.


```{python}
#| tangle: geogfm/c03.py
#| mode: append

def train_model(model, train_loader, val_loader, epochs=10, lr=1e-4):
    """
    Trains a classification model using a simple training and validation loop.

    This function optimizes the model's parameters based on the training dataset and evaluates 
    performance on the validation set at the end of each epoch. It supports standard image 
    classification tasks with models returning logits, such as those from TerraTorch or PyTorch.

    Parameters
    ----------
    model : torch.nn.Module
        The neural network model to train. Should output logits when given images.
    train_loader : torch.utils.data.DataLoader
        DataLoader instance providing the training data batches (images, labels).
    val_loader : torch.utils.data.DataLoader
        DataLoader instance providing the validation data batches (images, labels).
    epochs : int, optional
        Number of times to iterate over the training dataset (default is 10).
    lr : float, optional
        Learning rate for the Adam optimizer (default is 1e-4).

    Returns
    -------
    history : dict
        A dictionary containing lists of loss and accuracy values per epoch for training and validation.
        Keys are: 'train_loss', 'train_acc', 'val_loss', 'val_acc'.

    Examples
    --------
    >>> model = model_factory.build_model(task="classification", backbone="prithvi_eo_v1_100", decoder="FCNDecoder", num_classes=3)
    >>> model = model.to(device)
    >>> train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
    >>> val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64)
    >>> history = train_model(model, train_loader, val_loader, epochs=5, lr=1e-4)
    >>> print(history["train_acc"])
    [0.70, 0.84, 0.89, 0.91, 0.93]
    """
    # The optimizer updates model parameters to minimize the loss during training.
    # Here we use Adam, a popular optimizer for deep learning that adapts learning rates.
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # The criterion defines the loss function, which measures how far the model's predictions
    # are from the correct labels. CrossEntropyLoss is standard for multiclass classification.
    criterion = nn.CrossEntropyLoss()

    history = {'train_loss': [], 'train_acc': [],
               'val_loss': [], 'val_acc': []}

    for epoch in range(epochs):
        # Train
        model.train()
        train_loss = 0    # Sum of losses for the epoch
        correct = 0      # Number of correctly predicted samples
        total = 0        # Total number of samples processed

        for images, targets in train_loader:
            images, targets = images.to(device), targets.to(device)

            optimizer.zero_grad()
            outputs = model(images)

            # Extract tensor from ModelOutput if needed
            if hasattr(outputs, 'output'):
                outputs = outputs.output

            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        train_loss /= len(train_loader)
        train_acc = correct / total

        # Validate
        model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for images, targets in val_loader:
                images, targets = images.to(device), targets.to(device)
                outputs = model(images)

                # Extract tensor from ModelOutput if needed
                if hasattr(outputs, 'output'):
                    outputs = outputs.output

                loss = criterion(outputs, targets)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

        val_loss /= len(val_loader)
        val_acc = correct / total

        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        if (epoch + 1) % 5 == 0:
            logger.info(
                f"Epoch {epoch+1}: Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}")

    return history
```

```{python}
# Create loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
test_loader = DataLoader(test_dataset, batch_size=32)

# Train
logger.info("Training classification model...")
history = train_model(model, train_loader, val_loader, epochs=15)
```

### Evaluate

```{python}
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for images, targets in test_loader:
        images, targets = images.to(device), targets.to(device)
        outputs = model(images)

        # Extract tensor from ModelOutput if needed
        if hasattr(outputs, 'output'):
            outputs = outputs.output

        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

test_acc = correct / total
logger.info(f"Test accuracy: {test_acc:.3f}")
```

### Visualize Results

```{python}
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

epochs = range(1, len(history['train_loss']) + 1)

ax1.plot(epochs, history['train_loss'], label='Train')
ax1.plot(epochs, history['val_loss'], label='Val')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.set_title('Training Loss')
ax1.legend()
ax1.grid(alpha=0.3)

ax2.plot(epochs, history['train_acc'], label='Train')
ax2.plot(epochs, history['val_acc'], label='Val')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.set_title('Training Accuracy')
ax2.legend()
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

## Task 2: Segmentation

Using the same data for a different task.

```{python}
try:
    # Create segmentation model with same backbone
    seg_model = model_factory.build_model(
        task="segmentation",
        backbone="prithvi_eo_v1_100",
        decoder="UperNetDecoder",
        num_classes=3
    )

    logger.info("Loaded Prithvi segmentation model")
    logger.info(f"Parameters: {sum(p.numel() for p in seg_model.parameters()):,}")

except (ImportError, NameError):
    # Demo segmentation model
    class DemoSegModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(6, 64, 3, padding=1)  # 6 channels for HLS bands
            self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
            self.conv3 = nn.Conv2d(128, 3, 1)

        def forward(self, x):
            x = torch.relu(self.conv1(x))
            x = torch.relu(self.conv2(x))
            return self.conv3(x)

    seg_model = DemoSegModel()
    logger.warning("Demo segmentation model")

seg_model = seg_model.to(device)
```

### Create Segmentation Targets

```{python}
# | tangle: geogfm/c03.py
# | mode: append
import numpy as np
import torch

def create_seg_masks(patches):
    """
    Create pixel-wise segmentation masks based on spectral signatures for each pixel.

    Labels are assigned per-pixel as follows:
        - 0: Vegetation (NDVI > 0.5)
        - 1: Water (NDVI < 0)
        - 2: Other (otherwise)

    Parameters
    ----------
    patches : torch.Tensor
        Tensor of patches with shape (N, C, H, W) where C=6 bands.

    Returns
    -------
    masks : torch.Tensor
        Tensor of masks with shape (N, H, W) containing class labels for each pixel.
    """
    masks = []
    for patch in patches:
        # Extract bands: Blue, Green, Red, NIR, SWIR1, SWIR2
        blue, green, red, nir, swir1, swir2 = patch

        # Calculate NDVI for each pixel
        ndvi = (nir - red) / (nir + red + 1e-8)

        # Initialize mask with "Other" class (2)
        mask = torch.ones_like(ndvi, dtype=torch.long) * 2

        # Assign vegetation (0) where NDVI > 0.5
        mask[ndvi > 0.5] = 0

        # Assign water (1) where NDVI < 0
        mask[ndvi < 0] = 1

        masks.append(mask)

    return torch.stack(masks)

```

```{python}
#| tangle: geogfm/c03.py
#| mode: append
class SegDataset(Dataset):
    def __init__(self, patches, masks):
        self.patches = patches
        self.masks = masks

    def __len__(self):
        return len(self.patches)

    def __getitem__(self, idx):
        return self.patches[idx], self.masks[idx]
```

```{python}
# Create masks
train_masks = create_seg_masks(patches[:n_train])
val_masks = create_seg_masks(patches[n_train:n_train+n_val])
```

```{python}

seg_train_loader = DataLoader(SegDataset(patches[:n_train], train_masks), batch_size=16, shuffle=True)
seg_val_loader = DataLoader(SegDataset(patches[n_train:n_train+n_val], val_masks), batch_size=16)

logger.info(f"Segmentation datasets ready")
```

### Train Segmentation

```{python}
#| tangle: geogfm/c03.py
#| mode: append

def train_segmentation(model, train_loader, val_loader, epochs=10):
    """Train segmentation model."""
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        model.train()
        train_loss = 0

        for images, masks in train_loader:
            images, masks = images.to(device), masks.to(device)

            optimizer.zero_grad()
            outputs = model(images)

            # Extract tensor from ModelOutput if needed
            if hasattr(outputs, 'output'):
                outputs = outputs.output

            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)

        # Validate
        model.eval()
        val_loss = 0

        with torch.no_grad():
            for images, masks in val_loader:
                images, masks = images.to(device), masks.to(device)
                outputs = model(images)

                # Extract tensor from ModelOutput if needed
                if hasattr(outputs, 'output'):
                    outputs = outputs.output

                loss = criterion(outputs, masks)
                val_loss += loss.item()

        val_loss /= len(val_loader)

        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}")
```



```{python}
print("Training segmentation model...")
train_segmentation(seg_model, seg_train_loader, seg_val_loader, epochs=10)
```

#### Visualize output and accuracy

```{python}
# | tangle: geogfm/c03.py
# | mode: append

def get_accuracy(model, data_loader, device_name=None):
    """Compute pixelwise accuracy for segmentation"""
    if device_name is None:
        device_name = device
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, masks in data_loader:
            images, masks = images.to(device_name), masks.to(device_name)
            outputs = model(images)
            # Extract tensor from ModelOutput if needed
            if hasattr(outputs, 'output'):
                outputs = outputs.output
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == masks).sum().item()
            total += masks.numel()
    accuracy = correct / total
    return accuracy
```

```{python}
# Compute accuracy on validation data
accuracy = get_accuracy(seg_model, seg_val_loader)
logger.info(
    f"Segmentation Model Pixel Accuracy on Validation Data: {accuracy:.3%}")
```

```{python}
# | tangle: geogfm/c03.py
# | mode: append

def visualize_predictions(model, data_loader, device_name=None, n=3):
    if device_name is None:
        device_name = device
    model.eval()
    images, masks = next(iter(data_loader))
    images, masks = images.to(device_name), masks.to(device_name)
    with torch.no_grad():
        outputs = model(images)
        if hasattr(outputs, 'output'):
            outputs = outputs.output
        preds = torch.argmax(outputs, dim=1)

    # Choose up to n samples to show
    num_show = min(n, images.shape[0])
    for i in range(num_show):
        fig, axs = plt.subplots(1, 3, figsize=(12, 4))
        # Display RGB composite from the image
        img_np = images[i].cpu()
        if img_np.shape[0] >= 3:
            # For 6-band Sentinel-2: bands are [Blue, Green, Red, NIR, SWIR1, SWIR2]
            # Select first 3 bands (Blue, Green, Red) and reorder to RGB
            rgb = torch.stack([img_np[2], img_np[1], img_np[0]], dim=0)  # R, G, B
            rgb = rgb.permute(1, 2, 0)  # (H, W, C)
            axs[0].imshow(rgb)
        else:
            # Single band image - show as grayscale
            axs[0].imshow(img_np[0], cmap='gray')
        axs[0].set_title('Input Image')
        axs[0].axis('off')

        axs[1].imshow(masks[i].cpu(), cmap='tab20')
        axs[1].set_title('Ground Truth Mask')
        axs[1].axis('off')

        axs[2].imshow(preds[i].cpu(), cmap='tab20')
        axs[2].set_title('Predicted Mask')
        axs[2].axis('off')

        plt.tight_layout()
        plt.show()
```

```{python}
visualize_predictions(seg_model, seg_val_loader, n=3)
```


## Comparing Backbones

TerraTorch makes it easy to compare different foundation models.

**Available backbones in TerraTorch (examples):**

- `prithvi_eo_v1_100` &mdash; Prithvi 100M parameter model  
- `prithvi_eo_v1_300` &mdash; Prithvi 300M parameter model  
- `prithvi_eo_v2_300` &mdash; Prithvi V2 300M model  
- `timm_resnet50` &mdash; ResNet-50 from timm  

---

**To use a different backbone, you can build your model like this:**

```python
model = model_factory.build_model(
    task='classification',
    backbone='prithvi_eo_v2_300',
    decoder='FCNDecoder',
    num_classes=3
)
```

### Key Takeaways

1. **TerraTorch simplifies model loading**: One API for multiple foundation models
2. **Task flexibility**: Same backbone for classification, segmentation, etc.
3. **Model comparison**: Easy to swap backbones
4. **Production ready**: Built for real-world geospatial applications

## Resources

- [TerraTorch GitHub](https://github.com/IBM/terratorch)
- [Prithvi Models](https://huggingface.co/ibm-nasa-geospatial)
- [Model Zoo](https://github.com/IBM/terratorch/blob/main/MODEL_ZOO.md)

